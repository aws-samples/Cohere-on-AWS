{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27346772-a703-4cc8-ab30-147d918057b3",
   "metadata": {},
   "source": [
    "# Cohere SDK /tokenize and /detokenize API Calls Translation to the Amazon Bedrock API\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bae33f-7135-4533-870b-c2f244c34b94",
   "metadata": {},
   "source": [
    "**Notebook Author:** Albert Opher (@ophera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19639e-b4aa-46dd-900d-c86026a975ca",
   "metadata": {},
   "source": [
    "## Description\n",
    "In this notebook, we demonstrate how to translate the Cohere SDK /tokenize and /detokenize API calls into Amazon Bedrock API calls\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361e97d-6070-43ae-a5f2-b8adf5623b2b",
   "metadata": {},
   "source": [
    "## Run /tokenize on Cohere\n",
    "Using Cohere's Command R + model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "17e75dca-28c2-4804-9f91-5efc9a9d2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cohere --upgrade --quiet\n",
    "\n",
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "654f6feb-2a36-4866-b738-a3fe57436853",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(\"JIHjrSvP27PD0K77kSRIIXH1OX8pieLPn6joSFpA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f1f6ea75-9f13-4460-b919-b6e89cdabe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.cohere.com/v1/models/command-r-plus \"HTTP/1.1 200 OK\"\n",
      "INFO:cohere.manually_maintained.tokenizers:Downloading tokenizer for model command-r-plus. Size is 3.78 MBs.\n"
     ]
    }
   ],
   "source": [
    "response = co.tokenize(text=\"sample message\", model=\"command-r-plus\")\n",
    "response2 = co.tokenize(text=\"toothpaste\", model=\"command-r-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ec93f4f9-d7eb-4e02-a1d0-36344775c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 'sample message':  tokens=[22429, 6680] token_strings=[] meta=None  |  Tokenize 'toothpaste':  tokens=[150601, 64507] token_strings=[] meta=None\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenize 'sample message': \", response, \" | \", \"Tokenize 'toothpaste': \", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399d2e9-dd7e-4145-a554-9a9f118fe1ea",
   "metadata": {},
   "source": [
    "## Run /detokenize on Cohere\n",
    "Using Cohere's Commard R + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "26f45b0c-c9ec-4ebd-b00d-c3d0efe2f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = co.detokenize(tokens = [22429, 6680], model=\"command-r-plus\")\n",
    "response4 = co.detokenize(tokens=[150601, 64507], model=\"command-r-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b4099adb-6dba-4ab2-8a49-20905f1140e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenize [22429, 6680]: text='sample message' meta=None | Detokenize [150601, 64507]: text='toothpaste' meta=None\n"
     ]
    }
   ],
   "source": [
    "print(\"Detokenize [22429, 6680]:\", response3, \"|\", \"Detokenize [150601, 64507]:\", response4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e3122-1a33-4094-8bd3-1e3d22a4c50a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fd180-c954-40d2-a079-b741f1452d00",
   "metadata": {},
   "source": [
    "## Install Bedrock Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2dd64422-b919-402a-8609-0736bd971557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets --quiet\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4c5e66a4-5c9d-459b-acea-2e5bce00b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, logging, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "45a11912-d573-4357-98f3-9067f9fd83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R_PLUS = \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R = \"cohere.command-r-v1:0\"\n",
    "model_iD = DEFAULT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f02ace-9856-4686-8fed-07ab88ea9e68",
   "metadata": {},
   "source": [
    "## Enstantiate Command R + on Bedrock and Attempt Tokenize Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88da23-67cb-4b6f-b031-e53aa1a38aa4",
   "metadata": {},
   "source": [
    "## Tokenize Model v.1.1 -> Ask Bedrock How to Implement Tokenize using Chosen Cohere Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f60dd64a-6f84-4cb8-a7be-ff2a542ed19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_rt= boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "77c2c519-8299-43e4-9c0d-42c9a4d5c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"Tokenize the Following Text using cohere on bedrock. bedrock should make a call to the /tokenize api call in cohere's command library or it should find an equivalent method to get the desired result: text to tokenize. Explain how you got this. The prompt should return exactly: tokens=[2912, 1705, 10587, 2261] token_strings=[] meta=None. How do I get this result? what command can i use to access cohere's vocalulary for tokenizing on bedrock? provide me the code using boto3 to access this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "17b6eb4d-0154-4d06-bdd3-aa2c43368bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to generate the text\n",
    "#temp set to 0.3 by default\n",
    "def generate_text(prompt, model_id, temp=0.3):\n",
    "    body = {\n",
    "    'message': prompt,\n",
    "    'temperature': temp,\n",
    "    'preamble':\"\"\n",
    "    }\n",
    "# Invoke the Bedrock model\n",
    "    response = bedrock_rt.invoke_model_with_response_stream(\n",
    "        modelId= model_iD,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "# Print the response\n",
    "    stream = response.get('body')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                byte = chunk.get('bytes').decode()\n",
    "                output=json.loads(byte)\n",
    "            if output['event_type'] == 'text-generation':\n",
    "                print(output['text'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b34c77fc-572a-47e2-b67f-0ca1f34b2f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tokenize the text \"text to tokenize\" using Cohere on Bedrock, you can use the `/tokenize` API endpoint. Here's how you can achieve the result you mentioned:\n",
      "\n",
      "tokens = [2912, 1705, 10587, 2261]\n",
      "token_strings = []\n",
      "meta = None\n",
      "\n",
      "Explanation:\n",
      "\n",
      "- tokens: This is a list of integers representing the tokenized form of the input text. Each integer corresponds to a specific token in Cohere's vocabulary. In this case, the tokens are [2912, 1705, 10587, 2261].\n",
      "- token_strings: This list is empty because you have not asked for it. If you want the token strings, you can modify the code to make a call to cohere's vocabulary endpoint.\n",
      "- meta: This variable is set to None because you have not requested any additional metadata for the tokenization result.\n",
      "\n",
      "To access Cohere's vocabulary for tokenizing on Bedrock using Boto3, you can use the following Python code:\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "\n",
      "# Replace <your-api-key> with your actual Cohere API key\n",
      "api_key = \"<your-api-key>\"\n",
      "\n",
      "# Create a Boto3 client for Cohere\n",
      "cohere = boto3.client(\n",
      "    service_name=\"cohere\",\n",
      "    region_name=\"us-east-1\",\n",
      "    api_version=\"2022-03-01\",\n",
      "    endpoint_url=\"https://api.cohere.com\",\n",
      "    aws_secret_access_key=api_key,\n",
      "    aws_access_key_id=api_key,\n",
      ")\n",
      "\n",
      "# Text to tokenize\n",
      "text = \"text to tokenize\"\n",
      "\n",
      "# Make a call to the /tokenize API endpoint\n",
      "response = cohere.tokenize(text=text)\n",
      "\n",
      "# Extract the tokens and token strings from the response\n",
      "tokens = response[\"tokens\"]\n",
      "token_strings = response[\"tokenStrings\"]\n",
      "\n",
      "# Print the result\n",
      "print(f\"tokens={tokens} token_strings={token_strings} meta={response.get('meta', None)}\")\n",
      "```\n",
      "\n",
      "Replace `<your-api-key>` with your actual Cohere API key. This code uses Boto3 to interact with the Cohere API. It sends a request to the `/tokenize` endpoint with the input text and then extracts the tokens and token strings from the response. Finally, it prints the result in the format you requested."
     ]
    }
   ],
   "source": [
    "response_v11 = generate_text(prompt, model_iD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef75603-7f4b-4c3d-87b1-69a5cf11c220",
   "metadata": {},
   "source": [
    "## Tokenize Model v.1.2 -> Trying to Implement the Code Bedrock Gave on a Given Prompting Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3d604b78-c197-4593-bc04-e3326ca04d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens=[22429, 6680] token_strings=[] meta=None\n"
     ]
    }
   ],
   "source": [
    "client = co\n",
    "\n",
    "text_to_tokenize = \"sample message\"\n",
    "\n",
    "# Use the /tokenize API endpoint to tokenize the text\n",
    "response_v12 = client.tokenize(model=\"command-r-plus\", text=text_to_tokenize)\n",
    "#client.text_to_tokenize\n",
    "\n",
    "#tokens = response.json()[\"tokens\"]\n",
    "#token_strings = response.json().get(\"tokenStrings\", [])\n",
    "#meta = response.json().get(\"meta\", None)\n",
    "\n",
    "#print(f\"tokens={tokens} token_strings={token_strings} meta={meta}\")\n",
    "print(response_v12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494831a5-1cf6-4d2f-9f48-f73cf75ed03f",
   "metadata": {},
   "source": [
    "**This model uses a chatbot approach. However, it doesn't give us the correct code to implement nor does it give us the right tokenization through a chat prompt. In addition, Bedrock usually calls boto3 but never references itslef. Instead, it will opt to make a convoluted variation of the Cohere /token api call in the documentation and shown above. Code commented out to not throw errors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e3368-4aa9-42f2-a7ac-fdafec69fd55",
   "metadata": {},
   "source": [
    "## Tokenize Model v.2.1 -> Adopoted Mistral Style Class Call for Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bc526006-0aaa-448c-a0f9-dcc21f82d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")\n",
    "        \n",
    "    def converse(self, prompt, temperature=0.0, max_tokens=3000):\n",
    "        messages = \"tokenize text string: text to tokenize\",\n",
    "        \n",
    "        prompt = json.dumps({\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        })\n",
    "\n",
    "        \n",
    "        response = self.bedrock.converse(modelId=self.model_id,\n",
    "        messages = messages)\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body['outputs'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f0745c17-dfc3-4561-ab2b-573cc87d2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "commandRPlus = LLM(model_iD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "eef76f39-6b73-412b-b649-d6c23095989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_v21 = commandRPlus.converse(\"tokenize text string: text to tokenize\", temperature=0.0, max_tokens=3000)\n",
    "#print(result_v21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ac376-eb90-4e8f-9e8f-7064e7640775",
   "metadata": {},
   "source": [
    "**This model is adopted from the Bedrock and Mistral Translation Notebook. It follows to enstantiate an LLM class and call Command R + on Bedrock that way. I couldn't quite figure out how to provide the json.dump{} dictionary to prompt. This could be promising in the future if someone can pick up the reigns. Code commented out to not throw errors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859c6b3-2fca-43d3-966f-dbb988bd4e6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37418aed-54cc-4f6e-b831-7a3518615d00",
   "metadata": {},
   "source": [
    "**The following models follow the Command R + Setup on AWS Documentation and Modify the Code: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c22097-a1a9-4353-91d2-6366ddd3a21f",
   "metadata": {},
   "source": [
    "## Tokenize Model v.3.1 -> Modify Cohere AWS Documentation Template"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f723a96c-5e1f-405b-bb74-590c098421e0",
   "metadata": {},
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using a Cohere Command R model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating text with Cohere model %s\", model_id)\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Cohere Command R model %s\", model_id)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Cohere tokenization example.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "    model_id = DEFAULT_MODEL\n",
    "    text_to_tokenize = \"text to tokenize\"\n",
    "\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"message\": f\"Tokenize the following text using BPE encoding: {text_to_tokenize}\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0.6,\n",
    "            \"p\": 0.5,\n",
    "            \"k\": 250\n",
    "        })\n",
    "        response = generate_text(model_id=model_id, body=body)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        print(\"Tokenization result\\n-------------------\")\n",
    "        print(f\"Stop reason: {response_body['finish_reason']}\")\n",
    "        print(f\"Response text: \\n{response_body['text']}\")\n",
    "        \n",
    "        # Note: The exact format of the tokenization output may vary\n",
    "        # You might need to parse the response text to extract token information\n",
    "        \n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occurred: \" + format(message))\n",
    "    else:\n",
    "        print(f\"Finished tokenizing text with Cohere model {model_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949c34f-da65-4439-92fb-71238485af59",
   "metadata": {},
   "source": [
    "## Tokenize Model v.3.2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff5bc9e7-be4a-4cff-84b0-799abec1d626",
   "metadata": {},
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DEFAULT_MODEL = 'cohere.command-r-plus-v1:0'\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using a Cohere Command R model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating text with Cohere model %s\", model_id)\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Cohere Command R model %s\", model_id)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Cohere tokenization example.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "    model_id = DEFAULT_MODEL\n",
    "    text_to_tokenize = \"toothpaste\"\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"message\": f\"Tokenize the following text using BPE encoding and return the result in the format 'tokens=[token_ids] token_strings=[] meta=None': {text_to_tokenize}\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0,\n",
    "            \"p\": 0.99,\n",
    "            \"k\": 0\n",
    "        })\n",
    "        response = generate_text(model_id=model_id, body=body)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        print(\"Tokenization result\\n-------------------\")\n",
    "        print(f\"Stop reason: {response_body['finish_reason']}\")\n",
    "        print(f\"Response text: \\n{response_body['text']}\")\n",
    "        \n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occurred: \" + format(message))\n",
    "    else:\n",
    "        print(f\"Finished tokenizing text with Cohere model {model_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047095e-db96-4695-8634-d6786bfb2614",
   "metadata": {},
   "source": [
    "## Tokenize Model v.3.3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3305adb-a940-4127-aaee-c922ef22707c",
   "metadata": {},
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DEFAULT_MODEL = 'cohere.command-r-plus-v1:0'\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using a Cohere Command R model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating text with Cohere model %s\", model_id)\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Cohere Command R model %s\", model_id)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Cohere tokenization example.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "    model_id = DEFAULT_MODEL\n",
    "    text_to_tokenize = \"sample message\"\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"message\": f\"Tokenize the following text using BPE encoding and return the result in the format 'tokens=[token_ids] token_strings=[word_tokens] meta=None'. Only include whole words in token_strings: {text_to_tokenize}\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0,\n",
    "            \"p\": 0.1,\n",
    "            \"k\": 0\n",
    "        })\n",
    "        response = generate_text(model_id=model_id, body=body)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        print(\"Tokenization result\\n-------------------\")\n",
    "        print(f\"Stop reason: {response_body['finish_reason']}\")\n",
    "        print(f\"Response text: \\n{response_body['text']}\")\n",
    "        \n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occurred: \" + format(message))\n",
    "    else:\n",
    "        print(f\"Finished tokenizing text with Cohere model {model_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da980ff-52dd-4279-8b02-e84e14925a0b",
   "metadata": {},
   "source": [
    "## Tokenize Model v.3.4 [best so far]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "96fc5b80-9342-4d0e-ae39-34f28f30d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with Cohere model cohere.command-r-plus-v1:0\n",
      "INFO:__main__:Successfully generated text with Cohere Command R model cohere.command-r-plus-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization result\n",
      "-------------------\n",
      "Stop reason: COMPLETE\n",
      "Response text: \n",
      "tokens=[0 2472 4660] token_strings=['<s>', 'sample', 'message', '</s>'] meta=None\n",
      "Finished tokenizing text with Cohere model cohere.command-r-plus-v1:0.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DEFAULT_MODEL = 'cohere.command-r-plus-v1:0'\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using a Cohere Command R model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating text with Cohere model %s\", model_id)\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Cohere Command R model %s\", model_id)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Cohere tokenization example.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "    model_id = DEFAULT_MODEL\n",
    "    text_to_tokenize = \"sample message\"\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"message\": f\"For the text '{text_to_tokenize}', return exactly this tokenization result: tokens=[token ids] token_strings=[word_tokens] meta=None\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0,\n",
    "            \"p\": 0.99,\n",
    "            \"k\": 0\n",
    "        })\n",
    "        response = generate_text(model_id=model_id, body=body)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        print(\"Tokenization result\\n-------------------\")\n",
    "        print(f\"Stop reason: {response_body['finish_reason']}\")\n",
    "        print(f\"Response text: \\n{response_body['text']}\")\n",
    "        \n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occurred: \" + format(message))\n",
    "    else:\n",
    "        print(f\"Finished tokenizing text with Cohere model {model_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e895354-367b-4234-86bf-8b85734a8c6c",
   "metadata": {},
   "source": [
    "## Detokenize Model v.3.1 [best so far]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5328ee82-0531-4a70-a592-d36bbc0a8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with Cohere model cohere.command-r-plus-v1:0\n",
      "INFO:__main__:Successfully generated text with Cohere Command R model cohere.command-r-plus-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization result\n",
      "-------------------\n",
      "Stop reason: COMPLETE\n",
      "Response text: \n",
      "detokenized_words=['22429 6680'] tokens=[22429, 6680] meta=None\n",
      "Finished tokenizing text with Cohere model cohere.command-r-plus-v1:0.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "DEFAULT_MODEL = 'cohere.command-r-plus-v1:0'\n",
    "\n",
    "def generate_text(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text using a Cohere Command R model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The request body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating text with Cohere model %s\", model_id)\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Successfully generated text with Cohere Command R model %s\", model_id)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for Cohere tokenization example.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "    model_id = DEFAULT_MODEL\n",
    "    tokens_to_detokenize = \"tokens=[22429, 6680] token_strings=[] meta=None\"\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"message\": f\"For the text '{text_to_tokenize}', detokenization this tokens= meta=None\",\n",
    "            \"message\": f\"For the list of integers '{tokens_to_detokenize}', return exactly this detokenization result: detokenized_words=[string] tokens=[token ids] meta=None\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0,\n",
    "            \"p\": 0.99,\n",
    "            \"k\": 0\n",
    "        })\n",
    "        response = generate_text(model_id=model_id, body=body)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        print(\"Tokenization result\\n-------------------\")\n",
    "        print(f\"Stop reason: {response_body['finish_reason']}\")\n",
    "        print(f\"Response text: \\n{response_body['text']}\")\n",
    "        \n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occurred: \" + format(message))\n",
    "    else:\n",
    "        print(f\"Finished tokenizing text with Cohere model {model_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39442c3-aebd-436a-a522-46a2e2654873",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It's evident from our recent attempts to tokenize text on Bedrock using Cohere that there's no direct 1-to-1 mapping between the specific BPE integer values words correlate to. Both Bedrock and Cohere possess underlying dictionaries that specific words correlate to when strings are parsed. This situation creates challenges when attempting to correlate these dictionaries through a straightforward program, as demonstrated in our code.\n",
    "\n",
    "The problem is exacerbated by Bedrock's lack of a tokenization/detokenization API endpoint, making it difficult to access or reverse the exact tokenization process used by the model.\n",
    "To address these issues and improve the interoperability of different NLP models and platforms, several avenues should be explored:\n",
    "\n",
    "1. Development of a Tokenization API for Bedrock: AWS could implement a specific tokenization API endpoint for Bedrock, allowing developers to access the exact tokenization process used by different models, including Cohere.\n",
    "2. Mapping Between Model Providers: A comprehensive study and mapping of tokenization processes across various model providers could help create a standardized approach or translation layer between different tokenization schemes.\n",
    "3. Enhanced Documentation: More detailed documentation on how Bedrock implements and potentially modifies the tokenization processes of its hosted models would be beneficial.\n",
    "4. Client-Side Tokenization Libraries: Development of client-side libraries that accurately mimic the tokenization processes of different models could provide a workaround for the lack of server-side tokenization APIs. This may already exist and I was unable to find it.\n",
    "\n",
    "Improving tokenization allows for greater interoperability between different NLP models on Bedrock, ensuring more consistent and accurate tokenization across various implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
