{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz33G3t6gbOl"
   },
   "source": [
    "# Cohere's Multilingual Models on AWS\n",
    "\n",
    "In today's globalized world, the ability to understand and process multiple languages is becoming increasingly important. [Cohere](https://cohere.com/) has developed a suite of multilingual models designed to tackle this challenge. In this notebook, we'll explore three of Cohere's multilingual models and their potential applications.\n",
    "\n",
    "## Why Multilingual Models Matter\n",
    "\n",
    "Language is a fundamental aspect of human communication and cultural expression. As businesses and organizations expand their reach across borders, the need for effective multilingual solutions becomes paramount. Multilingual models enable seamless communication, bridging language barriers and facilitating cross-cultural understanding.\n",
    "\n",
    "[Cohere's multilingual models](https://aws.amazon.com/marketplace/seller-profile?id=87af0c85-6cf9-4ed8-bee0-b40ce65167e0) offer several benefits:\n",
    "\n",
    "1. **Broad Language Coverage**: Cohere's models support a wide range of languages, allowing you to process and generate content in multiple languages simultaneously.\n",
    "\n",
    "2. **Improved Accuracy**: By training on diverse language data, these models can better capture nuances, idioms, and cultural contexts, resulting in more accurate translations and language processing.\n",
    "\n",
    "3. **Scalability**: With a single multilingual model, you can address language needs across multiple regions, reducing the need for maintaining separate models for each language.\n",
    "\n",
    "4. **Cost-Efficiency**: Deploying a single multilingual model can be more cost-effective than maintaining multiple monolingual models, especially for organizations with global operations.\n",
    "\n",
    "## The Models\n",
    "\n",
    "In this notebook, we'll be working with three of Cohere's multilingual models:\n",
    "\n",
    "1. **Cohere Command R+**: A powerful Large Language Model (LLM) capable of understanding and generating text in multiple languages.\n",
    "\n",
    "2. **Cohere Embed Multilingual V3**: An embedding model designed to encode text from various languages into dense vector representations, enabling efficient similarity comparisons and semantic search.\n",
    "\n",
    "3. **Cohere Rerank Multilingual V3**: A reranker model used in conjunction with the Retrieval-Augmented Generation (RAG) approach, which enhances the relevance and accuracy of generated text by leveraging retrieved information from a knowledge base.\n",
    "\n",
    "Throughout this notebook, we'll explore practical examples and use cases for these models, showcasing their capabilities in areas such as multilingual content generation, and information retrieval.\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "![MultilingualRAGArchitecture](images/MultilingualRAGArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "Use kernel either `conda_python3`, `conda_pytorch_p310` or `conda_tensorflow2_p310`.\n",
    "\n",
    "<b>Follow the steps below if you are running the notebook in your owned AWS enviornment.<br>If running through the workshop studio, SKIP the steps below<b>\n",
    "    \n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "2. To deploy a reranker model from Cohere, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        2. **aws-marketplace:Unsubscribe**\n",
    "        3. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [cohere-rerank-multilingual](https://aws.amazon.com/marketplace/pp/prodview-ydysc72qticsw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Here, we will install all the required dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H787BXXYvD0a",
    "outputId": "04ef5e04-7760-4d40-deeb-663536b38f20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wikipedia==1.4.0 --quiet\n",
    "!pip install cohere-aws==0.8.16 --quiet\n",
    "!pip install faiss-cpu==1.8.0 --quiet\n",
    "!pip install langchain-text-splitters==0.2.2 --quiet\n",
    "!pip install numpy==1.26.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from cohere_aws import Client\n",
    "import faiss\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "import re\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSB0pnt0gbOo"
   },
   "source": [
    "## Step 2 - Downloading dataset\n",
    "\n",
    "To get started, we'll leverage data from wikipedia to answer questions about the diverse culinary traditions across different countries and languages. Our goal is to showcase Cohere's multilingual capabilities by crawling wikipedia to get cuisine information of the United States, Mexico, and Germany from their respective language editions of Wikipedia. \n",
    "\n",
    "By retrieving and analyzing data from these multilingual sources, we can gain insights into the unique flavors, ingredients, and cultural influences that shape the food heritage of each nation. This approach not only allows us to explore the richness of global cuisine but also demonstrates the power of multilingual language models in accessing and processing information from diverse linguistic sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USA - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# English\n",
    "wikipedia.set_lang('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP-bWt9XgbOq",
    "outputId": "72276fb2-0d6b-415d-af74-452a013ae84b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we'll get some wikipedia data\n",
    "article = wikipedia.page('American Cuisine')\n",
    "en_text = article.content\n",
    "print(f\"The text has roughly {len(en_text.split())} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(en_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mexico - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spanish\n",
    "wikipedia.set_lang('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we'll get some wikipedia data\n",
    "article = wikipedia.page('Gastronomia de Mexico')\n",
    "es_text = article.content\n",
    "print(f\"The text has roughly {len(es_text.split())} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(es_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Germany - German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# German\n",
    "wikipedia.set_lang('de') # Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we'll get some wikipedia data\n",
    "article = wikipedia.page('Deutsches Essen')\n",
    "de_text = article.content\n",
    "print(f\"The text has roughly {len(de_text.split())} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(de_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1aJ7hKGgbOr"
   },
   "source": [
    "## Step 3 - Vector Indexing\n",
    "\n",
    "We index the document in an open-source vectorstore called FAISS. This requires chunking the documents, creating embeddings, and indexing them into FAISS.\n",
    "\n",
    "To efficiently store and retrieve relevant information from our multilingual data, we will leverage [FAISS](https://ai.meta.com/tools/faiss/) (Facebook AI Similarity Search), an open-source library for efficient similarity search and clustering of dense vectors. FAISS allows us to index and search large collections of embeddings, enabling quick retrieval of the most relevant documents or passages based on their vector representations. \n",
    "\n",
    "Our process involves chunking the retrieved documents into smaller, more manageable segments, creating dense vector embeddings for each chunk using Cohere's Embed Multilingual V3, and indexing these embeddings into FAISS. \n",
    "\n",
    "By utilizing FAISS's powerful similarity search capabilities, we can quickly identify the most relevant chunks of information based on their semantic similarity to a given query or context. This approach not only facilitates efficient information retrieval but also unlocks the potential for advanced applications such as semantic search, question answering, and knowledge base construction from multilingual sources. FAISS's scalability and performance make it an ideal choice for handling large volumes of embeddings, ensuring responsive and accurate results even with extensive multilingual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhXW7iHC1-Q6",
    "outputId": "d68ac348-4b73-4c6a-a445-6c510bdb0881",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For chunking let's use langchain to help us split the text\n",
    "def get_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512, # Limiting chunk size for embedding model\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Split the text into chunks with some overlap\n",
    "    chunks_ = text_splitter.create_documents([text])\n",
    "    chunks = [c.page_content for c in chunks_]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USA - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_chunks = get_chunks(en_text)\n",
    "print(f\"The text has been broken down in {len(en_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mexico - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "es_chunks = get_chunks(es_text)\n",
    "print(f\"The text has been broken down in {len(es_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Germany - German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "de_chunks = get_chunks(de_text)\n",
    "print(f\"The text has been broken down in {len(de_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8g0sE2hgbOs"
   },
   "source": [
    "### Create embeddings for every text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    texts,\n",
    "    batch_size=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert text into embeddings.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        texts (list) : The texts to send to the embedding model.\n",
    "        batch_size (int): Batch size to limit the number of chunks sent to the embedding model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The embeddings that the model generated.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        body = {\n",
    "            \"texts\": batch_texts,\n",
    "            \"input_type\": \"search_document\"\n",
    "        }\n",
    "        # Send the message.\n",
    "        response = bedrock_client.invoke_model(modelId=model_id, body=json.dumps(body))\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        embeddings.extend(response_body[\"embeddings\"])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define bedrock client\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USA - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_embeddings = generate_embeddings(bedrock_client, \"cohere.embed-multilingual-v3\", en_chunks)\n",
    "print(f\"We just computed {len(en_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mexico - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_embeddings = generate_embeddings(bedrock_client, \"cohere.embed-multilingual-v3\", es_chunks)\n",
    "print(f\"We just computed {len(es_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Germany - German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_embeddings = generate_embeddings(bedrock_client, \"cohere.embed-multilingual-v3\", de_chunks)\n",
    "print(f\"We just computed {len(de_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM6vKeypgbOs"
   },
   "source": [
    "### Store embeddings into FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USA - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an index\n",
    "en_vectorstore = faiss.IndexFlatL2(1024)\n",
    "\n",
    "# Add data to the index\n",
    "en_vectorstore.add(np.array(en_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mexico - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an index\n",
    "es_vectorstore = faiss.IndexFlatL2(1024)\n",
    "\n",
    "# Add data to the index\n",
    "es_vectorstore.add(np.array(es_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Germany - German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an index\n",
    "de_vectorstore = faiss.IndexFlatL2(1024)\n",
    "\n",
    "# Add data to the index\n",
    "de_vectorstore.add(np.array(de_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6NGVurZgbOs"
   },
   "source": [
    "## Step 4: RAG: Retrieve relevant chunks from the vector database\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a powerful approach that combines the strengths of retrieval and generation models for natural language processing tasks. In this approach, we first utilize an embedding model to identify relevant information from FAISS based on the input query. The retrieved information is then incorporated into a large language model, like Cohere's Command R+, which uses this additional context to generate more informed and accurate outputs. By leveraging the vast knowledge contained in multilingual sources, RAG enables us to produce high-quality, factual responses that go beyond the model's initial training data. This approach is particularly valuable for tasks like question answering, where relevant external knowledge can significantly improve the quality and completeness of the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility function for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(vectorstore, question, chunks, k=4):\n",
    "    # Embed the user question\n",
    "    query = generate_embeddings(bedrock_client, \"cohere.embed-multilingual-v3\", [question])\n",
    "    \n",
    "    # Retrieve the top K indices from the vector database\n",
    "    D, top_indices = vectorstore.search(np.array(query), k)\n",
    "    \n",
    "    top_indices.sort()\n",
    "    \n",
    "    # Retrieve the top K most similar chunks\n",
    "    top_chunks_after_retrieval = [re.sub(r\"\\t+|\\n+\", \"\", chunks[i]) for i in top_indices[0]]\n",
    "    \n",
    "    return top_chunks_after_retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility function for conversation with Bedrock converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history=[],\n",
    "    temperature=0.3,\n",
    "    max_tokens=400,\n",
    "    top_p=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompt (str) : The system prompt for the model to use.\n",
    "        prompt (str) : The message/question to send to the model.\n",
    "        chat_history (list): The chat history from user and assistant.\n",
    "\n",
    "    Returns:\n",
    "        response (str): The text generated output from the model.\n",
    "        chat_history (str): The full conversation between user and assistant that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompts = [\n",
    "        {\n",
    "            \"text\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    chat_history.extend(messages)\n",
    "\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"topP\": top_p,\n",
    "    }\n",
    "\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    chat_history.append(response[\"output\"][\"message\"])\n",
    "\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"], chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the system prompt and guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an AI assistant with expertise in American cuisine. Your knowledge is based solely on the information provided between the <documents> and </documents> tags.\n",
    "\n",
    "Before answering any questions, first check if the user has provided information between the <documents> and </documents> tags. If no information is provided, respond with the following JSON:\n",
    "\n",
    "{\n",
    "    \"answer\": \"I do not have enough information to answer that question.\"\n",
    "}\n",
    "\n",
    "If documents are provided, your task is to answer questions accurately and concisely, using only the details from the given documents. Do not use your own knowledge or any external sources to answer the questions, even if you know the answer.\n",
    "\n",
    "If a question cannot be fully answered using the provided documents, respond with the following JSON:\n",
    "\n",
    "{\n",
    "    \"answer\": \"I do not have enough information to answer that question.\"\n",
    "}\n",
    "\n",
    "All responses must be in valid JSON format, with the 'answer' key containing the actual response text.\n",
    "\n",
    "To provide transparency, include your reasoning process with the 'thinking' key as the following format:\n",
    "\n",
    "{\n",
    "    \"answer\": \"Your response here\",\n",
    "    \"thinking\": \"Your reasoning process here\"\n",
    "}\n",
    "\n",
    "Be concise and objective in your responses, without any personal opinions or subjective statements.\n",
    "\"\"\"\n",
    "prompt_template = \"<documents>\\n{documents}\\n</documents>\\n\\nQuestion: {question}\\nThink step-by-step.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model ID parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"cohere.command-r-plus-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2HTxspKgbOs"
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"What are some popular regional dishes in American cuisine?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(documents=\"\", question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieved top K most relevant chunks from FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting number of nearest neighbours\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USA - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_chunks_after_retrieval = rag(en_vectorstore, question, en_chunks, k)\n",
    "\n",
    "print(f\"Here are the top {k} chunks after retrieval: \")\n",
    "for t in top_chunks_after_retrieval:\n",
    "    print(\"== \" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "prompt = prompt_template.format(documents=top_chunks_after_retrieval, question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mexico - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"¿Cuáles son algunos platos regionales populares en la cocina mexicana?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_chunks_after_retrieval = rag(es_vectorstore, question, es_chunks, k)\n",
    "\n",
    "print(f\"Here are the top {k} chunks after retrieval: \")\n",
    "for t in top_chunks_after_retrieval:\n",
    "    print(\"== \" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(documents=top_chunks_after_retrieval, question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Germany - German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"Was sind einige beliebte regionale Gerichte der deutschen Küche?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_chunks_after_retrieval = rag(de_vectorstore, question, de_chunks, k)\n",
    "\n",
    "print(f\"Here are the top {k} chunks after retrieval: \")\n",
    "for t in top_chunks_after_retrieval:\n",
    "    print(\"== \" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(documents=top_chunks_after_retrieval, question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzcpds3VgbOt"
   },
   "source": [
    "## Step 5 - RAG: Rerank the chunks retrieved from the vector database\n",
    "\n",
    "Building upon the power of Retrieval-Augmented Generation (RAG), we can further enhance the relevance and accuracy of our generated outputs by incorporating a reranker model. In this approach, we first retrieve a set of potentially relevant information from our knowledge base using a retrieval model like Cohere's Embed Multilingual V3. These retrieved pieces of information are then reranked using a specialized reranker model, such as Cohere's Rerank Multilingual V3, which scores each retrieved item based on its relevance to the input query or context. The top-ranked items are then passed to the generative language model, Cohere's Command R+, which generates the final output while considering this highly relevant contextual information. By incorporating a reranker, we can effectively filter out irrelevant or tangential information, ensuring that the generated responses are focused, coherent, and directly address the specific query or context at hand.\n",
    "\n",
    "Below we will define the Cohere Rerank endpoint which is hosted on Sagemaker. There is an option to create the endpoint or use an existing endpoint. By default it will create the Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create cohere client\n",
    "# TODO: can we hardcode the region?\n",
    "co = Client(region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: is this the name that we will use for workshop by default\n",
    "endpoint_name = \"cohere-rerank-multilingual-v3-0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute cell if using workshop studio to run the notebook\n",
    "#Connect to Rerank Endpoint\n",
    "co.connect_to_endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create the Rerank endpoint in SageMaker. <br>\n",
    "SKIP the step below if using workshop studio to run the notebook.<br>\n",
    "Execute cell only if using your own AWS account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKIP this cell if using workshop studio to run the notebook\n",
    "#Execute cell only if using your own AWs account\n",
    "\n",
    "#Enter the model ARN for the model_package_arn fields if running the notebook from your own AWS account\n",
    "model_package_arn = \"<model_ARN_when subscribed_to_model_through_marketplace>\"\n",
    "#If instance type differs from the one below, modify it\n",
    "co.create_endpoint(arn=model_package_arn, endpoint_name=endpoint_name, instance_type=\"ml.g5.xlarge\", n_instances=1)\n",
    "co.connect_to_endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility function for RAG with Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_with_reranker(vectorstore, question, chunks, k=4, top_n=3):\n",
    "    # Embed the user question\n",
    "    query = generate_embeddings(bedrock_client, \"cohere.embed-multilingual-v3\", [question])\n",
    "    \n",
    "    # Retrieve the top K indices from the vector database\n",
    "    D, top_indices = vectorstore.search(np.array(query), k)\n",
    "    \n",
    "    top_indices.sort()\n",
    "    \n",
    "    # Retrieve the top K most similar chunks\n",
    "    top_chunks_after_retrieval = [re.sub(r\"\\t+|\\n+\", \"\", chunks[i]) for i in top_indices[0]]\n",
    "    \n",
    "    print (\"Here are the top 3 chunks out of the 10 before rerank:\")\n",
    "    cnt=0\n",
    "    for t in top_chunks_after_retrieval:\n",
    "        cnt+=1\n",
    "        if (cnt < 4):\n",
    "            print(f\"== {t}\")\n",
    "    \n",
    "    response = co.rerank(\n",
    "        query=question,\n",
    "        documents=top_chunks_after_retrieval,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    top_chunks_after_rerank = [result.document['text'] for result in response]  \n",
    "    return top_chunks_after_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieved top N most relevant chunks after retrieval from FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting number of nearest neighbours\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting number of docs reranked from top K results\n",
    "top_n = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USA - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"What are some popular regional dishes in American cuisine?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We increase the number of returned docs from the retriever to rerank them and increase\n",
    "# the relevant information fed to the large language model\n",
    "top_chunks_after_rerank = rag_with_reranker(en_vectorstore, question, en_chunks, k, top_n)\n",
    "\n",
    "print(\"\\nHere are the top 3 chunks after rerank: \")\n",
    "for t in top_chunks_after_rerank:\n",
    "    print(\"== \" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(documents=top_chunks_after_rerank, question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mexico - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"¿Cuáles son algunos platos regionales populares en la cocina mexicana?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We increase the number of returned docs from the retriever to rerank them and increase\n",
    "# the relevant information fed to the large language model\n",
    "top_chunks_after_rerank = rag_with_reranker(es_vectorstore, question, es_chunks, k, top_n)\n",
    "\n",
    "print(\"\\nHere are the top 3 chunks after rerank: \")\n",
    "for t in top_chunks_after_rerank:\n",
    "    print(\"== \" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(documents=top_chunks_after_rerank, question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Germany - German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question = \"Was sind einige beliebte regionale Gerichte der deutschen Küche?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We increase the number of returned docs from the retriever to rerank them and increase\n",
    "# the relevant information fed to the large language model\n",
    "top_chunks_after_rerank = rag_with_reranker(de_vectorstore, question, de_chunks, k, top_n)\n",
    "\n",
    "print(\"\\nHere are the top 3 chunks after rerank: \")\n",
    "for t in top_chunks_after_rerank:\n",
    "    print(\"== \" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(documents=top_chunks_after_rerank, question=question)\n",
    "\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "If the endpoint was created by the execution of this notebook, then make sure to delete the endpoint after completion to avoid charges. Skip the below step if you are using workshop studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete the endpoint\n",
    "# Skip this step if using workshop studio\n",
    "\n",
    "co.delete_endpoint()\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored Cohere's multilingual models, including Command R+, Embed Multilingual V3, and Rerank Multilingual V3 for reranking retrieved information, RAG significantly enhances the relevance and accuracy of generated content.\n",
    "\n",
    "The reranking step, in particular, proved invaluable in improving the quality of generated text by incorporating relevant information from a knowledge base with support for multiple languages. This approach not only ensures factual accuracy but also provides context-specific responses tailored to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
