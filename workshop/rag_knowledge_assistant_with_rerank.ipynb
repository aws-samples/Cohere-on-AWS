{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea31db8-a39e-4667-90e9-b7e5edbfa51a",
   "metadata": {},
   "source": [
    "# Build Retrieval Augmented Generation Intelligent Query Agent\n",
    "In the following use case example, we’ll showcase how Cohere’s Generate, Embed, and Reranker model can search and\n",
    "query across a repository of financial reports and articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe5065",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "Use kernel either `conda_python3`, `conda_pytorch_p310` or `conda_tensorflow2_p310`.\n",
    "\n",
    "<b>Follow the steps below if you are running the notebook in your owned AWS enviornment.<br>If running through the workshop studio, SKIP the steps below<b>\n",
    "    \n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "2. To deploy a reranker model from Cohere, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        2. **aws-marketplace:Unsubscribe**\n",
    "        3. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [cohere-rerank-multilingual](https://aws.amazon.com/marketplace/pp/prodview-ydysc72qticsw)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c504f-0886-4b29-8716-0a47e6450647",
   "metadata": {},
   "source": [
    "### Step 1: Install Packages, Import Modules, and Create Model Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2388da",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade cohere-aws cohere hnswlib pandas numpy markdown boto3 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd64d4b",
   "metadata": {},
   "source": [
    "### Step 2: Import, create endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54602a-944d-43c4-a611-6f8f57285618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cohere_aws\n",
    "import cohere\n",
    "import hnswlib\n",
    "import warnings\n",
    "import boto3\n",
    "import json\n",
    "import markdown as md\n",
    "from IPython.display import HTML\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e7a18",
   "metadata": {},
   "source": [
    "#### Connect to Command, Embed and Rerank endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to Bedrock endpoint for Command\n",
    "co_chat = cohere_aws.Client(mode=cohere_aws.Mode.BEDROCK)\n",
    "model_id_chat = \"cohere.command-r-plus-v1:0\"\n",
    "\n",
    "#connect to Bedrock endpoint for embed\n",
    "co_em = cohere_aws.Client(mode=cohere_aws.Mode.BEDROCK)\n",
    "model_id_em = \"cohere.embed-multilingual-v3\"\n",
    "\n",
    "#Define sagemaker endpoint for Rerank\n",
    "endpoint_name = \"cohere-rerank-multilingual-v3-0\" \n",
    "co_rk = cohere_aws.Client(region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe64ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute cell if using workshop studio to run the notebook\n",
    "#Connect to Rerank Endpoint\n",
    "# TODO: region and name does that need to change?\n",
    "co_rk.connect_to_endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177837bd",
   "metadata": {},
   "source": [
    "Below we will create the Rerank endpoint in SageMaker. <br>\n",
    "SKIP the step below if using workshop studio to run the notebook.<br>\n",
    "Execute cell only if using your own AWS account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKIP this cell if using workshop studio to run the notebook\n",
    "#Execute cell only if using your own AWs account\n",
    "\n",
    "#Enter the model ARN for the model_package_arn fields if running the notebook from your own AWS account\n",
    "model_package_arn = \"<model_ARN_when subscribed_to_model_through_marketplace>\"\n",
    "#If instance type differs from the one below, modify it\n",
    "co_rk.create_endpoint(arn=model_package_arn, endpoint_name=endpoint_name, instance_type=\"ml.g5.xlarge\", n_instances=1)\n",
    "co_rk.connect_to_endpoint(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3095b6d",
   "metadata": {},
   "source": [
    "### Step 3: Retrieval\n",
    "\n",
    "In the retrieval step, the model aims to gather relevant information from an external source or database. This step is facilitated by a retrieval model (Embed+Rerank). The goal is to retrieve contextually appropriate and factually accurate data that can be used to inform the generation process. The retrieval model employs techniques such as semantic matching to identify and extract relevant passages or documents from the given source.\n",
    "\n",
    "![Retrival](images/Retrieval.png)\n",
    "<br>\n",
    "#### Import data set and split them into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed29d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "def iterate_bucket_items(bucket):\n",
    "    text = []\n",
    "    names = []\n",
    "\n",
    "\n",
    "    result = s3.list_objects(Bucket = bucket)\n",
    "    for o in result.get('Contents'):\n",
    "        data = s3.get_object(Bucket=bucket, Key=o.get('Key'))\n",
    "        text.append(str(data['Body'].read()))\n",
    "        names.append(str(o.get('Key'))),\n",
    "    return text, names\n",
    "\n",
    "\n",
    "bucket = \"finserv-analyst-reports\"\n",
    "text, names = iterate_bucket_items(bucket)\n",
    "df = pd.DataFrame({'text': text, 'name': names})\n",
    "\n",
    "# Read the data\n",
    "print(f'Total number of financial reports imported : {len(df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split each article into chunks of n words or less return a list of tuples (chunk, original text) with an overlap of m words\n",
    "def split_text(text,name, n, m=0):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), n-m):\n",
    "        chunk = ' '.join(words[i:i+n])\n",
    "        chunks.append((chunk, text, name))\n",
    "    return chunks\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "chunks = []\n",
    "for i, row in df.iterrows():\n",
    "    chunks.extend(split_text(row['text'], row['name'],150, 15))\n",
    "df = pd.DataFrame(chunks, columns=['chunk', 'original_text', 'name'])\n",
    "print(f'The reports were split into a total number of {len(df)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f9061-96a4-4d22-af35-8c2f9d9ae19a",
   "metadata": {},
   "source": [
    "#### Bulk Embed and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa6476-9cff-4af0-9d03-50213d72a2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "# def embed_docs():\n",
    "doc_embs =[]\n",
    "docs = df['chunk'].to_list()\n",
    "input_type = 'search_document'\n",
    "\n",
    "batch_size=96\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "\n",
    "    batch_docs = docs[i:i+batch_size]\n",
    "    response = co_em.embed(\n",
    "        texts=batch_docs, \n",
    "        input_type=input_type, \n",
    "        model_id=model_id_em\n",
    "    )\n",
    "    doc_embs.extend(response.embeddings)\n",
    "\n",
    "# doc_embs, docs = embed_docs()\n",
    "print(f'Total Documents embedded : {len(docs)}')    \n",
    "        \n",
    "#     return doc_embs, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51309858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a search index with hnswlib, a library for fast approximate nearest neighbor search\n",
    "\n",
    "index = hnswlib.Index(space='ip', dim=1024) # Cohere embed outputs embeddings with 1024 dimensions\n",
    "index.init_index(max_elements=len(doc_embs), ef_construction=1024, M=64) # For more info: https://github.com/nmslib/hnswlib#api-description\n",
    "index.add_items(doc_embs, list(range(len(doc_embs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924bd1d-930a-4624-8ad6-cc0eef8c5b07",
   "metadata": {},
   "source": [
    "#### Query the Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ace640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(query, chat):\n",
    "    docs = df['chunk'].to_list()\n",
    "\n",
    "    # Embed query and retrieve results\n",
    "    query_emb = co_em.embed(texts=[query], model_id=model_id_em, input_type=\"search_query\").embeddings\n",
    "    \n",
    "    doc_ids = index.knn_query(query_emb, k=15)[0][0] # we will retrieve 15 closest neighbors\n",
    "\n",
    "    if not chat:\n",
    "        print(f\"DOCUMENT IDs returned => {doc_ids} \\n\" )\n",
    "        print(f\"-> QUERY: '{query.upper()}' \\n\")\n",
    "        print(\"-> Printing first 2 Results out of the total: \\n\")\n",
    "\n",
    "    retrieved_docs = []\n",
    "    \n",
    "    for doc_id in doc_ids:\n",
    "        # Append results\n",
    "        retrieved_docs.append(docs[doc_id])\n",
    "\n",
    "    # Print results\n",
    "    if not chat:\n",
    "        for doc_id in doc_ids[:2]:\n",
    "            print(\"\\t{}\\t{}\".format(doc_id, docs[doc_id].replace(\"\\n\", \" \")))\n",
    "        \n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense retrieval does return the search results. However the most relevant results may not always be at \n",
    "# the top of the retrieved dataset\n",
    "\n",
    "query = 'Impact of cash ban on the Indian economy'\n",
    "retrieved_docs = retrieval(query, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5605c59-0ef4-48f5-8dc9-4cbc7b95ec5d",
   "metadata": {},
   "source": [
    "#### Improve Results with Cohere Rerank\n",
    "\n",
    "The following query is not returning the most relevant result at the top, here is where Rerank will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_retrieval(query, chat):\n",
    "    results = co_rk.rerank(query=query,documents=retrieved_docs, top_n=5)\n",
    "    re_ranked_docs=[]\n",
    "    for idx, r in enumerate(results):\n",
    "        if not chat:\n",
    "            print(\"\\t{} was(#{})\\t{}\\t{}\".format(idx + 1,r.index,r.relevance_score,r.document[\"text\"].replace(\"\\n\", \" \")))\n",
    "        re_ranked_docs.append({\"text\": r.document['text']})\n",
    "    \n",
    "    return re_ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413bd42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "re_ranked_docs = rerank_retrieval(query, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194c2e3",
   "metadata": {},
   "source": [
    "### Step 4: Generation \n",
    "\n",
    "Once the relevant information has been retrieved, the generation step comes into play. This is where the generative (Command R+) model takes the retrieved context and generates a response. The generation model utilizes the provided input, along with the retrieved information, to create a coherent and informative output. It combines the factual data from the retrieval step with its language generation capabilities to produce a response that is both contextually appropriate and factually grounded.\n",
    "\n",
    "Bringing it all together - Using Cohere Command, Embed and Rerank<br>\n",
    "Generate query responses from the reranked document set; Guide the model to produce responses based on a preamble; Add citations for results produced\n",
    "\n",
    "![Generation](images/Generation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"You are a helpful digital assistant. \\nPlease answer questions only based on the contents of the documents. \\nIf the documents do not contain the answers to my question please say I dont know.\"\n",
    "print(preamble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the chatbot application.\n",
    "\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get the user message\n",
    "            message = input(\"User: \")\n",
    "\n",
    "            # Typing \"quit\" ends the conversation\n",
    "            if message.lower() == \"quit\":\n",
    "              print(\"Ending chat.\")\n",
    "              break\n",
    "            # Generate search queries (if any)\n",
    "            response = co_chat.chat(message=message,model_id=model_id_chat,search_queries_only=True)\n",
    "\n",
    "            # If there are search queries, retrieve document chunks and respond\n",
    "            if response.search_queries:\n",
    "                print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "                # Retrieve relevant document chunks for inputted query\n",
    "                retrieved_docs = retrieval(message,True)\n",
    "                # Rerank documents before passing the documents to the generative model\n",
    "                re_ranked_docs = rerank_retrieval(message,True)\n",
    "    \n",
    "                # Use reranked document chunks to respond\n",
    "                print (\"\\nUsing Re-ranked docs to respond...\")\n",
    "                response = co_chat.chat(\n",
    "                    message=message,\n",
    "                    model_id=model_id_chat,\n",
    "                    preamble = preamble,\n",
    "                    documents=re_ranked_docs,\n",
    "                    stream=True\n",
    "                )\n",
    "\n",
    "            # If there is no search query, directly respond\n",
    "            # No RAG\n",
    "            else:\n",
    "                response = co_chat.chat(\n",
    "                    message=message,\n",
    "                    model_id=model_id_chat,\n",
    "                    stream=True\n",
    "                )\n",
    "\n",
    "            # Print the chatbot response, citations, and documents\n",
    "            citations = []\n",
    "            cited_documents = []\n",
    "            cited_document_ids = []\n",
    "            unique_cited_document_ids = []\n",
    "\n",
    "            print(\"\\nRESPONSE:\")\n",
    "\n",
    "            # Display response\n",
    "            for event in response:\n",
    "                #print (event)\n",
    "                if event.event_type == \"text-generation\":\n",
    "                    print(event.text, end=\"\")\n",
    "                elif event.event_type == \"citation-generation\":\n",
    "                    citations.extend(event.citations)\n",
    "                elif event.event_type == \"search-results\":\n",
    "                    cited_documents = event.documents\n",
    "\n",
    "\n",
    "            # Display citations and source documents\n",
    "            if citations:\n",
    "                print(\"\\n\\nCITATIONS:\")\n",
    "                for citation in citations:\n",
    "                  print(citation)\n",
    "                  for ids in citation['document_ids']:\n",
    "                      cited_document_ids.append(ids)\n",
    "\n",
    "                unique_cited_document_ids = list(dict.fromkeys(cited_document_ids))\n",
    "\n",
    "                print(\"\\nDOCUMENTS:\")\n",
    "                for document in cited_documents:\n",
    "                  if document['id'] in unique_cited_document_ids:\n",
    "                   print(document)\n",
    "                   print(f\"\\n{'-'*100}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328899a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Chatbot class\n",
    "chatbot = Chatbot()\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dafbd9",
   "metadata": {},
   "source": [
    "### Step 5: Clean-up\n",
    "If the endpoint was created by the execution of this notebook, then make sure to delete the endpoint after completion to avoid charges. Skip the below step if you are using workshop studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d238cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete the endpoint\n",
    "# Skip this step if using workshop studio\n",
    "\n",
    "# co_rk.delete_endpoint()\n",
    "# co_rk.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca13d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
