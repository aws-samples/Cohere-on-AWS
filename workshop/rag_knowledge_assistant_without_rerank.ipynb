{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea31db8-a39e-4667-90e9-b7e5edbfa51a",
   "metadata": {},
   "source": [
    "# Build Retrieval Augmented Generation Intelligent Query Agent\n",
    "In the following use case example, we’ll showcase how Cohere’s Generate and Embed that can search and\n",
    "query across a repository of financial reports and articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe5065",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "Use kernel either `conda_python3`, `conda_pytorch_p310` or `conda_tensorflow2_p310`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c504f-0886-4b29-8716-0a47e6450647",
   "metadata": {},
   "source": [
    "### Step 1: Install Packages, Import Modules, and Create Model Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2388da",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade cohere-aws cohere hnswlib pandas numpy markdown boto3 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd64d4b",
   "metadata": {},
   "source": [
    "### Step 2: Import, create endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54602a-944d-43c4-a611-6f8f57285618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cohere_aws\n",
    "import cohere\n",
    "import hnswlib\n",
    "import warnings\n",
    "import boto3\n",
    "import json\n",
    "import markdown as md\n",
    "from IPython.display import HTML\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e7a18",
   "metadata": {},
   "source": [
    "#### Connect to Command, Embed and Rerank endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to Bedrock endpoint for Command\n",
    "co_chat = cohere_aws.Client(mode=cohere_aws.Mode.BEDROCK)\n",
    "model_id_chat = \"cohere.command-r-plus-v1:0\"\n",
    "\n",
    "#connect to Bedrock endpoint for embed\n",
    "co_em = cohere_aws.Client(mode=cohere_aws.Mode.BEDROCK)\n",
    "model_id_em = \"cohere.embed-multilingual-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3095b6d",
   "metadata": {},
   "source": [
    "### Step 3: Retrieval\n",
    "\n",
    "In the retrieval step, the model aims to gather relevant information from an external source or database. This step is facilitated by a retrieval model (Embed+Rerank). The goal is to retrieve contextually appropriate and factually accurate data that can be used to inform the generation process. The retrieval model employs techniques such as semantic matching to identify and extract relevant passages or documents from the given source.\n",
    "\n",
    "![Retrival](images/RetrievalNoRerank.png)\n",
    "<br>\n",
    "#### Import data set and split them into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed29d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "def iterate_bucket_items(bucket):\n",
    "    text = []\n",
    "    names = []\n",
    "\n",
    "\n",
    "    result = s3.list_objects(Bucket = bucket)\n",
    "    for o in result.get('Contents'):\n",
    "        data = s3.get_object(Bucket=bucket, Key=o.get('Key'))\n",
    "        text.append(str(data['Body'].read()))\n",
    "        names.append(str(o.get('Key'))),\n",
    "    return text, names\n",
    "\n",
    "\n",
    "bucket = \"finserv-analyst-reports\"\n",
    "text, names = iterate_bucket_items(bucket)\n",
    "df = pd.DataFrame({'text': text, 'name': names})\n",
    "\n",
    "# Read the data\n",
    "print(f'Total number of financial reports imported : {len(df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split each article into chunks of n words or less return a list of tuples (chunk, original text) with an overlap of m words\n",
    "def split_text(text,name, n, m=0):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), n-m):\n",
    "        chunk = ' '.join(words[i:i+n])\n",
    "        chunks.append((chunk, text, name))\n",
    "    return chunks\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "chunks = []\n",
    "for i, row in df.iterrows():\n",
    "    chunks.extend(split_text(row['text'], row['name'],150, 15))\n",
    "df = pd.DataFrame(chunks, columns=['chunk', 'original_text', 'name'])\n",
    "print(f'The reports were split into a total number of {len(df)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f9061-96a4-4d22-af35-8c2f9d9ae19a",
   "metadata": {},
   "source": [
    "#### Bulk Embed and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa6476-9cff-4af0-9d03-50213d72a2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "# def embed_docs():\n",
    "doc_embs =[]\n",
    "docs = df['chunk'].to_list()\n",
    "input_type = 'search_document'\n",
    "\n",
    "batch_size=96\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "\n",
    "    batch_docs = docs[i:i+batch_size]\n",
    "    response = co_em.embed(\n",
    "        texts=batch_docs, \n",
    "        input_type=input_type, \n",
    "        model_id=model_id_em\n",
    "    )\n",
    "    doc_embs.extend(response.embeddings)\n",
    "\n",
    "# doc_embs, docs = embed_docs()\n",
    "print(f'Total Documents embedded : {len(docs)}')    \n",
    "        \n",
    "#     return doc_embs, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51309858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a search index with hnswlib, a library for fast approximate nearest neighbor search\n",
    "\n",
    "index = hnswlib.Index(space='ip', dim=1024) # Cohere embed outputs embeddings with 1024 dimensions\n",
    "index.init_index(max_elements=len(doc_embs), ef_construction=1024, M=64) # For more info: https://github.com/nmslib/hnswlib#api-description\n",
    "index.add_items(doc_embs, list(range(len(doc_embs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924bd1d-930a-4624-8ad6-cc0eef8c5b07",
   "metadata": {},
   "source": [
    "#### Query the Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ace640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(query, chat):\n",
    "    docs = df['chunk'].to_list()\n",
    "\n",
    "    # Embed query and retrieve results\n",
    "    query_emb = co_em.embed(texts=[query], model_id=model_id_em, input_type=\"search_query\").embeddings\n",
    "    \n",
    "    doc_ids = index.knn_query(query_emb, k=15)[0][0] # we will retrieve 15 closest neighbors\n",
    "\n",
    "    if not chat:\n",
    "        print(f\"DOCUMENT IDs returned => {doc_ids} \\n\" )\n",
    "        print(f\"-> QUERY: '{query.upper()}' \\n\")\n",
    "        print(\"-> Printing first 2 Results out of the total: \\n\")\n",
    "\n",
    "    retrieved_docs = []\n",
    "    \n",
    "    for doc_id in doc_ids:\n",
    "        # Append results\n",
    "        retrieved_docs.append(docs[doc_id])\n",
    "\n",
    "    # Print results\n",
    "    if not chat:\n",
    "        for doc_id in doc_ids[:2]:\n",
    "            print(\"\\t{}\\t{}\".format(doc_id, docs[doc_id].replace(\"\\n\", \" \")))\n",
    "        \n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense retrieval does return the search results. However the most relevant results may not always be at \n",
    "# the top of the retrieved dataset\n",
    "\n",
    "query = 'Impact of cash ban on the Indian economy'\n",
    "retrieved_docs = retrieval(query, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194c2e3",
   "metadata": {},
   "source": [
    "### Step 4: Generation \n",
    "\n",
    "Once the relevant information has been retrieved, the generation step comes into play. This is where the generative (Command R+) model takes the retrieved context and generates a response. The generation model utilizes the provided input, along with the retrieved information, to create a coherent and informative output. It combines the factual data from the retrieval step with its language generation capabilities to produce a response that is both contextually appropriate and factually grounded.\n",
    "\n",
    "Bringing it all together - Using Cohere Command and Embed <br>\n",
    "Generate query responses from the reranked document set; Guide the model to produce responses based on a preamble; Add citations for results produced\n",
    "\n",
    "![Generation](images/GenerationNoRerank.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"You are a helpful digital assistant. \\nPlease answer questions only based on the contents of the documents. \\nIf the documents do not contain the answers to my question please say I dont know.\"\n",
    "print(preamble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the chatbot application.\n",
    "\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get the user message\n",
    "            message = input(\"User: \")\n",
    "\n",
    "            # Typing \"quit\" ends the conversation\n",
    "            if message.lower() == \"quit\":\n",
    "              print(\"Ending chat.\")\n",
    "              break\n",
    "            # Generate search queries (if any)\n",
    "            response = co_chat.chat(message=message,model_id=model_id_chat,search_queries_only=True)\n",
    "\n",
    "            # If there are search queries, retrieve document chunks and respond\n",
    "            if response.search_queries:\n",
    "                print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "                # Retrieve relevant document chunks for inputted query\n",
    "                retrieved_docs = retrieval(message,True)\n",
    "    \n",
    "                # Use retrieved document chunks to respond\n",
    "                print (\"\\nUsing Re-ranked docs to respond...\")\n",
    "                response = co_chat.chat(\n",
    "                    message=message,\n",
    "                    model_id=model_id_chat,\n",
    "                    preamble = preamble,\n",
    "                    documents=retrieved_docs,\n",
    "                    stream=True\n",
    "                )\n",
    "\n",
    "            # If there is no search query, directly respond\n",
    "            # No RAG\n",
    "            else:\n",
    "                response = co_chat.chat(\n",
    "                    message=message,\n",
    "                    model_id=model_id_chat,\n",
    "                    stream=True\n",
    "                )\n",
    "\n",
    "            # Print the chatbot response, citations, and documents\n",
    "            citations = []\n",
    "            cited_documents = []\n",
    "            cited_document_ids = []\n",
    "            unique_cited_document_ids = []\n",
    "\n",
    "            print(\"\\nRESPONSE:\")\n",
    "\n",
    "            # Display response\n",
    "            for event in response:\n",
    "                #print (event)\n",
    "                if event.event_type == \"text-generation\":\n",
    "                    print(event.text, end=\"\")\n",
    "                elif event.event_type == \"citation-generation\":\n",
    "                    citations.extend(event.citations)\n",
    "                elif event.event_type == \"search-results\":\n",
    "                    cited_documents = event.documents\n",
    "\n",
    "\n",
    "            # Display citations and source documents\n",
    "            if citations:\n",
    "                print(\"\\n\\nCITATIONS:\")\n",
    "                for citation in citations:\n",
    "                  print(citation)\n",
    "                  for ids in citation['document_ids']:\n",
    "                      cited_document_ids.append(ids)\n",
    "\n",
    "                unique_cited_document_ids = list(dict.fromkeys(cited_document_ids))\n",
    "\n",
    "                print(\"\\nDOCUMENTS:\")\n",
    "                for document in cited_documents:\n",
    "                  if document['id'] in unique_cited_document_ids:\n",
    "                   print(document)\n",
    "                   print(f\"\\n{'-'*100}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328899a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Chatbot class\n",
    "chatbot = Chatbot()\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
