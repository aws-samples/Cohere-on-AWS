{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embeddings Archives\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In this notebook, we demonstrate how to use the [Amazon Bedrock InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://cohere.com/blog/embedding-archives-wikipedia) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use the 2023 version of [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary) and binary embeddings. We also use the [Amazon Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html) to demonstrate how we can use the results of semantic search.\n",
    "\n",
    "### Semantic Search and Text Embeddings\n",
    "Semantic search leverages text embeddings and similarity to find responses based on meaning, not just keywords. Text embeddings represent pieces of text as numeric vectors that encode semantic meaning. These embeddings allow for mathematical comparisons of word and sentence meaning. Multilingual embeddings map text in different languages to the same vector space, enabling semantic search across languages. See [What is Semantic Search](https://cohere.com/blog/what-is-semantic-search) to read about improvement algorithms such as hierarchical navigable small world (HNSW) and multiple negative ranking loss.\n",
    "\n",
    "### Int8/byte and Binary Encoded Embeddings\n",
    "Semantic search over large datasets can require a lot of memory because most vector databases store embeddings and vector indices in memory. Dimensionality reduction to conserve memory and reduce costs can perform poorly ([Cohere research](https://arxiv.org/abs/2205.11498?ref=cohere-ai.ghost.io)). \n",
    "\n",
    "A better approach is to use a model that uses fewer bits per dimension. Cohere's Embed is a text embedding model that offers leading performance in 100+ languages. It translates text into vector representations which encode semantic meaning. Cohere's Embed is the first embedding model that natively supports int8/byte and binary embeddings.\n",
    "\n",
    "Binary embeddings give you a 32x reduction in memory and can be searched 40x faster. Given that embeddings are typically stored as float32, an embedding with 1024 dimensions requires 1024 x 4 bytes = 4096 bytes. Using 1 bit per dimension results in a 32x reduction in required memory (or, 4096 * 8 / 1024). See [Cohere int8 & binary embeddings](https://cohere.com/blog/int8-binary-embeddings).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Step 0: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IUnwp2cYNnP0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's install HF datasets and boto3, the AWS SDK for Python\n",
    "%pip install datasets --quiet\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "### Step 1: Install the Wikipedia embeddings archives published by Cohere\n",
    "\n",
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v8Pogz7gPQwg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb_int8', 'emb_ubinary'],\n",
      "    n_shards: 7\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Import torch, the open-source machine learning library\n",
    "import torch\n",
    "\n",
    "# Load at max 1000 documents and embeddings\n",
    "max_docs = 1000\n",
    "# Use the Simple English Wikipedia subset\n",
    "lang = \"simple\"\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\", lang, split=\"train\", streaming=True)\n",
    "\n",
    "# To verify we have loaded the data, print docs_stream\n",
    "print(docs_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `IterableDataset` object contains a collection of 1000 examples, each with `features` which are the names of the columns for each example.\n",
    "\n",
    "The `emb_int8` is an integer encoded embedding while `emb_ubinary` is a binary encoded embedding for each Wikipedia article article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Create tensor of binary embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory consumed by the tensor for the integer embeddings of the first doc in bytes is 8256\n",
      "The memory consumed by the tensor for the binary embeddings of the first doc in bytes is 1088\n",
      "The tensor for binary embeddings consumes 7.588235294117647 less memory.\n"
     ]
    }
   ],
   "source": [
    "# Access python interpreter and command-line arguments\n",
    "import sys\n",
    "# Let's create lists of documents and binary embeddings\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc[\"emb_ubinary\"])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "# Convert doc_embeddings into a PyTorch tensor\n",
    "doc_embeddings = torch.tensor(doc_embeddings)\n",
    "\n",
    "first_doc = next(iter(docs_stream))\n",
    "\n",
    "# Size of a tensor with the integer embeddings of the first doc\n",
    "first_integer_tensor_size = sys.getsizeof(torch.tensor(first_doc[\"emb_int8\"]).untyped_storage())\n",
    "# Size of a tensor with the binary embeddings of the first doc\n",
    "first_binary_tensor_size = sys.getsizeof(torch.tensor(first_doc[\"emb_ubinary\"]).untyped_storage())\n",
    "\n",
    "print(f\"The memory consumed by the tensor for the integer embeddings of the first doc in bytes is {first_integer_tensor_size}\")\n",
    "print(f\"The memory consumed by the tensor for the binary embeddings of the first doc in bytes is {first_binary_tensor_size}\")\n",
    "print(f\"The tensor for binary embeddings consumes {first_integer_tensor_size / first_binary_tensor_size} less memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://cohere.com/blog/sentence-word-embeddings) of 128 values. \n",
    "\n",
    "Note that the tensor for binary embeddings is approximately 7.59 times smaller than the tensor for integer embeddings. This is expected as integer embeddings use 1 byte (8 bits) per dimension while binary embeddings use 1 bit per dimension. The memory reduction is smaller than 8 because the tensor array itself has a non-zero size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the tensor shape\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "### Step 3: Embed query and compute dot product with document embeddings\n",
    "We can now search these vectors for any query we want. For this example, we'll ask a question about Alan Turing since we know the Wikipedia page for Alan Turing is included in this subset of the archive.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product).\n",
    "\n",
    "This shows the top `k` passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Alan Turing because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embeddings: [[18, 63, 75, 232, 59, 67, 51, 160, 255, 68, 251, 186, 114, 165, 136, 58, 82, 15, 211, 232, 128, 37, 107, 204, 75, 163, 74, 251, 32, 233, 200, 154, 106, 241, 127, 125, 74, 31, 123, 209, 82, 220, 228, 15, 254, 151, 220, 43, 199, 230, 143, 73, 67, 229, 149, 61, 34, 86, 69, 56, 215, 178, 131, 49, 108, 251, 76, 187, 134, 2, 155, 169, 129, 130, 229, 103, 12, 113, 145, 9, 32, 139, 212, 3, 224, 64, 27, 151, 175, 217, 139, 30, 132, 192, 111, 60, 221, 162, 108, 120, 153, 219, 214, 165, 164, 133, 78, 232, 203, 63, 149, 53, 135, 117, 100, 213, 75, 46, 114, 159, 22, 216, 255, 233, 98, 26, 252, 22]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To use Cohere models on Bedrock we need to install dependencies\n",
    "import boto3, json, logging\n",
    "# Set up the Bedrock client\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create request paramaters for Bedrock\n",
    "model_id = \"cohere.embed-multilingual-v3\"\n",
    "accept = \"*/*\"\n",
    "content_type = \"application/json\"\n",
    "embedding_types = [\"ubinary\"]\n",
    "input_type = \"search_query\"\n",
    "\n",
    "# Create the text used for semantic search\n",
    "query = \"Tell me about Alan Turing\"\n",
    "\n",
    "# Set the number of nearest neighbors\n",
    "k = 7\n",
    "\n",
    "body = json.dumps({\n",
    "    \"texts\": [query],\n",
    "    \"input_type\": input_type,\n",
    "    \"embedding_types\": embedding_types}\n",
    ")\n",
    "\n",
    "# Call the Bedrock InvokeModel API\n",
    "response = bedrock_rt.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept=accept,\n",
    "    contentType=content_type\n",
    ")\n",
    "\n",
    "# Load the response into response_body\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "# Extract the binary embeddings\n",
    "query_emb_ubinary = response_body[\"embeddings\"][\"ubinary\"]\n",
    "print(\"Query embeddings:\", query_emb_ubinary, \"\\n\")\n",
    "\n",
    "# Convert query into a PyTorch tensor\n",
    "query_emb_ubinary = torch.tensor(query_emb_ubinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's imagine that we didn't know that our documents contain text with information about Alan Turing. The way to search for relevant documents is to search the `doc_embeddings` with the binary embeddings that we just created above. The semantic meaning of our query is captured by the embeddings, and a simliar query will return an embedding with similar elements. A high score for the dot product indicates similarity. See [What is similarity between sentences](https://cohere.com/blog/what-is-similarity-between-sentences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest element from the dot_scores tensor is 2703919\n",
      "As expected, this value is still lower than the dot product of the query embeddings and itself 2873374 \n",
      "\n",
      "The query is: Tell me about Alan Turing \n",
      "\n",
      "The below is a list of top k relevant documents:\n",
      "Title: Alan Turing\n",
      "Text: Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Alan Turing\n",
      "Text: In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Botany\n",
      "Text: Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.\n",
      "https://simple.wikipedia.org/wiki/Botany \n",
      "\n",
      "Title: Creativity\n",
      "Text: Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.\n",
      "https://simple.wikipedia.org/wiki/Creativity \n",
      "\n",
      "Title: Computer\n",
      "Text: In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.\n",
      "https://simple.wikipedia.org/wiki/Computer \n",
      "\n",
      "Title: Alan Turing\n",
      "Text: Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Angel\n",
      "Text: The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "https://simple.wikipedia.org/wiki/Angel \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dot score between query embeddings and document embeddings\n",
    "dot_scores = torch.mm(query_emb_ubinary, doc_embeddings.transpose(0, 1))\n",
    "\n",
    "print(\"The largest element from the dot_scores tensor is\", dot_scores.max().item())\n",
    "print(\"As expected, this value is still lower than the dot product of the query embeddings and itself\", torch.mm(query_emb_ubinary, query_emb_ubinary.transpose(0, 1)).item(), '\\n')\n",
    "\n",
    "# Use topk to return the largest elements of the dot_scores tensor\n",
    "top_k = torch.topk(dot_scores, k)\n",
    "\n",
    "# Print results\n",
    "print(\"The query is:\", query, \"\\n\\nThe below is a list of top k relevant documents:\")\n",
    "# This loop iterates over the indices of the top k relevant documents\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print(\"Title:\", docs[doc_id][\"title\"])\n",
    "    print(\"Text:\", docs[doc_id][\"text\"])\n",
    "    print(docs[doc_id][\"url\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add the results of semantic search as context to a prompt\n",
    "\n",
    "Let's start by sending the same query to the Command R+ model using Amazon Bedrock to compare responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of API call 1 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to many fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and one of the key people who helped shape the field of artificial intelligence.\n",
      "\n",
      "Early Life and Education:\n",
      "Alan Mathison Turing was born on June 23, 1912, in Maida Vale, London, to Ethel Sara Turing and Julius Mathison Turing. From an early age, he displayed a talent for science and mathematics. He attended Sherborne School, where he excelled in mathematics and science, and later earned a scholarship to study at King's College, University of Cambridge, in 1931. At Cambridge, he studied mathematics, logic, and cryptology, and his work on probability theory and computability laid the foundations for his later achievements.\n",
      "\n",
      "Contributions to Computing and Cryptography: \n",
      "\n",
      "\n",
      "Result of API call 2 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to many fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and is often referred to as the \"father of computer science\" and \"father of artificial intelligence.\"\n",
      "\n",
      "Turing was born in London, England, in 1912 and showed a talent for science and mathematics from an early age. He attended the University of Cambridge, where he studied mathematics and gained a first-class degree in 1934. He then went on to do postgraduate work at Princeton University, where he received his PhD in mathematics in 1938.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, particularly those generated by the Enigma machine. His work at Bletchley Park, the British code-breaking center, was instrumental in helping the Allies gain \n",
      "\n",
      "\n",
      "Result of API call 3 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to many fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and is often referred to as the \"father of computer science\" and \"father of artificial intelligence.\"\n",
      "\n",
      "Turing was born in London, England, in 1912 and showed a talent for science and mathematics from an early age. He attended the University of Cambridge, where he studied mathematics and gained a reputation as a brilliant and innovative thinker.\n",
      "\n",
      "One of Turing's most significant achievements was his work during World War II at Britain's code-breaking center, Bletchley Park. He played a crucial role in breaking the German Enigma machine, a complex encryption device used by the German military to send coded messages. Turing designed a machine, known as the \"Turing Bombe,\" which helped decipher Enigma messages, providing the Allies \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the variables to make a call to the converse API\n",
    "user_message = \"Tell me about Alan Turing.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Make the API call three times to visualize the different responses\n",
    "    for i in range(3):\n",
    "        print(\"Result of API call\", str(i+1), ':')\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = bedrock_rt.converse(\n",
    "            modelId=\"cohere.command-r-plus-v1:0\",\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text, \"\\n\\n\")\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the large language model (LLM) is clearly non-deterministic. [Read more about LLM parameters here](https://cohere.com/blog/llm-parameters-best-outputs-language-ai) to learn about the parameters used to control model output. Note that we can also add the results of the semantic search as context for our prompt to augment the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is now: Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "Given the information above, answer this question: Tell me about Alan Turing.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string\n",
    "context = \"\"\n",
    "\n",
    "# Append the text from the relevant documents to the context\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    context += docs[doc_id][\"text\"]\n",
    "\n",
    "# Create a new prompt\n",
    "prompt = f\"\"\"{context}\n",
    "Given the information above, answer this question: {user_message}\"\"\"\n",
    "\n",
    "print(\"The prompt is now:\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 0.5 for the `temperature` to illustrate variance in responses from the LLM. The default value for `temperature` is 0.3 and lower values decrease randomness in the response. [Read more about inference requests to Amazon Bedrock here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html). Also, change the `modelId` to `cohere.command-r-v1:0` and use the Command R model if you want to test chat history. The Command R model supports a conversation history with multiple turns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of API call 1 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science, and his work laid the foundations for many aspects of modern computing. \n",
      "\n",
      "One of his most significant contributions was the conception of the Turing machine in 1936, which was a theoretical device that could perform complex calculations and has since become a central concept in computer science and theory. The Turing machine demonstrated the idea of a programmable computer, even though it was never intended to be built as a physical machine. \n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, most notably those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, is credited with shortening the war and saving countless lives. \n",
      "\n",
      "Turing's life and career were also marked by significant challenges. He was prosecuted in 1952 for homosexual acts, which were \n",
      "\n",
      "\n",
      "Result of API call 2 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science, and his work laid the foundation for modern computing. \n",
      "\n",
      "As you mentioned, Turing is known for his concept of the \"Turing machine,\" which he described in 1936. This hypothetical device served as a simple model for a general-purpose computer, and it introduced the idea of a computer program. Despite being a theoretical construct, the Turing machine demonstrated the potential for a machine to follow a set of instructions and perform complex calculations.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, most notably those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, significantly contributed to the Allied victory. After the war, Turing continued his work in computer science and mathematics.\n",
      "\n",
      "In addition to his scientific achievements, Turing's personal life \n",
      "\n",
      "\n",
      "Result of API call 3 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science, and his work laid the foundation for modern computing. \n",
      "\n",
      "As you mentioned, Turing is known for his concept of the \"Turing machine,\" which he described in 1936. This hypothetical device served as a simple model for a general-purpose computer, and it introduced the idea of a computer program. Despite being a theoretical construct, the Turing machine had a significant impact on the development of computing.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, including those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, is believed to have shortened the war significantly and saved countless lives. Turing's contributions to cryptanalysis and his development of code-breaking techniques were instrumental in this success.\n",
      "\n",
      "However, Turing's life was also \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send the same query to the Command R+ model using the Bedrock converse API but add the results of the semantic search as context\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}],\n",
    "    }\n",
    "]\n",
    "\n",
    "# For use as the chat_history parameter when modelId is cohere.command-r-v1:0\n",
    "# history = [\n",
    "#     {\"role\": \"USER\", \"message\": \"Example question from user\"},\n",
    "#     {\"role\": \"CHATBOT\", \"message\": \"Example response from chatbot\"}\n",
    "# ]\n",
    "\n",
    "try:\n",
    "    for i in range(3):\n",
    "        print('Result of API call', str(i+1), ':')\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = bedrock_rt.converse(\n",
    "            modelId='cohere.command-r-plus-v1:0',\n",
    "            messages=conversation,\n",
    "            # chat_history = history,\n",
    "            inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text, '\\n\\n')\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "In this notebook, we discussed the benefits of using different types of embeddings. Dimensionality reduction negatively impacts performance, but Cohere's Embed helps solve this problem by natively supporting int8/byte and binary embeddings. Binary embeddings provide a 32x memory reduction and enable 40x faster search compared to float32 embeddings, offering a highly efficient solution for large-scale semantic search. In the example code of this notebook, we used Amazon Bedrock to call Cohere's Embed and Command R+ LLMs. In addition, we can use the results of semantic search as context when sending prompts to a LLM. As seen above, using the augmented prompt containing the top `k` results from the semantic search gives much more deterministic responses. This is just one of the techniques we can use to return responses with up-to-date and domain-specific information. Semantic search also forms the basis for more advanced improvement algorithms such as HNSW and multiple negative ranking loss."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
