{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0deaf99b-0107-48a7-8844-10c7e87e87f6",
   "metadata": {},
   "source": [
    "# Using Knowledge Bases for Amazon Bedrock, Amazon OpenSearch Serverless, and Cohere Embed\n",
    "---\n",
    "# Introduction\n",
    "This notebook builds on the content in the [Text Embeddings using Cohere LLM stored in Amazon OpenSearch Serverless](https://github.com/aws-samples/Cohere-on-AWS/blob/main/cohere-cookbooks/Embeddings/Cohere_Embeddings_Search.ipynb) notebook. Amazon OpenSearch Serverless allows developers to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. A [Knowledge Base for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) supports Amazon OpenSearch Serverless, Pinecone, Redis Enterprise Cloud, Amazon Aurora, and MongoDB as vector stores. OpenSearch Serverless delivers millisecond response times with the simplicity of a serverless environment, making it ideal as a vector store for Retrieval-Augmented Generation (RAG). Using OpenSearch also allows developers to take advantage of visualization and monitoring through OpenSearch Dashboard features that developers may already be familiar with. In addition, OpenSearch Serverless is cost-effective because you only pay for the resources you consume. There is no need for upfront provisioning and overprovisioning for peak workloads. OpenSearch Serverless also automatically updates your collections to consume the latest bug fixes, features, and performance improvements.\n",
    "\n",
    "Within the AWS ecosystem, there are different ways to use Amazon OpenSearch Serverless. One way is to use Amazon OpenSearch Serverless as a vector store within a Knowledge Base. A Knowledge Base is fully managed Retrieval-Augmented Generation (RAG) capability that allows you to connect to foundation models (FMs) to deliver more relevant, context-specific, and accurate responses. \n",
    "\n",
    "This notebook focuses on using the Cohere Embed Multilingual V3 LLM (Large Language Model) to create embedings stored in a Knowledge Base for Amazon Bedrock powered by an Amazon OpenSearch Serverles vector store. The goal is to help developers generate accurate responses without performing undifferentiated steps to implement RAG. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e189327-6df5-4e6a-8549-b1f85b5be0e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "1.  Ensure you have requested access to the models provided by Cohere in the Bedrock console by clicking \"model access.\" Instructions can be found here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n",
    "1.  Make sure you have the permissions to access Bedrock and you have the correct IAM permissions from your administrator.\n",
    "2. Run the following cell to install boto3 and necessary packages.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e760aab-3672-42d5-b86e-be5d96ee3196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opensearch-py==2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.26.19)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.32.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.9.0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: boto3==1.33.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.33.2)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (1.33.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.2->boto3==1.33.2) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.2->boto3==1.33.2) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.2->boto3==1.33.2) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: retrying==1.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: six>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from retrying==1.3.4) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "import random\n",
    "from retrying import retry\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install dependencies\n",
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U boto3==1.33.2\n",
    "%pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f3a23-f0b1-4f20-9941-56f9ec00b6c3",
   "metadata": {},
   "source": [
    "## Step 0: Configure permissions by creating helper functions and create an Amazon S3 bucket\n",
    "One benefit of using an Amazon Bedrock Knowledge Base is that users have centralized control over permissions through AWS Identity and Access Management (IAM). The helper functions below allow Amazon Bedrock to access resources such as Amazon Simple Storage Service (S3) and Amazon OpenSearch Serverless (AOSS). By using Knowledge Bases, users also do not need to perform undifferentiated steps such as configuring an orchestrator framework such as LangChain.\n",
    "\n",
    "Run the cell below to configure the permissions required to create a Knowledge Base from this Python notebook. The code below is based on the [AWS Samples Amazon Bedrock workshop](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f54ac0-5b87-404e-a14f-85b18003b6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The create_bedrock_execution_role function creates an IAM role that Bedrock can assume to access resources like S3 and OpenSearch. \n",
    "# This role has policies to allow access to specific S3 bucket and the Amazon Cohere Embed Multilingual V3 LLM.\n",
    "def create_bedrock_execution_role(bucket_name):\n",
    "    foundation_model_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-multilingual-v3\",\n",
    "                    # Uncomment to try out the Amazon Titan Text Embeddings Models\n",
    "                    # f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\",\n",
    "                    # f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    s3_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket_name}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # create policies based on the policy documents\n",
    "    fm_policy = iam_client.create_policy(\n",
    "        PolicyName=fm_policy_name,\n",
    "        PolicyDocument=json.dumps(foundation_model_policy_document),\n",
    "        Description='Policy for accessing foundation model',\n",
    "    )\n",
    "\n",
    "    s3_policy = iam_client.create_policy(\n",
    "        PolicyName=s3_policy_name,\n",
    "        PolicyDocument=json.dumps(s3_policy_document),\n",
    "        Description='Policy for reading documents from s3')\n",
    "\n",
    "    # create bedrock execution role\n",
    "    bedrock_kb_execution_role = iam_client.create_role(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "        Description='Amazon Bedrock Knowledge Base Execution Role for accessing OSS and S3',\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "\n",
    "    # fetch arn of the policies and role created above\n",
    "    bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "    s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "    fm_policy_arn = fm_policy[\"Policy\"][\"Arn\"]\n",
    "    \n",
    "\n",
    "    # attach policies to Amazon Bedrock execution role\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=s3_policy_arn\n",
    "    )\n",
    "    return bedrock_kb_execution_role\n",
    "\n",
    "\n",
    "# The create_oss_policy_attach_bedrock_execution_role creates an IAM policy granting access to a specific OpenSearch collection\n",
    "# The IAM policy created is attached to the Bedrock execution role.\n",
    "def create_oss_policy_attach_bedrock_execution_role(collection_id, bedrock_kb_execution_role):\n",
    "    # define oss policy document\n",
    "    oss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:APIAccessAll\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:aoss:{region_name}:{account_number}:collection/{collection_id}\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    oss_policy = iam_client.create_policy(\n",
    "        PolicyName=oss_policy_name,\n",
    "        PolicyDocument=json.dumps(oss_policy_document),\n",
    "        Description='Policy for accessing opensearch serverless',\n",
    "    )\n",
    "    oss_policy_arn = oss_policy[\"Policy\"][\"Arn\"]\n",
    "    print(\"Opensearch serverless arn: \", oss_policy_arn)\n",
    "\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=oss_policy_arn\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "# The create_policies_in_oss function creates OpenSearch policies for encryption, network access, and data access for a vector store.\n",
    "def create_policies_in_oss(vector_store_name, aoss_client, bedrock_kb_execution_role_arn):\n",
    "    encryption_policy = aoss_client.create_security_policy(\n",
    "        name=encryption_policy_name,\n",
    "        policy=json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                           'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type='encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name=network_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                            'ResourceType': 'collection'}],\n",
    "                 'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type='network'\n",
    "    )\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name=access_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity, bedrock_kb_execution_role_arn],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type='data'\n",
    "    )\n",
    "    return encryption_policy, network_policy, access_policy\n",
    "\n",
    "\n",
    "def delete_iam_role_and_policies():\n",
    "    fm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{fm_policy_name}\"\n",
    "    s3_policy_arn = f\"arn:aws:iam::{account_number}:policy/{s3_policy_name}\"\n",
    "    oss_policy_arn = f\"arn:aws:iam::{account_number}:policy/{oss_policy_name}\"\n",
    "    sm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{sm_policy_name}\"\n",
    "\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=s3_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=oss_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=sm_policy_arn\n",
    "    )\n",
    "    iam_client.delete_role(RoleName=bedrock_execution_role_name)\n",
    "    iam_client.delete_policy(PolicyArn=s3_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=fm_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=oss_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=sm_policy_arn)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = ''\n",
    "    for i in range(seconds):\n",
    "        dots += '.'\n",
    "        print(dots, end='\\r')\n",
    "        time.sleep(1)\n",
    "\n",
    "def create_bedrock_execution_role_multi_ds(bucket_names = None, secrets_arns = None):\n",
    "    # 0. Create bedrock execution role\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # create bedrock execution role\n",
    "    bedrock_kb_execution_role = iam_client.create_role(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "        Description='Amazon Bedrock Knowledge Base Execution Role for accessing OSS, secrets manager and S3',\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "\n",
    "    # fetch arn of the role created above\n",
    "    bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "    # 1. Cretae and attach policy for foundation models\n",
    "    foundation_model_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-multilingual-v3\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    fm_policy = iam_client.create_policy(\n",
    "        PolicyName=fm_policy_name,\n",
    "        PolicyDocument=json.dumps(foundation_model_policy_document),\n",
    "        Description='Policy for accessing foundation model',\n",
    "    )\n",
    "  \n",
    "    # fetch arn of this policy \n",
    "    fm_policy_arn = fm_policy[\"Policy\"][\"Arn\"]\n",
    "    \n",
    "    # attach this policy to Amazon Bedrock execution role\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "\n",
    "    # 2. Cretae and attach policy for s3 bucket\n",
    "    if bucket_names:\n",
    "        s3_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [item for sublist in [[f'arn:aws:s3:::{bucket}', f'arn:aws:s3:::{bucket}/*'] for bucket in bucket_names] for item in sublist], \n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # create policies based on the policy documents\n",
    "        s3_policy = iam_client.create_policy(\n",
    "            PolicyName=s3_policy_name,\n",
    "            PolicyDocument=json.dumps(s3_policy_document),\n",
    "            Description='Policy for reading documents from s3')\n",
    "\n",
    "        # fetch arn of this policy \n",
    "        s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "        \n",
    "        # attach this policy to Amazon Bedrock execution role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "            PolicyArn=s3_policy_arn\n",
    "        )\n",
    "\n",
    "    # 3. Cretae and attach policy for secrets manager\n",
    "    if secrets_arns:\n",
    "        secrets_manager_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"secretsmanager:GetSecretValue\",\n",
    "                        \"secretsmanager:PutSecretValue\"\n",
    "                    ],\n",
    "                    \"Resource\": secrets_arns\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # create policies based on the policy documents\n",
    "        \n",
    "        secrets_manager_policy = iam_client.create_policy(\n",
    "            PolicyName=sm_policy_name,\n",
    "            PolicyDocument=json.dumps(secrets_manager_policy_document),\n",
    "            Description='Policy for accessing secret manager',\n",
    "        )\n",
    "\n",
    "        # fetch arn of this policy\n",
    "        sm_policy_arn = secrets_manager_policy[\"Policy\"][\"Arn\"]\n",
    "\n",
    "        # attach policy to Amazon Bedrock execution role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "            PolicyArn=sm_policy_arn\n",
    "        )\n",
    "    \n",
    "    return bedrock_kb_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47dff7d-82f2-4312-aea7-4972be784b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a suffix to use as a unique name for the S3 bucket\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "# Initialize a boto3 session.\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Initialize the AWS Identity and Access Management (IAM) client\n",
    "iam_client = boto3_session.client('iam')\n",
    "account_number = boto3.client('sts').get_caller_identity().get('Account')\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "# Initialize f-strings to be used as variable names\n",
    "encryption_policy_name = f\"bedrock-sample-rag-sp-{suffix}\"\n",
    "network_policy_name = f\"bedrock-sample-rag-np-{suffix}\"\n",
    "access_policy_name = f'bedrock-sample-rag-ap-{suffix}'\n",
    "bedrock_execution_role_name = f'AmazonBedrockExecutionRoleForKnowledgeBase_{suffix}'\n",
    "fm_policy_name = f'AmazonBedrockFoundationModelPolicyForKnowledgeBase_{suffix}'\n",
    "s3_policy_name = f'AmazonBedrockS3PolicyForKnowledgeBase_{suffix}'\n",
    "sm_policy_name = f'AmazonBedrockSecretPolicyForKnowledgeBase_{suffix}'\n",
    "oss_policy_name = f'AmazonBedrockOSSPolicyForKnowledgeBase_{suffix}'\n",
    "\n",
    "# Initialize a AWS Security Token Service client for temporary credentials\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}' # replace it with your bucket name.\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05eb0ab-1eaa-4b90-8d63-c3e5c49c8bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-east-1-809719347864 Exists\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except ClientError as e:\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    if region_name == \"us-east-1\":\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name)\n",
    "    else:\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2015f034-9af6-4678-bacc-eb5655331727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_name' (str)\n"
     ]
    }
   ],
   "source": [
    "# Store the bucket name\n",
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0ba97-b4a1-4196-99b8-a60e622ebc2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Create the Amazon OpenSearch Vector Store\n",
    "OpenSearch Serverless continually adjusts to get millisecond response times during changing usage patterns and demand. This makes it an ideal solution for a RAG-based workflow. Run the cell below to use the f-strings to create the vector store. An Amazon OpenSearch Serverless collection is a logical grouping of one or more indexes that represent an analytics workload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d4c5e7-6779-42b3-b862-5be74fe9304f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Fri, 30 Aug 2024 20:10:46 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '1993fc64-8c32-47ad-ad70-2a16da80b823'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '1993fc64-8c32-47ad-ad70-2a16da80b823',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:809719347864:collection/7ltrt6krz53hkn7ovgb9',\n",
      "                              'createdDate': 1725048646803,\n",
      "                              'id': '7ltrt6krz53hkn7ovgb9',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1725048646803,\n",
      "                              'name': 'bedrock-sample-rag-665',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "# Create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "# Print the OpenSearch vector search collection\n",
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3e0239-a47e-442f-84e0-9cb0878e8a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encryption_policy' (dict)\n",
      "Stored 'network_policy' (dict)\n",
      "Stored 'access_policy' (dict)\n",
      "Stored 'collection' (dict)\n"
     ]
    }
   ],
   "source": [
    "# Store the encryption policy, network policy, access policy, and collection variables\n",
    "%store encryption_policy network_policy access_policy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7100c9-c42e-4cd3-8a28-644533f6266d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7ltrt6krz53hkn7ovgb9.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0293e36a-54ed-498d-b8b8-40f96ec6aeec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "..............................\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:809719347864:collection/7ltrt6krz53hkn7ovgb9',\n",
      "    'collectionEndpoint': 'https://7ltrt6krz53hkn7ovgb9.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1725048646803,\n",
      "    'dashboardEndpoint': 'https://7ltrt6krz53hkn7ovgb9.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': '7ltrt6krz53hkn7ovgb9',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1725048670252,\n",
      "    'name': 'bedrock-sample-rag-665',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# Wait for collection creation. This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261ac7ba-839c-4331-8768-49a63f395793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::809719347864:policy/AmazonBedrockOSSPolicyForKnowledgeBase_665\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa4b98-cf3f-45f5-b1bc-55dc7dcc710b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Create Vector Index\n",
    "In Step 1, we created a vector store. This is where we will be storing the index which powers the Knowledge Base for Amazon Bedrock. An index is a way to structure your data. In this case, the data we need to store and structure are the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aabdd8e-ff83-4bb5-b4ac-2970788d57f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-665',\n",
      "  'shards_acknowledged': True}\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "# Use the credentials from the boto3 session.\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            # \"dimension\": 1536,\n",
    "            \"dimension\": 1024,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "# An OpenSearch client interfaces with OpenSearch to perform actions such as indexing, searching, or updating data. \n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548a601-7753-47aa-8593-8b8714c8f4f2",
   "metadata": {},
   "source": [
    "## Step 3: Download data to ingest into our Knowledge Base\n",
    "Now that we have created the Knowledge Base, it is time to configure data for testing. Note that it is also possible to test your own documents by directly uploading these documents to the S3 bucket created in Step 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f1d736-e68b-4a1b-971b-29a9dfcee840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7159ca2-cece-4457-93f2-c2cb8a9ab591",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run the cell below to upload the data to the S3 bucket created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2387229e-4013-49f6-8b7f-8b6597675783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload data to s3 to the bucket that was configured as a data source to the knowledge base\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucket_name,file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a0a21-4b7b-4ff8-b0a0-e7bcc6d540d7",
   "metadata": {},
   "source": [
    "## Step 4: Create a Knowledge Base\n",
    "Amazon Bedrock splits your documents or content into manageable chunks for efficient data retrieval. Chunks are converted to embeddings and written to a vector index while maintaining a mapping to the original document. If a single document or piece of content contains less than the specified number of tokens in a chunk, the document is not further split for the fixed-size chunking strategy. The overlap percentage controls the overlap tokens that each parent chunk has with its children. You can experiment with these parameters to find the settings that provide the best quality of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "325775a9-6886-4147-9a3b-25ad14e0af99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ingest strategy - How to ingest data from the data source\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "# embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-multilingual-v3\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n",
    "\n",
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b8870-aa9d-41da-864d-f75b16bb9115",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run the cell below to create the Knowledge Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ebea06-5daf-454f-b0f1-9240ce0c8b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 8, 30, 20, 18, 1, 33812, tzinfo=tzlocal()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:809719347864:knowledge-base/BH8TBDWUMM',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3'}},\n",
      "  'knowledgeBaseId': 'BH8TBDWUMM',\n",
      "  'name': 'bedrock-sample-knowledge-base-665',\n",
      "  'roleArn': 'arn:aws:iam::809719347864:role/AmazonBedrockExecutionRoleForKnowledgeBase_665',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:809719347864:collection/7ltrt6krz53hkn7ovgb9',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-665'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 20, 18, 1, 33812, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")\n",
    "    \n",
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dec97ea3-5276-4fd8-8808-cacedf2f36a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the Knowledge Base\n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9924add3-7aba-4891-82d8-3176e60a6b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 8, 30, 20, 18, 5, 690247, tzinfo=tzlocal()),\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-809719347864'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'SUVA1HHMAC',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'BH8TBDWUMM',\n",
      "  'name': 'bedrock-sample-knowledge-base-665',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 20, 18, 5, 690247, tzinfo=tzlocal()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3997ac-a770-4a1d-b637-a1cea28bb86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'a2f09785-89fc-464e-88d4-3093286ef09d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 30 Aug 2024 20:18:08 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '603',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'a2f09785-89fc-464e-88d4-3093286ef09d',\n",
       "   'x-amz-apigw-id': 'dVtYDF2QoAMEavA=',\n",
       "   'x-amzn-trace-id': 'Root=1-66d22900-3ec2914b0645907c0852c24a'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'knowledgeBaseId': 'BH8TBDWUMM',\n",
       "  'dataSourceId': 'SUVA1HHMAC',\n",
       "  'name': 'bedrock-sample-knowledge-base-665',\n",
       "  'status': 'AVAILABLE',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'dataSourceConfiguration': {'type': 'S3',\n",
       "   's3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-809719347864'}},\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}},\n",
       "  'createdAt': datetime.datetime(2024, 8, 30, 20, 18, 5, 690247, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 30, 20, 18, 5, 690247, tzinfo=tzlocal())}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource\n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82855a8e-4abf-48a0-bb8e-77f9586c5204",
   "metadata": {},
   "source": [
    "## Step 5: Start data ingestion \n",
    "After you create your Knowledge Base, you ingest your data source into your Knowledge Base to be queried. Data ingestion converts the raw data in your data source into vector embeddings. You must sync the data sourch each time you add, modify, or remove files so that it is re-indexed to the Knowledge Base. Syncing is incremental, so only added, modified, or deleted documents since the last sync are processed.\n",
    "\n",
    "OpenSearch Serverless uses a cloud-native architecture that separates the indexing (ingest) components from the search (query) components with Amazon S3 as the primary data storage for indexes. [Refer to this documentation to understand more about Amazon OpenSearch Serverless indexing and search compute units](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e886f819-9840-4334-8587-1d70708c2bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d390fde-d4d5-43fe-a65e-f54a73add718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'SUVA1HHMAC',\n",
      "  'ingestionJobId': '5NDW8RS9J3',\n",
      "  'knowledgeBaseId': 'BH8TBDWUMM',\n",
      "  'startedAt': datetime.datetime(2024, 8, 30, 20, 18, 15, 287168, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 0},\n",
      "  'status': 'STARTING',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 20, 18, 15, 287168, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "452ec9fb-5e89-41e7-9af2-3bea65b11922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'SUVA1HHMAC',\n",
      "  'ingestionJobId': '5NDW8RS9J3',\n",
      "  'knowledgeBaseId': 'BH8TBDWUMM',\n",
      "  'startedAt': datetime.datetime(2024, 8, 30, 20, 18, 15, 287168, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 20, 18, 30, 756774, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d820276b-de54-4ded-b374-c6ffb73d7c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'SUVA1HHMAC',\n",
      "  'ingestionJobId': '5NDW8RS9J3',\n",
      "  'knowledgeBaseId': 'BH8TBDWUMM',\n",
      "  'startedAt': datetime.datetime(2024, 8, 30, 20, 18, 15, 287168, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 20, 18, 30, 756774, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "988ab6ec-be85-4887-84ec-13bf7ad38013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'BH8TBDWUMM'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97c8bb1f-40cf-4837-9232-6fbb2cb8b279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# Store the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850fd0a-2f04-4227-beaa-a081f9378aa2",
   "metadata": {},
   "source": [
    "## Step 6: Test the Knowledge Base\n",
    "Now that we have indexed data into the Knowledge Base, it is time to test the Knowledge Base with the [Retrieve and Generate API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_RetrieveAndGenerate.html). [Here are the models supported by the Retrieve and Generate API](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-process). Run the cell below to generate responses using two different FMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f99cce9-3a41-422b-9236-5b01c2a69b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generated using Claude 3 Sonnet:\n",
      "('Amazon is investing heavily in Large Language Models (LLMs) and Generative '\n",
      " 'AI. The company believes Generative AI will transform and improve virtually '\n",
      " 'every customer experience across its consumer, seller, brand, and creator '\n",
      " 'offerings. Amazon has been working on its own LLMs for a while and plans to '\n",
      " 'continue substantial investments in these models. On the AWS side, Amazon is '\n",
      " 'democratizing Generative AI technology so that companies of all sizes can '\n",
      " 'leverage it. AWS offers machine learning chips like Trainium and Inferentia '\n",
      " 'that provide cost-effective training and running of LLMs. AWS also enables '\n",
      " \"companies to choose from various LLMs and build applications with AWS's \"\n",
      " 'security, privacy, and other features. Additionally, AWS has launched '\n",
      " 'applications like CodeWhisperer that use Generative AI to improve developer '\n",
      " 'productivity.')\n",
      "---------- The citations for the response generated by Claude 3 Sonnet:\n",
      "[ 'The customer reaction to what we’ve shared thus far about Kuiper has been '\n",
      "  'very positive, and we believe Kuiper represents a very large potential '\n",
      "  'opportunity for Amazon. It also shares several similarities to AWS in that '\n",
      "  'it’s capital intensive at the start, but has a large prospective consumer, '\n",
      "  'enterprise, and government customer base, significant revenue and operating '\n",
      "  'profit potential, and relatively few companies with the technical and '\n",
      "  'inventive aptitude, as well as the investment hypothesis to go after it.   '\n",
      "  'One final investment area that I’ll mention, that’s core to setting Amazon '\n",
      "  'up to invent in every area of our business for many decades to come, and '\n",
      "  'where we’re investing heavily is Large Language Models (“LLMs”) and '\n",
      "  'Generative AI. Machine learning has been a technology with high promise for '\n",
      "  'several decades, but it’s only been the last five to ten years that it’s '\n",
      "  'started to be used more pervasively by companies. This shift was driven by '\n",
      "  'several factors, including access to higher volumes of compute capacity at '\n",
      "  'lower prices than was ever available. Amazon has been using machine '\n",
      "  'learning extensively for 25 years, employing it in everything from '\n",
      "  'personalized ecommerce recommendations, to fulfillment center pick paths, '\n",
      "  'to drones for Prime Air, to Alexa, to the many machine learning services '\n",
      "  'AWS offers (where AWS has the broadest machine learning functionality and '\n",
      "  'customer base of any cloud provider). More recently, a newer form of '\n",
      "  'machine learning, called Generative AI, has burst onto the scene and '\n",
      "  'promises to significantly accelerate machine learning adoption. Generative '\n",
      "  'AI is based on very Large Language Models (trained on up to hundreds of '\n",
      "  'billions of parameters, and growing), across expansive datasets, and has '\n",
      "  'radically general and broad recall and learning capabilities.',\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n",
      "---------- Generated using Claude 3 Haiku:\n",
      "('Amazon is heavily investing in large language models (LLMs) and generative '\n",
      " 'AI, which they believe will transform and improve virtually every customer '\n",
      " 'experience. They have been working on their own LLMs and are democratizing '\n",
      " 'this technology so companies of all sizes can leverage generative AI. AWS is '\n",
      " 'offering the most price-performant machine learning chips in Trainium and '\n",
      " 'Inferentia to enable companies to train and run their LLMs in production. '\n",
      " 'Additionally, AWS is delivering applications like CodeWhisperer that use '\n",
      " 'generative AI to revolutionize developer productivity by generating code '\n",
      " 'suggestions in real time.')\n",
      "---------- The citations for the response generated by Claude 3 Haiku:\n",
      "[ 'The customer reaction to what we’ve shared thus far about Kuiper has been '\n",
      "  'very positive, and we believe Kuiper represents a very large potential '\n",
      "  'opportunity for Amazon. It also shares several similarities to AWS in that '\n",
      "  'it’s capital intensive at the start, but has a large prospective consumer, '\n",
      "  'enterprise, and government customer base, significant revenue and operating '\n",
      "  'profit potential, and relatively few companies with the technical and '\n",
      "  'inventive aptitude, as well as the investment hypothesis to go after it.   '\n",
      "  'One final investment area that I’ll mention, that’s core to setting Amazon '\n",
      "  'up to invent in every area of our business for many decades to come, and '\n",
      "  'where we’re investing heavily is Large Language Models (“LLMs”) and '\n",
      "  'Generative AI. Machine learning has been a technology with high promise for '\n",
      "  'several decades, but it’s only been the last five to ten years that it’s '\n",
      "  'started to be used more pervasively by companies. This shift was driven by '\n",
      "  'several factors, including access to higher volumes of compute capacity at '\n",
      "  'lower prices than was ever available. Amazon has been using machine '\n",
      "  'learning extensively for 25 years, employing it in everything from '\n",
      "  'personalized ecommerce recommendations, to fulfillment center pick paths, '\n",
      "  'to drones for Prime Air, to Alexa, to the many machine learning services '\n",
      "  'AWS offers (where AWS has the broadest machine learning functionality and '\n",
      "  'customer base of any cloud provider). More recently, a newer form of '\n",
      "  'machine learning, called Generative AI, has burst onto the scene and '\n",
      "  'promises to significantly accelerate machine learning adoption. Generative '\n",
      "  'AI is based on very Large Language Models (trained on up to hundreds of '\n",
      "  'billions of parameters, and growing), across expansive datasets, and has '\n",
      "  'radically general and broad recall and learning capabilities.',\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Let's see how different Anthropic Claude 3 models responds to the input text we provide\n",
    "claude_model_ids = [ [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"], [\"Claude 3 Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]]\n",
    "\n",
    "# Define a function to send a query to a FM using a Knowledge Base.\n",
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "# Modify this query if you customized the documents uploaded to Amazon S3.\n",
    "query = \"What is Amazon doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d20147f-def8-4766-a879-60e6e8c7cd77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ { 'content': { 'text': 'Generative AI is based on very Large Language Models '\n",
      "                         '(trained on up to hundreds of billions of '\n",
      "                         'parameters, and growing), across expansive datasets, '\n",
      "                         'and has radically general and broad recall and '\n",
      "                         'learning capabilities. We have been working on our '\n",
      "                         'own LLMs for a while now, believe it will transform '\n",
      "                         'and improve virtually every customer experience, and '\n",
      "                         'will continue to invest substantially in these '\n",
      "                         'models across all of our consumer, seller, brand, '\n",
      "                         'and creator experiences. Additionally, as we’ve done '\n",
      "                         'for years in AWS, we’re democratizing this '\n",
      "                         'technology so companies of all sizes can leverage '\n",
      "                         'Generative AI. AWS is offering the most '\n",
      "                         'price-performant machine learning chips in Trainium '\n",
      "                         'and Inferentia so small and large companies can '\n",
      "                         'afford to train and run their LLMs in production. We '\n",
      "                         'enable companies to choose from various LLMs and '\n",
      "                         'build applications with all of the AWS security, '\n",
      "                         'privacy and other features that customers are '\n",
      "                         'accustomed to using. And, we’re delivering '\n",
      "                         'applications like AWS’s CodeWhisperer, which '\n",
      "                         'revolutionizes        developer productivity by '\n",
      "                         'generating code suggestions in real time. I could '\n",
      "                         'write an entire letter on LLMs and Generative AI as '\n",
      "                         'I think they will be that transformative, but I’ll '\n",
      "                         'leave that for a future letter. Let’s just say that '\n",
      "                         'LLMs and Generative AI are going to be a big deal '\n",
      "                         'for customers, our shareholders, and Amazon.   So, '\n",
      "                         'in closing, I’m optimistic that we’ll emerge from '\n",
      "                         'this challenging macroeconomic time in a stronger '\n",
      "                         'position than when we entered it. There are several '\n",
      "                         'reasons for it and I’ve mentioned many of them '\n",
      "                         'above. But, there are two relatively simple '\n",
      "                         'statistics that underline our immense future '\n",
      "                         'opportunity. While we have a consumer business '\n",
      "                         'that’s $434B in 2022, the vast majority of total '\n",
      "                         'market segment share in global retail still resides '\n",
      "                         'in physical stores (roughly 80%). And, it’s a '\n",
      "                         'similar story for Global IT spending, where we have '\n",
      "                         'AWS revenue of $80B in 2022, with about 90% of '\n",
      "                         'Global IT spending still on-premises and yet to '\n",
      "                         'migrate to the cloud.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-809719347864/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'score': 0.6058815},\n",
      "  { 'content': { 'text': 'The customer reaction to what we’ve shared thus far '\n",
      "                         'about Kuiper has been very positive, and we believe '\n",
      "                         'Kuiper represents a very large potential opportunity '\n",
      "                         'for Amazon. It also shares several similarities to '\n",
      "                         'AWS in that it’s capital intensive at the start, but '\n",
      "                         'has a large prospective consumer, enterprise, and '\n",
      "                         'government customer base, significant revenue and '\n",
      "                         'operating profit potential, and relatively few '\n",
      "                         'companies with the technical and inventive aptitude, '\n",
      "                         'as well as the investment hypothesis to go after '\n",
      "                         'it.   One final investment area that I’ll mention, '\n",
      "                         'that’s core to setting Amazon up to invent in every '\n",
      "                         'area of our business for many decades to come, and '\n",
      "                         'where we’re investing heavily is Large Language '\n",
      "                         'Models (“LLMs”) and Generative AI. Machine learning '\n",
      "                         'has been a technology with high promise for several '\n",
      "                         'decades, but it’s only been the last five to ten '\n",
      "                         'years that it’s started to be used more pervasively '\n",
      "                         'by companies. This shift was driven by several '\n",
      "                         'factors, including access to higher volumes of '\n",
      "                         'compute capacity at lower prices than was ever '\n",
      "                         'available. Amazon has been using machine learning '\n",
      "                         'extensively for 25 years, employing it in everything '\n",
      "                         'from personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption. Generative AI '\n",
      "                         'is based on very Large Language Models (trained on '\n",
      "                         'up to hundreds of billions of parameters, and '\n",
      "                         'growing), across expansive datasets, and has '\n",
      "                         'radically general and broad recall and learning '\n",
      "                         'capabilities.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-809719347864/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'score': 0.5768737},\n",
      "  { 'content': { 'text': 'Most companies are still in the training stage, but '\n",
      "                         'as they develop models that graduate to large-scale '\n",
      "                         'production, they’ll find that most of the cost is in '\n",
      "                         'inference because models are trained periodically '\n",
      "                         'whereas inferences are happening all the time as '\n",
      "                         'their associated application is being exercised. We '\n",
      "                         'launched our first inference chips (“Inferentia”) in '\n",
      "                         '2019, and they have saved companies like Amazon over '\n",
      "                         'a hundred million dollars in capital expense '\n",
      "                         'already. Our Inferentia2 chip, which just launched, '\n",
      "                         'offers up to four times higher throughput and ten '\n",
      "                         'times lower latency than our first Inferentia '\n",
      "                         'processor. With the enormous upcoming growth in '\n",
      "                         'machine learning, customers will be able to get a '\n",
      "                         'lot more done with AWS’s training and inference '\n",
      "                         'chips at a significantly lower cost. We’re not close '\n",
      "                         'to being done innovating here, and this long-term '\n",
      "                         'investment should prove fruitful for both customers '\n",
      "                         'and AWS. AWS is still in the early stages of its '\n",
      "                         'evolution, and has a chance for unusual growth in '\n",
      "                         'the next decade.   Similarly high potential, '\n",
      "                         'Amazon’s Advertising business is uniquely effective '\n",
      "                         'for brands, which is part of why it continues to '\n",
      "                         'grow at a brisk clip. Akin to physical retailers’ '\n",
      "                         'advertising businesses selling shelf space, end- '\n",
      "                         'caps, and placement in their circulars, our '\n",
      "                         'sponsored products and brands offerings have been an '\n",
      "                         'integral part        of the Amazon shopping '\n",
      "                         'experience for more than a decade. However, unlike '\n",
      "                         'physical retailers, Amazon can tailor these '\n",
      "                         'sponsored products to be relevant to what customers '\n",
      "                         'are searching for given what we know about shopping '\n",
      "                         'behaviors and our very deep investment in machine '\n",
      "                         'learning algorithms. This leads to advertising '\n",
      "                         'that’s more useful for customers; and as a result, '\n",
      "                         'performs better for brands. This is part of why our '\n",
      "                         'Advertising revenue has continued to grow rapidly '\n",
      "                         '(23% YoY in Q4 2022, 25% YoY overall for 2022 on a '\n",
      "                         '$31B revenue base), even as most large '\n",
      "                         'advertising-focused businesses’ growth have slowed '\n",
      "                         'over the last several quarters.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-809719347864/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'score': 0.5283473},\n",
      "  { 'content': { 'text': 'This leads to advertising that’s more useful for '\n",
      "                         'customers; and as a result, performs better for '\n",
      "                         'brands. This is part of why our Advertising revenue '\n",
      "                         'has continued to grow rapidly (23% YoY in Q4 2022, '\n",
      "                         '25% YoY overall for 2022 on a $31B revenue base), '\n",
      "                         'even as most large advertising-focused businesses’ '\n",
      "                         'growth have slowed over the last several quarters.   '\n",
      "                         'We strive to be the best place for advertisers to '\n",
      "                         'build their brands. We have near and long-term '\n",
      "                         'opportunities that will help us achieve that '\n",
      "                         'mission. We’re continuing to make large investments '\n",
      "                         'in machine learning to keep honing our advertising '\n",
      "                         'selection algorithms. For the past couple of years, '\n",
      "                         'we’ve invested in building comprehensive, flexible, '\n",
      "                         'and durable planning and measurement solutions, '\n",
      "                         'giving marketers greater insight into advertising '\n",
      "                         'effectiveness. An example is Amazon Marketing Cloud '\n",
      "                         '(“AMC”). AMC is a “clean room” (i.e. secure digital '\n",
      "                         'environment) in which advertisers can run custom '\n",
      "                         'audience and campaign analytics across a range of '\n",
      "                         'first and third-party inputs, in a privacy-safe '\n",
      "                         'manner, to generate advertising and business '\n",
      "                         'insights to inform their broader marketing and sales '\n",
      "                         'strategies. The Advertising and AWS teams have '\n",
      "                         'collaborated to enable companies to store their data '\n",
      "                         'in AWS, operate securely in AMC with Amazon and '\n",
      "                         'other third-party data sources, perform analytics in '\n",
      "                         'AWS, and have the option to activate advertising on '\n",
      "                         'Amazon or third-party publishers through the Amazon '\n",
      "                         'Demand-Side Platform. Customers really like this '\n",
      "                         'concerted capability. We also see future opportunity '\n",
      "                         'to thoughtfully integrate advertising into our '\n",
      "                         'video, live sports, audio, and grocery products. '\n",
      "                         'We’ll continue to work hard to help brands uniquely '\n",
      "                         'engage with the right audience, and grow this part '\n",
      "                         'of our business.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-809719347864/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'score': 0.50575167},\n",
      "  { 'content': { 'text': 'To ensure that future generations have the skills '\n",
      "                         'they need to thrive in a technology-driven economy, '\n",
      "                         'we started a program last year called Amazon Future '\n",
      "                         'Engineer, which is designed to educate and train '\n",
      "                         'low-income and disadvantaged young people to pursue '\n",
      "                         'careers in computer science. We have an ambitious '\n",
      "                         'goal: to help hundreds of thousands of students each '\n",
      "                         'year learn computer science and coding. Amazon '\n",
      "                         'Future Engineer currently funds Introduction to '\n",
      "                         'Computer Science and AP Computer Science classes for '\n",
      "                         'more than 2,000 schools in underserved communities '\n",
      "                         'across the country. Each year, Amazon Future '\n",
      "                         'Engineer also gives 100 four-year, $40,000 college '\n",
      "                         'scholarships to computer science students from '\n",
      "                         'low-income backgrounds. Those scholarship recipients '\n",
      "                         'also receive guaranteed, paid internships at Amazon '\n",
      "                         'after their first year of college. Our program in '\n",
      "                         'the UK funds 120 engineering apprenticeships and '\n",
      "                         'helps students from disadvantaged backgrounds pursue '\n",
      "                         'technology careers.   For now, my own time and '\n",
      "                         'thinking continues to be focused on COVID-19 and how '\n",
      "                         'Amazon can help while we’re in the middle of it. I '\n",
      "                         'am extremely grateful to my fellow Amazonians for '\n",
      "                         'all the grit and ingenuity they are showing as we '\n",
      "                         'move through this. You can count on all of us to '\n",
      "                         'look beyond the immediate crisis for insights and '\n",
      "                         'lessons and how to apply them going forward.   '\n",
      "                         'Reflect on this from Theodor Seuss Geisel:   “When '\n",
      "                         'something bad happens you have three choices. You '\n",
      "                         'can either let it define you, let it destroy you, or '\n",
      "                         'you can let it strengthen you.”   I am very '\n",
      "                         'optimistic about which of these civilization is '\n",
      "                         'going to choose.   Even in these circumstances, it '\n",
      "                         'remains Day 1. As always, I attach a copy of our '\n",
      "                         'original 1997 letter.   Sincerely,   Jeffrey P.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-809719347864/AMZN-2019-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'score': 0.49981433}]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to use the Amazon Bedrock Retrieve API\n",
    "def retrieve(query, kb_id, number_of_results=5):\n",
    "    return bedrock_agent_runtime_client.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': query\n",
    "        },\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': number_of_results,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Run the same query using the Amazon Bedrock Retrieve API. \n",
    "# You can use these results for any prompt or FM.\n",
    "query = 'What is Amazon doing in the field of Generative AI?'\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrieval_results = response['retrievalResults']\n",
    "pp.pprint(retrieval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113040e-a16c-4529-be74-b8baea9f6e28",
   "metadata": {},
   "source": [
    "## Step 7: Clean up\n",
    "\n",
    "When you finish this exercise, remove your resources with the following steps to prevent incurring costs:\n",
    "\n",
    "Delete the vector index.\n",
    "Delete data, network, and encryption access policies.\n",
    "Delete the Amazon OpenSearch Serverless collection.\n",
    "Optionally, empty and delete the S3 bucket, or keep whatever you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0337b04e-d37c-400c-9899-d56b352f6c08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deleteCollectionDetail': {'id': '7ltrt6krz53hkn7ovgb9',\n",
       "  'name': 'bedrock-sample-rag-665',\n",
       "  'status': 'DELETING'},\n",
       " 'ResponseMetadata': {'RequestId': '5a27a0e3-0e4a-4630-91a9-93d38929ac07',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5a27a0e3-0e4a-4630-91a9-93d38929ac07',\n",
       "   'date': 'Fri, 30 Aug 2024 20:21:40 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '108',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete vector index\n",
    "oss_client.indices.delete(index=index_name)\n",
    "\n",
    "# delete data, network, and encryption access ploicies\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])\n",
    "\n",
    "# delete collection\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "aoss_client.delete_collection(id=collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb30c40-d356-41b9-bbd8-76579cd043ab",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "In this notebook, we discussed the benefit of using a Knowledge Base for Amazon Bedrock. The main benefit of using a Knowledge Base is that developers do not need to perform undifferentiated steps to implement RAG. Amazon OpenSearch Serverless delivers millisecond response times, automatic scaling, and the vector search collection type is suitable for building Generative AI applications. We started by configuring the permissions to create a Knowledge Base for Amazon Bedrock. The Cohere Embed Multilingual V3 LLM is used to map text to a semantic vector space and create embeddings. Embeddings were created using Amazon shareholder letters from 2019 to 2022. From there, we ingested the embeddings into the Knowledge Base Amazon OpenSearch Serverless vector store. We called the Retrieve and Generate API and Retrieve Amazon Bedrock APIs to demonstrate how developers can interact with the Knowledge Base. In this example, we were able to answer the question \"What is Amazon doing in the field of generative AI?\". The responses generated depend on the data sources ingested. This means that different data sources will be suitable for other queries.\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
