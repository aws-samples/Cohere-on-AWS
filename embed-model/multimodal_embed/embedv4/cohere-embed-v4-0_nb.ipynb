{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy cohere Embed V4 Model Package from AWS Marketplace \n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "## <font color='orange'>Important:</font>\n",
    "\n",
    "Please visit model detail page in <a href=\"https://aws.amazon.com/marketplace/pp/prodview-g53hj27nurqc6\">https://aws.amazon.com/marketplace/pp/prodview-g53hj27nurqc6</a> to learn more. <font color='orange'>If you do not have access to the link, please contact account admin for the help.</font>\n",
    "\n",
    "You will find details about the model including pricing, supported region, and end user license agreement. To use the model, please click “<font color='orange'>Continue to Subscribe</font>” from the detail page, come back here and learn how to deploy and inference.\n",
    "\n",
    "-------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cohere builds a collection of Large Language Models (LLMs) trained on a massive corpus of curated web data. Powering these models, our infrastructure enables our product to be deployed for a wide range of use cases. The use cases we power include generation (copy writing, etc), summarization, classification, content moderation, information extraction, semantic search, and contextual entity extraction\n",
    "\n",
    "This sample notebook shows you how to deploy [cohere-embed-v4](https://aws.amazon.com/marketplace/pp/prodview-g53hj27nurqc6) using Amazon SageMaker:\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to one of the models listed above. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page [cohere-embed-v4](https://aws.amazon.com/marketplace/pp/prodview-qd64mji3pbnvk)\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:16.643218Z",
     "iopub.status.busy": "2025-07-08T03:35:16.642284Z",
     "iopub.status.idle": "2025-07-08T03:35:20.633998Z",
     "shell.execute_reply": "2025-07-08T03:35:20.633223Z",
     "shell.execute_reply.started": "2025-07-08T03:35:16.643177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade setuptools==69.5.1 cohere-aws\n",
    "# if you upgrade the package, you need to restart the kernel\n",
    "\n",
    "from cohere_aws import Client\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:22.640798Z",
     "iopub.status.busy": "2025-07-08T03:35:22.640273Z",
     "iopub.status.idle": "2025-07-08T03:35:22.655683Z",
     "shell.execute_reply": "2025-07-08T03:35:22.654811Z",
     "shell.execute_reply.started": "2025-07-08T03:35:22.640756Z"
    }
   },
   "outputs": [],
   "source": [
    "cohere_package = \"cohere-embed-v5-0-05302025-694d497b680337388087ca1f6d92b76c\"\n",
    "\n",
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{cohere_package}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{cohere_package}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{cohere_package}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{cohere_package}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{cohere_package}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{cohere_package}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{cohere_package}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{cohere_package}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{cohere_package}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{cohere_package}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{cohere_package}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{cohere_package}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{cohere_package}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{cohere_package}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{cohere_package}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{cohere_package}\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:27.663698Z",
     "iopub.status.busy": "2025-07-08T03:35:27.663171Z",
     "iopub.status.idle": "2025-07-08T03:35:27.944324Z",
     "shell.execute_reply": "2025-07-08T03:35:27.943383Z",
     "shell.execute_reply.started": "2025-07-08T03:35:27.663669Z"
    }
   },
   "outputs": [],
   "source": [
    "co = Client(region_name=region)\n",
    "\n",
    "ENDPOINT_NAME = \"<endpoint_name>\"\n",
    "\n",
    "# Uncomment to create an endpoint\n",
    "# co.create_endpoint(arn=model_package_arn, endpoint_name=ENDPOINT_NAME, instance_type=\"ml.g6.xlarge\", n_instances=1)\n",
    "\n",
    "# If the endpoint is already created, you just need to connect to it\n",
    "co.connect_to_endpoint(endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interleaved Text and Image Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, display all four images that are used as part of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:32.497747Z",
     "iopub.status.busy": "2025-07-08T03:35:32.497107Z",
     "iopub.status.idle": "2025-07-08T03:35:32.505608Z",
     "shell.execute_reply": "2025-07-08T03:35:32.504890Z",
     "shell.execute_reply.started": "2025-07-08T03:35:32.497717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; flex-direction: row; gap: 10px; align-items: flex-start;\"><div style=\"text-align: center;\"><img src=\"Picture1.jpg\" style=\"height: 200px;\"/><br>Picture1.jpg</div><div style=\"text-align: center;\"><img src=\"Picture2.jpg\" style=\"height: 200px;\"/><br>Picture2.jpg</div><div style=\"text-align: center;\"><img src=\"Picture3.jpg\" style=\"height: 200px;\"/><br>Picture3.jpg</div><div style=\"text-align: center;\"><img src=\"Picture4.jpg\" style=\"height: 200px;\"/><br>Picture4.jpg</div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# List of image filenames\n",
    "images = ['Picture1.jpg', 'Picture2.jpg', 'Picture3.jpg', 'Picture4.jpg']\n",
    "\n",
    "# Create HTML to display images in one row with their names below each image\n",
    "html_content = '<div style=\"display: flex; flex-direction: row; gap: 10px; align-items: flex-start;\">'\n",
    "for img in images:\n",
    "    html_content += f'<div style=\"text-align: center;\"><img src=\"{img}\" style=\"height: 200px;\"/><br>{img}</div>'\n",
    "html_content += '</div>'\n",
    "\n",
    "# Display the images in one row with names\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images have been picked to demonstrate how Cohere Embed v4 supports searching with interleaved input containing both text and images. Assuming that you are looking at Picture 4 and want to search for **\"the same style pants but with no stripes\"**, you can combine Picture 4 with this prompt to create embeddings. Embed v4 supports seeking an item within a scene as well as adding a modifer.\n",
    "\n",
    "As a next step, encode all candidate images as base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:34.391623Z",
     "iopub.status.busy": "2025-07-08T03:35:34.390879Z",
     "iopub.status.idle": "2025-07-08T03:35:34.399478Z",
     "shell.execute_reply": "2025-07-08T03:35:34.398418Z",
     "shell.execute_reply.started": "2025-07-08T03:35:34.391593Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\", region_name=\"<region>\")\n",
    "\n",
    "# Encode the images as base64\n",
    "with open(\"Picture1.jpg\", \"rb\") as img_file:\n",
    "    image1_b64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "with open(\"Picture2.jpg\", \"rb\") as img_file:\n",
    "    image2_b64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "with open(\"Picture3.jpg\", \"rb\") as img_file:\n",
    "    image3_b64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "with open(\"Picture4.jpg\", \"rb\") as img_file:\n",
    "    image4_b64 = base64.b64encode(img_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T02:40:05.790652Z",
     "iopub.status.busy": "2025-07-08T02:40:05.789942Z",
     "iopub.status.idle": "2025-07-08T02:40:05.817497Z",
     "shell.execute_reply": "2025-07-08T02:40:05.816450Z",
     "shell.execute_reply.started": "2025-07-08T02:40:05.790619Z"
    }
   },
   "source": [
    "Define a function to create embeddings with the prompt and Picture 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:36.359664Z",
     "iopub.status.busy": "2025-07-08T03:35:36.358925Z",
     "iopub.status.idle": "2025-07-08T03:35:36.363783Z",
     "shell.execute_reply": "2025-07-08T03:35:36.362876Z",
     "shell.execute_reply.started": "2025-07-08T03:35:36.359634Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_interleaved_input(\n",
    "    client,\n",
    "    endpoint_name,\n",
    "    interleaved_input,\n",
    "    input_type=\"search_query\",\n",
    "    truncate=\"NONE\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends interleaved text and image base64 inputs to the Cohere Embed endpoint.\n",
    "\n",
    "    Args:\n",
    "        client: The boto3 SageMaker runtime client.\n",
    "        endpoint_name (str): The name of the SageMaker endpoint.\n",
    "        interleaved_input (list): List of strings (text or base64 images).\n",
    "        input_type (str): The input type (\"search_query\" or \"classification\").\n",
    "        truncate (str): Truncation strategy (\"NONE\", \"START\", or \"END\").\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON response from the endpoint.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"texts\": interleaved_input,\n",
    "        \"input_type\": input_type,\n",
    "        \"truncate\": truncate\n",
    "    }\n",
    "\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    return json.loads(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings with Picture 4 and our prompt, **\"the same style pants but with no stripes\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:37.820257Z",
     "iopub.status.busy": "2025-07-08T03:35:37.819962Z",
     "iopub.status.idle": "2025-07-08T03:35:43.618723Z",
     "shell.execute_reply": "2025-07-08T03:35:43.617641Z",
     "shell.execute_reply.started": "2025-07-08T03:35:37.820233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the query\n",
    "interleaved_input = [\n",
    "    \"Same style pants but with no stripes.\",\n",
    "    image4_b64,  # base64-encoded image string\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "interleaved_embedding_floats = embed_interleaved_input(\n",
    "    client,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    interleaved_input=interleaved_input,\n",
    "    input_type=\"search_query\",\n",
    "    truncate=\"NONE\"\n",
    ")['embeddings']['float'][0]\n",
    "    \n",
    "# Uncomment to inspect interleaved_embedding_floats \n",
    "# print(interleaved_embedding_floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize an empty list named *responses* to store dictionaries containing the *filename* of the candidate picture and its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:43.620787Z",
     "iopub.status.busy": "2025-07-08T03:35:43.620276Z",
     "iopub.status.idle": "2025-07-08T03:35:43.852624Z",
     "shell.execute_reply": "2025-07-08T03:35:43.851945Z",
     "shell.execute_reply.started": "2025-07-08T03:35:43.620757Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of base64-encoded images and their filenames\n",
    "images_info = [\n",
    "    {\"filename\": \"Picture1.jpg\", \"b64\": image1_b64},\n",
    "    {\"filename\": \"Picture2.jpg\", \"b64\": image2_b64},\n",
    "    {\"filename\": \"Picture3.jpg\", \"b64\": image3_b64},\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store results with filenames\n",
    "responses = []\n",
    "\n",
    "for img in images_info:\n",
    "    # Construct the image data URI\n",
    "    image_data_uri = f\"data:image/jpeg;base64,{img['b64']}\"  # Use image/png if appropriate\n",
    "\n",
    "    single_payload = {\n",
    "        \"images\": [image_data_uri],\n",
    "        \"input_type\": \"image\",  # Use \"image\" for image embeddings\n",
    "        \"embedding_types\": [\"float\"],\n",
    "        \"max_tokens\": 8000,\n",
    "    }\n",
    "    single_response = client.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(single_payload)\n",
    "    )\n",
    "    single_result = json.loads(single_response['Body'].read())\n",
    "    responses.append({\n",
    "        \"filename\": img[\"filename\"],\n",
    "        \"result\": single_result\n",
    "    })\n",
    "\n",
    "# (Optional) Print or process responses as needed\n",
    "# print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to compare with the candidate embeddings, define a function that highlights the image with the highest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:43.853972Z",
     "iopub.status.busy": "2025-07-08T03:35:43.853476Z",
     "iopub.status.idle": "2025-07-08T03:35:43.862567Z",
     "shell.execute_reply": "2025-07-08T03:35:43.861870Z",
     "shell.execute_reply.started": "2025-07-08T03:35:43.853948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; flex-direction: row; gap: 16px; align-items: flex-start;\"><div style=\"text-align: center;\"><img src=\"Picture1.jpg\" style=\"height: 200px;\"/><br><mark>Picture1.jpg</mark><br><mark>cosine similarity: 0.3710</mark></div><div style=\"text-align: center;\"><img src=\"Picture2.jpg\" style=\"height: 200px;\"/><br><span style=\"font-family:monospace; font-size:14px;\">Picture2.jpg</span><br><span style=\"font-size:13px; color: #333;\">cosine similarity: 0.3477</span></div><div style=\"text-align: center;\"><img src=\"Picture3.jpg\" style=\"height: 200px;\"/><br><span style=\"font-family:monospace; font-size:14px;\">Picture3.jpg</span><br><span style=\"font-size:13px; color: #333;\">cosine similarity: 0.3241</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to highlight the image with the highest similarity\n",
    "def display_highest_similarity(\n",
    "    responses,\n",
    "    interleaved_embedding_floats\n",
    "):\n",
    "    \"\"\"\n",
    "    Display images with cosine similarity scores, highlighting the one with the highest similarity.\n",
    "\n",
    "    Args:\n",
    "        responses (list): List of dicts with keys 'filename' and 'result' (embedding).\n",
    "        interleaved_embedding_floats (np.ndarray): The embedding to compare against.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarities\n",
    "    for img in responses:\n",
    "        embedding = np.array(img['result']['embeddings']['float'][0])\n",
    "        cosine_sim = np.dot(embedding, interleaved_embedding_floats) / (\n",
    "            np.linalg.norm(embedding) * np.linalg.norm(interleaved_embedding_floats)\n",
    "        )\n",
    "        img[\"cosine_similarity\"] = cosine_sim\n",
    "\n",
    "    # Find the index of the maximum cosine similarity\n",
    "    max_idx = np.argmax([img[\"cosine_similarity\"] for img in responses])\n",
    "\n",
    "    html_content = '<div style=\"display: flex; flex-direction: row; gap: 16px; align-items: flex-start;\">'\n",
    "\n",
    "    for idx, img in enumerate(responses):\n",
    "        cosine_sim = img[\"cosine_similarity\"]\n",
    "        if idx == max_idx:\n",
    "            label_html = f'<mark>{img[\"filename\"]}</mark>'\n",
    "            sim_html = f'<mark>cosine similarity: {cosine_sim:.4f}</mark>'\n",
    "        else:\n",
    "            label_html = f'<span style=\"font-family:monospace; font-size:14px;\">{img[\"filename\"]}</span>'\n",
    "            sim_html = f'<span style=\"font-size:13px; color: #333;\">cosine similarity: {cosine_sim:.4f}</span>'\n",
    "\n",
    "        html_content += (\n",
    "            f'<div style=\"text-align: center;\">'\n",
    "            f'<img src=\"{img[\"filename\"]}\" style=\"height: 200px;\"/><br>'\n",
    "            f'{label_html}<br>{sim_html}'\n",
    "            f'</div>'\n",
    "        )\n",
    "\n",
    "    html_content += '</div>'\n",
    "    display(HTML(html_content))\n",
    "\n",
    "display_highest_similarity(responses, interleaved_embedding_floats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, **Picture 1** has the highest cosine similarity. This example shows how customers can take advantage of multimodality with Embed v4 for search only use cases in addition to generative AI applications.\n",
    "\n",
    "Now, let's try with another interleaved input. This time, we want to identify and highlight the image **\"with the same style pants but in pink\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:35:49.011802Z",
     "iopub.status.busy": "2025-07-08T03:35:49.010825Z",
     "iopub.status.idle": "2025-07-08T03:35:54.806359Z",
     "shell.execute_reply": "2025-07-08T03:35:54.805636Z",
     "shell.execute_reply.started": "2025-07-08T03:35:49.011773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; flex-direction: row; gap: 16px; align-items: flex-start;\"><div style=\"text-align: center;\"><img src=\"Picture1.jpg\" style=\"height: 200px;\"/><br><span style=\"font-family:monospace; font-size:14px;\">Picture1.jpg</span><br><span style=\"font-size:13px; color: #333;\">cosine similarity: 0.2968</span></div><div style=\"text-align: center;\"><img src=\"Picture2.jpg\" style=\"height: 200px;\"/><br><mark>Picture2.jpg</mark><br><mark>cosine similarity: 0.4743</mark></div><div style=\"text-align: center;\"><img src=\"Picture3.jpg\" style=\"height: 200px;\"/><br><span style=\"font-family:monospace; font-size:14px;\">Picture3.jpg</span><br><span style=\"font-size:13px; color: #333;\">cosine similarity: 0.2829</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the query\n",
    "interleaved_input = [\n",
    "    \"Same style pants but in pink.\",\n",
    "    image4_b64,  # base64-encoded image string\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "interleaved_embedding_floats = embed_interleaved_input(\n",
    "    client,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    interleaved_input=interleaved_input,\n",
    "    input_type=\"search_query\",\n",
    "    truncate=\"NONE\"\n",
    ")['embeddings']['float'][0]\n",
    "\n",
    "display_highest_similarity(responses, interleaved_embedding_floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, **Picture 2** has the highest cosine similarity. \n",
    "\n",
    "Repeat the steps above with a query for the **\"same outfit with a white top\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:36:07.336131Z",
     "iopub.status.busy": "2025-07-08T03:36:07.335467Z",
     "iopub.status.idle": "2025-07-08T03:36:13.125226Z",
     "shell.execute_reply": "2025-07-08T03:36:13.124382Z",
     "shell.execute_reply.started": "2025-07-08T03:36:07.336101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; flex-direction: row; gap: 16px; align-items: flex-start;\"><div style=\"text-align: center;\"><img src=\"Picture1.jpg\" style=\"height: 200px;\"/><br><span style=\"font-family:monospace; font-size:14px;\">Picture1.jpg</span><br><span style=\"font-size:13px; color: #333;\">cosine similarity: 0.1818</span></div><div style=\"text-align: center;\"><img src=\"Picture2.jpg\" style=\"height: 200px;\"/><br><span style=\"font-family:monospace; font-size:14px;\">Picture2.jpg</span><br><span style=\"font-size:13px; color: #333;\">cosine similarity: 0.1761</span></div><div style=\"text-align: center;\"><img src=\"Picture3.jpg\" style=\"height: 200px;\"/><br><mark>Picture3.jpg</mark><br><mark>cosine similarity: 0.3056</mark></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the query\n",
    "interleaved_input = [\n",
    "    \"Same outfit but with a white top.\",\n",
    "    image4_b64,  # base64-encoded image string\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "interleaved_embedding_floats = embed_interleaved_input(\n",
    "    client,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    interleaved_input=interleaved_input,\n",
    "    input_type=\"search_query\",\n",
    "    truncate=\"NONE\"\n",
    ")['embeddings']['float'][0]\n",
    "\n",
    "display_highest_similarity(responses, interleaved_embedding_floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T03:34:55.057811Z",
     "iopub.status.busy": "2025-07-08T03:34:55.056780Z",
     "iopub.status.idle": "2025-07-08T03:34:55.067244Z",
     "shell.execute_reply": "2025-07-08T03:34:55.065918Z",
     "shell.execute_reply.started": "2025-07-08T03:34:55.057774Z"
    }
   },
   "source": [
    "**Picture 3** is the correct candidate image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.delete_endpoint()\n",
    "co.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
