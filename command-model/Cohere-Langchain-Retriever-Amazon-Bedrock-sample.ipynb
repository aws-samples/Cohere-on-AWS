{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Exploring LangChain Retrieval Algorithms for improved RAG with Cohere models on Amazon Bedrock\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the use of Cohere's [Command](https://docs.cohere.com/docs/command-beta) model and their [Embed-english](https://cohere.com/embed) embeddings model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on a SageMaker Notebook. This notebook is powered by an `ml.t3.medium instance`. These models can be called through the Bedrock API, which we then use to build, experiment with, and tune for enhancing our RAG application for imrpved retrieval using [LangChain](https://www.langchain.com/). Additionally, we showcase how the [FAISS](https://github.com/facebookresearch/faiss) and ChromaDB can be utilized to archive and retrieve embeddings, integrating it into your RAG workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9774479-6120-43cf-873f-309f2958e8fb",
   "metadata": {},
   "source": [
    "## What are LangChain Retrievers?\n",
    "\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770dd68-a309-490a-b26a-e9bc943e8cf2",
   "metadata": {},
   "source": [
    "LangChain's retrieval algorithms can help improve upon the current capabilities of LLMs in processing, understanding, and generating human-like text. As the size and complexity of documents increase, representing multiple facets of the document in a single embedding can lead to a loss of specificity. Although it’s essential to capture the general essence of a document, it’s equally crucial to recognize and represent the varied sub-contexts within. This is a challenge you are often faced with when working with larger documents. Another challenge with RAG is that with retrieval, you aren’t aware of the specific queries that your document storage system will deal with upon ingestion. This could lead to information most relevant to a query being buried under text. For customers in industries such as healthcare, telecommunications, and financial services who are looking to implement RAG in their applications, the limitations of the regular retriever chain in providing precision, avoiding redundancy, and effectively compressing information make it less suited to fulfilling these needs compared to some of the advanced retrievers we will discuss in this notebook. These techniques are able to distill vast amounts of information into the concentrated, impactful insights that you need, while helping improve price-performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70287fd5-1c6f-4a05-a7cc-085cb10b4508",
   "metadata": {},
   "source": [
    "### Local setup (Optional):\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33af4f4-69fd-4a9f-acb3-f06bbe64fb21",
   "metadata": {},
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e19e16-86b3-4e27-94bf-00e2f832605c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "---\n",
    "\n",
    "1. [Requirements](#1.-Requirements)\n",
    "1. [Data Processing Steps](#2.-Data-Preparation)\n",
    "1. [Vector Store backed Retrieval](#3.-Vector-store-backed-retriever)\n",
    "1. [RetrievalQAchain](#4.-RetrievalQA-Chain)\n",
    "1. [Exploring some popular LangChain Retrievers](#5.-Exploring-some-popular-LangChain-Retrievers)\n",
    "1. [Conclusion](#6.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb282-2718-4911-a92b-4ef084441239",
   "metadata": {},
   "source": [
    "## 1. Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d071-695d-4102-a8cd-6fba6c4678a3",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d152-2889-4004-8eba-a0d9028708db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain-community==0.2.6\n",
    "langchain==0.2.6\n",
    "boto3==1.34.134\n",
    "pypdf==4.1.0\n",
    "faiss-cpu==1.8.0\n",
    "sqlalchemy==2.0.31\n",
    "langchain-aws==0.1.8\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e835",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "Before proceeding restart the kernel, go to the \"Kernel\" menu and select \"Restart Kernel\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada3b14-9c01-4597-82ce-ae31f9d6b000",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20737908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.31\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f89ca85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.6\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01013ed2-604c-4d94-ad95-f6377a64c279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.134\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import glob\n",
    "import json\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "#from langchain_aws import ChatBedrock\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab9fe9-7719-4faa-b592-303ec758ecc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be0b3ea-8482-4288-8ed2-e03d7bafcb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',\n",
    "                       region_name='us-east-1',\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd6416-7183-46ce-9e26-dd7f49b0ada3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "Currently only the Command and Command light text generation models from Cohere work with LangChain's [BedrockLLM](https://python.langchain.com/v0.2/docs/integrations/platforms/aws/) class\n",
    "LangChain's [ChatBedrock](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/) class which is the suggested way to perform chat completion tasks with LangChain is is yet to support Command R and Command R+ models, this notebook will be updated once the change has been made.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13c22418-d962-46b2-8e5e-0bcb74e6ff64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the desired cohere model as the default model\n",
    "#Currently only the Command and Command light text generation models from Cohere work with LangChain's BedrockLLM class\n",
    "#ChatBedrock class is yet to support command R and Coammnd R+, this notebook will be updated once the change has been made\n",
    "\n",
    "command_light = \"cohere.command-light-text-v14\"\n",
    "command_text = \"cohere.command-text-v14\"\n",
    "\n",
    "DEFAULT_MODEL = command_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "82fb1d89-8dc5-4306-9004-3b46ce237434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#llm\n",
    "llm = BedrockLLM(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 2048,  ## MAXIMUM NUMBER OF TOKENS for Mistral Large\n",
    "        \"temperature\": 0.5,\n",
    "        \"p\": 1\n",
    "    },\n",
    "    client=bedrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7be65a00-3747-44e6-92de-7580e05c7fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hi Human! I am excited to talk to you. How can I help you today? What would you like to know or discuss?'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize conversation chain with Cohere model on Bedrock\n",
    "conversation = ConversationChain(\n",
    "    # We set verbose to false to suppress the printing of logs during the execution of the conversation chain. This can be set to true when you're debugging your conversation chain or trying to understand how it's working under the hood.\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory() \n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00e3a47-9d7b-4c2c-8d91-47bfee0bda1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "#initialize cohere-embed-model with bedrockembeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"cohere.embed-english-v3\",\n",
    "                                       client=bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88793e5-c562-48ce-858b-50c918ac5249",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e0dc2-718f-47af-aa60-30fa9a60cae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's first build out our document store.\n",
    "\n",
    "In this example, we'll be using several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b0cbc6-367a-443a-9e59-c63640a1e4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2023-Shareholder-Letter.pdf',\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2023, source=filenames[0]),\n",
    "    dict(year=2022, source=filenames[1]),\n",
    "    dict(year=2021, source=filenames[2])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859253bf-4bb0-43bf-999a-e1abb1f6983b",
   "metadata": {},
   "source": [
    "As part of Amazon's culture, the CEO always includes a copy of the 1997 Letter to Shareholders with every new release. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 1997 letter (last 3 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac21b76-14b4-4c64-8cfe-408d877426c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "for local_pdf in local_pdfs:\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    for pagenum in range(len(pdf_reader.pages)-3):\n",
    "        page = pdf_reader.pages[pagenum]\n",
    "        pdf_writer.add_page(page)\n",
    "\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fa7ac-6605-4842-87f8-7cc844e01c12",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80600818-b41c-45b2-b86b-2e4c69271ed6",
   "metadata": {},
   "source": [
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13cbcc0-908c-4fa3-adab-f9eae209cf92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Dear Shareholders:\\nLast year at this time, I shared my enthusiasm and optimism for Amazon’s future. Today, I have even more.\\nThe reasons are many, but start with the progress we’ve made in our financial results and customerexperiences, and extend to our continued innovation and the remarkable opportunities in front of us.\\nIn 2023, Amazon’s total revenue grew 12% year-over-year (“Y oY”) from $514B to $575B. By segment, North\\nAmerica revenue increased 12% Y oY from $316B to $353B, International revenue grew 11% Y oY from$118B to $131B, and AWS revenue increased 13% Y oY from $80B to $91B.\\nFurther, Amazon’s operating income and Free Cash Flow (“FCF”) dramatically improved. Operating\\nincome in 2023 improved 201% Y oY from $12.2B (an operating margin of 2.4%) to $36.9B (an operatingmargin of 6.4%). Trailing Twelve Month FCF adjusted for equipment finance leases improved from -$12.8Bin 2022 to $35.5B (up $48.3B).' metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "\n",
    "    documents += document\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27bc3e-9d2b-47cc-9764-9c8b16200b06",
   "metadata": {},
   "source": [
    "Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d6183e-9ceb-429c-8042-935d56acf4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 21 documents loaded is 4515 characters.\n",
      "After the split we have 141 documents as opposed to the original 21.\n",
      "Average length among 141 documents (after split) is 689 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e72f-6162-4aa5-9aa9-0bbb29b026ea",
   "metadata": {},
   "source": [
    "We had 3 PDF documents and one txt file which have been split into smaller ~500 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b550cd-1f0f-445b-9c3b-dbd7abf5294f",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding with `cohere-embed` would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67cd64a-ba7a-419e-953a-307704772f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [-0.00990295 -0.00534058 -0.05657959 ...  0.04312134 -0.05709839\n",
      "  0.01496124]\n",
      "Size of the embedding:  (1024,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967efb1a-8586-4a74-bbd4-b52d1730693b",
   "metadata": {
    "tags": []
   },
   "source": [
    "This can be easily done using [FAISS](https://github.com/facebookresearch/faiss) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0993d824-e269-4e0c-80f0-1b3bde27302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95675c9-7116-4bd3-ba63-30963750e36f",
   "metadata": {},
   "source": [
    "## 3. Vector store backed retriever\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a8be6-29e5-4f2d-966d-2b73eedcdb01",
   "metadata": {},
   "source": [
    "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. This is the simplest method and the one that is easiest to get started with. It creates embeddings for each piece of text.\n",
    "\n",
    "Once you construct a vector store, it's very easy to construct a retriever. Let's walk through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fef25939-35c9-4704-9c07-be48e0412e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Sometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question. However, a question people never ask, and might be evenmore interesting is what’s the next set of primitives you’re building that enables breakthrough customer\\nexperiences? If you asked me today, I’d lead with Generative AI (“GenAI”).\\nMuch of the early public attention has focused on GenAI applications , with the remarkable 2022 launch of\\nChatGPT. But, to our “primitive” way of thinking, there are three distinct layers in the GenAI stack, each ofwhich is gigantic, and each of which we’re deeply investing.\\nThebottom layer is for developers and companies wanting to build foundation models (“FMs”). The', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='only been the last five to ten years that it’s started to be used more pervasively by companies. This shift wasdriven by several factors, including access to higher volumes of compute capacity at lower prices than was everavailable. Amazon has been using machine learning extensively for 25 years, employing it in everythingfrom personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically general and broad recall andlearning capabilities. We have been working on our own', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='on-premises. These businesses will keep shifting online and into the cloud. In Media and Advertising,\\ncontent will continue to migrate from linear formats to streaming. Globally, hundreds of millions of peoplewho don’t have adequate broadband access will gain that connectivity in the next few years. Last butcertainly not least, Generative AI may be the largest technology transformation since the cloud (whichitself, is still in the early stages), and perhaps since the Internet. Unlike the mass modernization of on-premises infrastructure to the cloud, where there’s work required to migrate, this GenAI revolution will bebuilt from the start on top of the cloud. The amount of societal and business benefit from the solutions thatwill be possible will astound us all.\\nThere has never been a time in Amazon’s history where we’ve felt there is so much opportunity to make our', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='developer productivity by generating code suggestions in real time. I could write an entire letter on LLMs\\nand Generative AI as I think they will be that transformative, but I’ll leave that for a future letter. Let’s justsay that LLMs and Generative AI are going to be a big deal for customers, our shareholders, and Amazon.\\nSo, in closing, I’m optimistic that we’ll emerge from this challenging macroeconomic time in a stronger', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore_faiss.as_retriever()\n",
    "vector_store_answer = retriever.invoke(\"Generative AI\")\n",
    "print(vector_store_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b81f6e5-d902-432b-8466-1373231d2531",
   "metadata": {},
   "source": [
    "<b>Maximum marginal relevance retrieval</b>\n",
    "\n",
    "By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddfab0ae-0b08-41c5-96c4-8652ddfc913f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Sometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question. However, a question people never ask, and might be evenmore interesting is what’s the next set of primitives you’re building that enables breakthrough customer\\nexperiences? If you asked me today, I’d lead with Generative AI (“GenAI”).\\nMuch of the early public attention has focused on GenAI applications , with the remarkable 2022 launch of\\nChatGPT. But, to our “primitive” way of thinking, there are three distinct layers in the GenAI stack, each ofwhich is gigantic, and each of which we’re deeply investing.\\nThebottom layer is for developers and companies wanting to build foundation models (“FMs”). The', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='past fall, leading FM-maker, Anthropic,announced it would use Trainium and Inferentia to build, train, and deploy its future FMs. We alreadyhave several customers using our AI chips, including Anthropic, Airbnb, Hugging Face, Qualtrics, Ricoh,and Snap.', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='Transparent , and eventually created multi-year franchises in The Marvelous Mrs. Maisel ,The Boys ,Bosch ,\\nandJack Ryan . Along the way, we’ve learned a lot about producing compelling entertainment with memorable\\nmoments and using machine learning and other inventive technology to provide a superior-quality streamingexperience (with useful, relevant data about actors, TV shows, movies, music, or sports stats a click awayin our unique X-Ray feature). Y ou might have seen some of this in action in our recent new hit series, Reacher ,\\nand you’ll hopefully see it in our upcoming Lord of the Rings series launch (coming Labor Day 2022). Wealso expect that you’ll see this iterative invention when we launch Thursday Night Football , the NFL’s first', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore_faiss.as_retriever(search_type=\"mmr\")\n",
    "vector_store_mmr = retriever.invoke(\"Generative AI\")\n",
    "print(vector_store_mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19255fb5-8577-443b-92ee-2ee33ac769e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Similarity score threshold retrieval</b>\n",
    "\n",
    "You can also set a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d7f2201-75ea-4e10-9e53-d14b0c118bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Sometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question. However, a question people never ask, and might be evenmore interesting is what’s the next set of primitives you’re building that enables breakthrough customer\\nexperiences? If you asked me today, I’d lead with Generative AI (“GenAI”).\\nMuch of the early public attention has focused on GenAI applications , with the remarkable 2022 launch of\\nChatGPT. But, to our “primitive” way of thinking, there are three distinct layers in the GenAI stack, each ofwhich is gigantic, and each of which we’re deeply investing.\\nThebottom layer is for developers and companies wanting to build foundation models (“FMs”). The', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore_faiss.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.2})\n",
    "vector_store_sst = retriever.invoke(\"Generative AI\")\n",
    "print(vector_store_sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9df11-1991-458f-a5be-0b02fcc9e403",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Question Answering with VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45ccc9-282e-4b4a-af22-243906a3add1",
   "metadata": {},
   "source": [
    "Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afcb328c-1b5a-4c12-a9ed-330941d639fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d335bb-63bf-4870-8e6e-019a1b7c005d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM. This wrapper performs the following steps behind the scences:\n",
    "\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2ff92-84dd-4ad7-856c-643e342cf5cd",
   "metadata": {},
   "source": [
    "*Note: In this example we are using `Cohere Command` as the underlying model, this particular model performs best if the inputs are provided under `\"\"\"\\\"prompt\\\":\\\"{query}\\\" \"\"\"` . In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73ea8c05-0de9-4c5a-a53d-869aeb034592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\\\"prompt\\\":\\\"{query}\\\" \"\"\"\n",
    "\n",
    "VSPROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"question\"]\n",
    "    )\n",
    "chain_type_kwargs = { \"prompt\" : VSPROMPT }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c398c738-0fb5-4281-b818-8edf6b610add",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_query = \"Provide me some highlights of the business in the year 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e5bb92f-6a96-4b79-9993-c6a1a2492cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't have the specific information regarding the business performance in 2023, however, the provided text does highlight several accomplishments achieved by the company in that year. \n",
      "\n",
      "Some of the notable highlights mentioned in the text include:\n",
      "\n",
      "- Strong overall performance in 2023, despite the challenging macroeconomic conditions of 2022. \n",
      "- Growth in demand and innovation across the company's largest businesses. \n",
      "- Improvements made to enhance the customer experience both in the short and long term. \n",
      "- A focus on invention, collaboration, discipline, execution, and reimagination. \n",
      "\n",
      "It seems that the company had a successful year in terms of adapting and growing, despite the challenges faced. \n",
      "\n",
      "Would you like me to elaborate on any of the mentioned points? \n"
     ]
    }
   ],
   "source": [
    "answer = wrapper_store_faiss.query(question=VSPROMPT.format(query=vs_query), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187b411-b7ce-48e7-bda4-d8d2abcefc53",
   "metadata": {},
   "source": [
    "## 4. RetrievalQA Chain\n",
    "---\n",
    "In the above scenario you explored some quick and easy ways to get a context-aware answers to your questions. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/) which can be specific to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "786b9a12-3407-47ce-8457-437059d84788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"Text: {context}\n",
    "    Question: {question}\n",
    "    you are a chatbot designed to assist the users.\n",
    "    Answer only the questions based on the text provided. If the text doesn't contain the answer,\n",
    "    reply that the answer is not available.\n",
    "    keep the answers precise to the question\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever( #default retriever with vectorstore\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff4d62-82a5-4f07-9ed4-828b468b6356",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a304e6ab-a8ed-4d85-be9b-35ed60721a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text highlights the success of Amazon's business in 2023, but does not mention specific events or achievements from that year.\n",
      "\n",
      "Here are some ways the text indirectly supports the claim:\n",
      "\n",
      "1. Success in business - The text states that 2023 was a strong year for Amazon, indicating that the Amazon Business investment has been successful.\n",
      "\n",
      "2. Investment in ecommerce - The text mentions Amazon Business as an investment where Amazon's ecommerce and logistics capabilities have been leveraged, suggesting that Amazon is focusing on expanding its online retail presence.\n",
      "\n",
      "3. Teamwork and execution - The author expresses gratitude towards their teams for their hard work and delivery, implying that Amazon's success is due to effective collaboration and execution.\n",
      "\n",
      "Can I help you with anything else regarding Amazon's business or the year 2023? \n",
      "\n",
      "[Document(page_content='Overall, 2023 was a strong year, and I’m grateful to our collective teams who delivered on behalf of\\ncustomers. These results represent a lot of invention, collaboration, discipline, execution, and reimagination', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf', 'doc_id': '31e88892-25b4-4ec9-a94e-6382afebb0f9'}), Document(page_content='Amazon Business is another example of an investment where our ecommerce and logistics capabilities', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='Amazon Business is another example of an investment where our ecommerce and logistics capabilities', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf', 'doc_id': '9d635f48-efeb-4810-9684-397e6f39c0c9'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Provide me some highlights of the business in the year 2023\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdc79d-03de-4482-9bb3-cef49d468222",
   "metadata": {},
   "source": [
    "We will be using the Retrieval QA chain and initialize it with the following retrievers. Previously, we used vectorstore_faiss as the retriever. Although this is the easiest way to get started, it is not very efficient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee2176-38e7-4293-83db-d3ba7425af56",
   "metadata": {},
   "source": [
    "## 5. Exploring some popular LangChain Retrievers\n",
    "\n",
    "In this section, we will be going over some popular LangChain Retrieval algorithms/retrievers. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.Retrievers accept a string `query` as input and return a list of `Document`s as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735263b3-388f-4d24-acb4-af4ba7b1e5eb",
   "metadata": {},
   "source": [
    "### 5.1 Multi-vector\n",
    "\n",
    "It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. The Multi-vector retriever is best used when you are able to extract information from documents that you think is more relevant to index than the text itself. In this example, we will be showing how to create multiple vectors by splitting a document into smaller chunks, and embedding those chunks. The next section will cover the [Parent Document Retriever](#5.2-Parent-Document-Retriever-Chain) type that goes into detail about this kind of retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2977f7ab-b37b-442b-9c83-043fc6bcbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31073e64-c0c6-41d7-9c8d-9f12a79d1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "mv_retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore_faiss,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30ad7669-5a9d-4e3e-8678-b025f80daaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b1fe24d-2864-485b-885b-0f2a7f1a54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bebdbbff-c564-41f8-a98c-8bbe835ff67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_retriever.vectorstore.add_documents(sub_docs)\n",
    "mv_retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3268a01-7b9d-45b9-9b42-03106d35c3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically general and broad recall andlearning capabilities. We have been working', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf', 'doc_id': '58672aa6-6533-4af7-921a-45f461933226'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorstore alone retrieves the small chunks\n",
    "mv_retriever.vectorstore.similarity_search(\"Generative AI\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "827555b7-9f80-4a13-a244-a57015db160a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'only been the last five to ten years that it’s started to be used more pervasively by companies. This shift wasdriven by several factors, including access to higher volumes of compute capacity at lower prices than was everavailable. Amazon has been using machine learning extensively for 25 years, employing it in everythingfrom personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically general and broad recall andlearning capabilities. We have been working on our own'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "mv_retriever.search_type = SearchType.mmr\n",
    "\n",
    "mv_retriever.invoke(\"Generative AI\")[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d911fe-8f03-487e-8109-6ab50ba9121a",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `MultiVectorRetriever`. We will pass the prompt in via the `chain_type_kwargs` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f490703-d94a-4fe6-b454-35fe4d6d5c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mv_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=mv_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f1242c17-4fdf-4396-88fc-6b2dcf3055d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Some of the highlights of the business based on the information provided from the shareholder letter are:\n",
      "- Optimism and energy going into the new year, despite a challenging macroeconomic backdrop in 2022\n",
      "- Growth in demand, even after experiencing unprecedented growth during the first half of the pandemic\n",
      "- Innovation across all business areas that improved the customer experience in the short and long term\n",
      "- Adjustments to investment strategies while preserving long-term investments that can drive change for customers, shareholders, and employees \n",
      "\n",
      "On AWS, they have made optimizations and improvements, such as leveraging better technology like Graviton chips, that while having a short-term negative impact on revenue, was best for their customers in the long run and should pay off in the future.\n",
      "\n",
      "Additionally, they had a significant delivery year and made various advancements and announcements of their next-generation generalized offerings. \n",
      "\n",
      "Would you like to know more about any of these points?\n",
      "\n",
      "\n",
      "[Document(page_content='Dear shareholders:\\nAs I sit down to write my second annual shareholder letter as CEO, I find myself optimistic and energized\\nby what lies ahead for Amazon. Despite 2022 being one of the harder macroeconomic years in recent memory,and with some of our own operating challenges to boot, we still found a way to grow demand (on top ofthe unprecedented growth we experienced in the first half of the pandemic). We innovated in our largestbusinesses to meaningfully improve customer experience short and long term. And, we made importantadjustments in our investment decisions and the way in which we’ll invent moving forward, while stillpreserving the long-term investments that we believe can change the future of Amazon for customers,\\nshareholders, and employees.\\nWhile there were an unusual number of simultaneous challenges this past year, the reality is that if you', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='cloud more efficiently and leverage more powerful, price-performant AWS capabilities like Graviton chips(our generalized CPU chips that provide ~40% better price-performance than other leading x86 processors),S3 Intelligent Tiering (a storage class that uses AI to detect objects accessed less frequently and store themin less expensive storage layers), and Savings Plans (which give customers lower prices in exchange for longercommitments). This work diminished short-term revenue, but was best for customers, much appreciated,and should bode well for customers and AWS longer-term. By the end of 2023, we saw cost optimizationattenuating, new deals accelerating, customers renewing at larger commitments over longer time periods, andmigrations growing again.\\nThe past year was also a significant delivery year for AWS. We announced our next generation of generalized', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Based on the latest shareholder letter, provide me some highlights of the business in the year 2023\"\n",
    "result = mv_qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f44ca2-e588-491f-9382-2e7136df6920",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c868c8-cf34-42f6-a06f-443e04a195f4",
   "metadata": {},
   "source": [
    "### 5.2 Parent Document Retriever Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26568540-3442-41dc-ae12-ca8e624d5fff",
   "metadata": {},
   "source": [
    "In this scenario, let's have a look at a more advanced rag option with the help of [ParentDocumentRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever). When working with document retrieval, you may encounter a trade-off between storing small chunks of a document for accurate embeddings and larger documents to preserve more context. The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. \n",
    "\n",
    "First, a `parent_splitter` is used to divide the original documents into larger chunks called `parent documents.` These parent documents can preserve a reasonable amount of context so the LLM can.\n",
    "\n",
    "Next, a `child_splitter` is applied to create smaller `child documents` from the original documents. These child documents allow the embeddings to reflect more accurately their meaning.\n",
    "\n",
    "The child documents are then indexed in a vectorstore using embeddings. This enables efficient retrieval of relevant child documents based on similarity.\n",
    "\n",
    "To retrieve relevant information, the `ParentDocumentRetriever` first fetches the child documents from the vectorstore. It then looks up the parent IDs for those child documents and returns the corresponding larger parent documents.\n",
    "\n",
    "The `ParentDocumentRetriever` uses an [InMemoryStore](https://api.python.langchain.com/en/v0.1.4/storage/langchain.storage.in_memory.InMemoryBaseStore.html) to store and manage the parent documents. By working with both parent and child documents, this approach aims to balance accurate embeddings with contextual information, providing more meaningful and relevant retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1166d969-e079-4eaa-9479-b69a0ef05b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7358d-00b3-4778-a981-8decddb5e1ec",
   "metadata": {},
   "source": [
    "Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10b5f339-b513-4ba5-b262-82d504dbd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    child_splitter.split_documents(documents),\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3d830d8-5101-4103-8958-f960c2728cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "pdr_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore_faiss,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15243983-e5e5-4024-9ecb-6b965827684c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdr_retriever.add_documents(documents, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46bb5a-efb7-47d4-8d0d-3ec1afa95f25",
   "metadata": {},
   "source": [
    "Let’s now call the vector store search functionality - we should see that it returns small chunks (since we’re storing the small chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80d7d44f-0669-4d22-9efc-64ee3b0ef247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore_faiss.similarity_search(\"Generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ada7c3d6-7eda-4d1a-8c0b-9889daec7f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6d2a1c9-ea18-47a6-ade7-d801c712e700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and Generative AI . Machine learning has been a technology with high promise for several decades, but it’s\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a54e4-0ac6-48b2-afb2-7568a0364ac0",
   "metadata": {},
   "source": [
    "Let’s now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54413ac3-0012-4b31-b926-1de798e3572c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdr_retrieved_docs = pdr_retriever.invoke(\"Generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "810c388a-494a-4a3f-aed1-a59e3bbccb04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1879"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdr_retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fab565fb-0240-4048-818a-da6a6b56b9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and Generative AI . Machine learning has been a technology with high promise for several decades, but it’s\n",
      "only been the last five to ten years that it’s started to be used more pervasively by companies. This shift wasdriven by several factors, including access to higher volumes of compute capacity at lower prices than was everavailable. Amazon has been using machine learning extensively for 25 years, employing it in everythingfrom personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically general and broad recall andlearning capabilities. We have been working on our own LLMs for a while now, believe it will transform andimprove virtually every customer experience, and will continue to invest substantially in these modelsacross all of our consumer, seller, brand, and creator experiences. Additionally, as we’ve done for years inAWS, we’re democratizing this technology so companies of all sizes can leverage Generative AI. AWS isoffering the most price-performant machine learning chips in Trainium and Inferentia so small and largecompanies can afford to train and run their LLMs in production. We enable companies to choose fromvarious LLMs and build applications with all of the AWS security, privacy and other features that customersare accustomed to using. And, we’re delivering applications like AWS’s CodeWhisperer, which revolutionizes\n"
     ]
    }
   ],
   "source": [
    "print(pdr_retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3152a7-d33d-4eaf-81fc-15e174d119d1",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ParentDocumentRetriever`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d573ec9-5865-4dc7-9a47-183b70afce7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdr_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=pdr_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c73af7-fde2-4f30-8dba-c9f3d1dfac83",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "320966d5-e056-452d-81ed-1cb67c873f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The following are some key highlights of Amazon's business in 2023:\n",
      "\n",
      "\n",
      "- Launch of two end-to-end prototype satellites as part of Project Kuiper, with all key systems validated, a significant milestone in the company's journey to provide broadband connectivity to underserved areas\n",
      "- On track to launch the first production satellites in 2024 as part of Project Kuiper\n",
      "- Expansion of Amazon's grocery business, including the Whole Foods Market and Amazon Fresh, to meet the needs of customers who prefer to shop for groceries in physical stores\n",
      "- Investment in Amazon Business, leveraging the company's e-commerce and logistics capabilities to serve businesses and governments seeking better connectivity and performance\n",
      "\n",
      "\n",
      "Overall, Amazon had strong results in 2023, driven by the collective efforts of its teams to deliver for customers, and is encouraged by its progress and innovation. \n",
      "\n",
      "Can I help you with anything else? \n",
      "\n",
      "[Document(page_content='In October, we hit a major milestone in our journey to commercialize Project Kuiper when we launched two\\nend-to-end prototype satellites into space, and successfully validated all key systems and sub-systems—\\nrare in an initial launch like this. Kuiper is our low Earth orbit satellite initiative that aims to providebroadband connectivity to the 400-500 million households who don’t have it today (as well as governmentsand enterprises seeking better connectivity and performance in more remote areas), and is a very large revenueopportunity for Amazon. We’re on track to launch our first production satellites in 2024. We’ve still got along way to go, but are encouraged by our progress.\\nOverall, 2023 was a strong year, and I’m grateful to our collective teams who delivered on behalf of\\ncustomers. These results represent a lot of invention, collaboration, discipline, execution, and reimagination', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='Beyond geographic expansion, we’ve been working to expand our customer offerings across some large,\\nunique product retail market segments. Grocery is an $800B market segment in the US alone, with the average\\nhousehold shopping three to four times per week. Amazon has built a somewhat unusual, but significantgrocery business over nearly 20 years. Similar to how other mass merchants entered the grocery space in the1980s, we began by adding products typically found in supermarket aisles that don’t require temperaturecontrol such as paper products, canned and boxed food, candy and snacks, pet care, health and personal care,and beauty. However, we offer more than three million items compared to a typical supermarket’s 30K forthe same categories. To date, we’ve also focused on larger pack sizes, given the current cost to serve onlinedelivery. While we’re pleased with the size and growth of our grocery business, we aspire to serve more ofour customers’ grocery needs than we do today. To do so, we need a broader physical store footprint given thatmost of the grocery shopping still happens in physical venues. Whole Foods Market pioneered the naturaland organic specialty grocery store concept 40 years ago. Today, it’s a large and growing business that continuesto raise the bar for healthy and sustainable food. Over the past year, we’ve continued to invest in thebusiness while also making changes to drive better profitability. Whole Foods is on an encouraging path,but to have a larger impact on physical grocery, we must find a mass grocery format that we believe is worthexpanding broadly. Amazon Fresh is the brand we’ve been experimenting with for a few years, and we’reworking hard to identify and build the right mass grocery format for Amazon scale. Grocery is a big growthopportunity for Amazon.\\nAmazon Business is another example of an investment where our ecommerce and logistics capabilities', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Provide me some highlights of the business in the year 2023\"\n",
    "result = pdr_qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b2233-d801-4692-885f-da9f96844bb9",
   "metadata": {},
   "source": [
    "### 5.3 Contextual Compression Chain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f39ba-f12e-40ac-929b-5d82a208c0f1",
   "metadata": {},
   "source": [
    "Contextual Compression is the final Retriever we will be looking at. One challenge with retrieval is that usually you don’t know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "`Contextual compression` is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the `Contextual Compression Retriever`, you’ll need: - a `base retriever` - a `Document Compressor`\n",
    "\n",
    "The `Contextual Compression Retriever` passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The `Contextual Compression Retriever` addresses the challenge of retrieving relevant information from a document storage system, where the pertinent data may be buried within documents containing a lot of irrelevant text. By compressing and filtering the retrieved documents based on the given query context, only the most relevant information is returned.\n",
    "To utilize the `Contextual Compression Retriever`, you'll need:\n",
    "\n",
    "- **A base retriever**: This is the initial retriever that fetches documents from the storage system based on the query.\n",
    "- **A Document Compressor**: This component takes the initially retrieved documents and shortens them by reducing the contents of individual documents or dropping irrelevant documents altogether, using the query context to determine relevance.\n",
    "\n",
    "The workflow is as follows: The query is passed to the base retriever, which fetches a set of potentially relevant documents. These documents are then fed into the Document Compressor, which compresses and filters them based on the query context. The resulting compressed and filtered documents, containing only the most relevant information, are then returned for further processing or use in downstream applications.\n",
    "\n",
    "By employing contextual compression, the `Contextual Compression Retriever` improves the quality of responses, reducing the cost of LLM calls, and enhancing the overall efficiency of the retrieval process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd35b19-4bb0-411d-89f1-a35b7d178261",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adding contextual compression with an LLMChainExtractor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33bfa7-108a-4bb8-828f-dfea005b2cde",
   "metadata": {},
   "source": [
    "Now let’s wrap our base retriever with a `ContextualCompressionRetriever`. We’ll add an [LLMChainExtractor](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html), which will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15ce0654-eeb1-486a-b9ec-15f3149ecb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Amazon has been using ML for 25 years and offers many machine learning services (like generative AI), and so has broad ML functionality and customer base. \\n(None of the other info was relevant to the question, so I didn't extract it as per instructions.)\", metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='I’m not sure how to answer your question. I am an AI chatbot and do not have real-time access to information on the internet. However, I can extract relevant information from the given context to provide you with some information. \\n\\nIs there anything else I can help you with?', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content=\"I'm not sure which parts of the context are most relevant, but here are some possibilities:\\n\\n- I could write an entire letter about LLMs and Generative AI, so they must be important topics for Amazon.\\n- LLMs and Generative AI will have a significant impact on customers and shareholders.\\n\\nIs there anything else you would like to know about the context?\", metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='The extracted relevant parts of the context are:\\n\\n- “We also announced AWS Trainium2 chips, which will deliver up to four times faster machine learning training for generative AI applications and three times more memory capacity than Trainium1.”\\n- “In Generative AI (GenAI), we added dozens of features to Amazon SageMaker to make it easier for developers to build new Foundation Models (FMs).”\\n\\nThese parts of the context talk about the development of generative AI and its relation to machine learning and Foundation Models.\\n\\nWould you like to extract more information from the provided context?', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "retriever = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \" How has generative AI impacted AWS?\"\n",
    ")\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6fae4-40f7-4808-a7da-4bd5afb1bab9",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ContextualCompressionRetriever` with an `LLMChainExtractor`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a42b3afe-f18c-4762-9888-de060a83635b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcc75d-1b1a-47d6-b14c-d83b6e320a09",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c70b9e88-5d5e-4e74-bbf4-ac01bf3ae129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some of the key highlights of Amazon's business in the year 2023, based on the shareholder letter:\n",
      "\n",
      "\n",
      "1. Total revenue increased by 12% compared to 2022.\n",
      "2. The North America segment, which generates the majority of Amazon's revenue, grew 12% year-over-year.\n",
      "3. Amazon's international segment saw an 11% revenue growth.\n",
      "4. AWS (Amazon Web Services) revenue increased by 13% compared to 2022.\n",
      "5. Amazon experienced a dramatic improvement in operating income and free cash flow.\n",
      "\n",
      "The answer to the question is not available. \n",
      "\n",
      "[Document(page_content='Dear shareholders:\\nAs I sit down to write my second annual shareholder letter as CEO, I find myself optimistic and energized\\nby what lies ahead for Amazon. Despite 2022 being one of the harder macroeconomic years in recent memory,and with some of our own operating challenges to boot, we still found a way to grow demand (on top ofthe unprecedented growth we experienced in the first half of the pandemic). We innovated in our largestbusinesses to meaningfully improve customer experience short and long term. And, we made importantadjustments in our investment decisions and the way in which we’ll invent moving forward, while stillpreserving the long-term investments that we believe can change the future of Amazon for customers,\\nshareholders, and employees.\\nWhile there were an unusual number of simultaneous challenges this past year, the reality is that if you', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='Dear Shareholders:\\nLast year at this time, I shared my enthusiasm and optimism for Amazon’s future. Today, I have even more.\\nThe reasons are many, but start with the progress we’ve made in our financial results and customerexperiences, and extend to our continued innovation and the remarkable opportunities in front of us.\\nIn 2023, Amazon’s total revenue grew 12% year-over-year (“Y oY”) from $514B to $575B. By segment, North\\nAmerica revenue increased 12% Y oY from $316B to $353B, International revenue grew 11% Y oY from$118B to $131B, and AWS revenue increased 13% Y oY from $80B to $91B.\\nFurther, Amazon’s operating income and Free Cash Flow (“FCF”) dramatically improved. Operating\\nincome in 2023 improved 201% Y oY from $12.2B (an operating margin of 2.4%) to $36.9B (an operatingmargin of 6.4%). Trailing Twelve Month FCF adjusted for equipment finance leases improved from -$12.8Bin 2022 to $35.5B (up $48.3B).', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Based on the latest shareholder letter, provide me some highlights of the business in the year 2023\"\n",
    "result = cc_qa.invoke({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c909c11-34d3-440d-bf3e-87325697ebd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### More built-in compressors: filters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e1328-ae7d-4127-8556-5c8a18e83222",
   "metadata": {},
   "source": [
    "##### LLMChainFilter\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84ad22-b15f-4099-88db-f937011aa68d",
   "metadata": {},
   "source": [
    "The [LLMChainFilter](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html) is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69b189c2-a0d4-4e78-930a-aaa925ed499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Sometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question. However, a question people never ask, and might be evenmore interesting is what’s the next set of primitives you’re building that enables breakthrough customer\\nexperiences? If you asked me today, I’d lead with Generative AI (“GenAI”).\\nMuch of the early public attention has focused on GenAI applications , with the remarkable 2022 launch of\\nChatGPT. But, to our “primitive” way of thinking, there are three distinct layers in the GenAI stack, each ofwhich is gigantic, and each of which we’re deeply investing.\\nThebottom layer is for developers and companies wanting to build foundation models (“FMs”). The', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='only been the last five to ten years that it’s started to be used more pervasively by companies. This shift wasdriven by several factors, including access to higher volumes of compute capacity at lower prices than was everavailable. Amazon has been using machine learning extensively for 25 years, employing it in everythingfrom personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air,to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learningfunctionality and customer base of any cloud provider). More recently, a newer form of machine learning,called Generative AI, has burst onto the scene and promises to significantly accelerate machine learningadoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billionsof parameters, and growing), across expansive datasets, and has radically general and broad recall andlearning capabilities. We have been working on our own', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'}), Document(page_content='on-premises. These businesses will keep shifting online and into the cloud. In Media and Advertising,\\ncontent will continue to migrate from linear formats to streaming. Globally, hundreds of millions of peoplewho don’t have adequate broadband access will gain that connectivity in the next few years. Last butcertainly not least, Generative AI may be the largest technology transformation since the cloud (whichitself, is still in the early stages), and perhaps since the Internet. Unlike the mass modernization of on-premises infrastructure to the cloud, where there’s work required to migrate, this GenAI revolution will bebuilt from the start on top of the cloud. The amount of societal and business benefit from the solutions thatwill be possible will astound us all.\\nThere has never been a time in Amazon’s history where we’ve felt there is so much opportunity to make our', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'}), Document(page_content='developer productivity by generating code suggestions in real time. I could write an entire letter on LLMs\\nand Generative AI as I think they will be that transformative, but I’ll leave that for a future letter. Let’s justsay that LLMs and Generative AI are going to be a big deal for customers, our shareholders, and Amazon.\\nSo, in closing, I’m optimistic that we’ll emerge from this challenging macroeconomic time in a stronger', metadata={'year': 2022, 'source': 'AMZN-2022-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Generative AI\"\n",
    ")\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241230b-5a28-4093-9e8c-5469cecbba07",
   "metadata": {},
   "source": [
    "Now, let's initialize the chain using the `ContextualCompressionRetriever` with an `LLMChainFilter`. We will pass the prompt in via the chain_type_kwargs argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d724d950-0047-42c9-b1ef-355485080d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495a8c-c96f-4c96-bbe0-df847fc9ed38",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "753ca7c3-d7e1-41b7-abfc-879e1eacf573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some of the most important highlights from the letter for the year 2023:\n",
      "\n",
      "1. Total revenue increased 12% year-over-year, reaching $575B. This growth was driven by all segments of the business, with a notable increase in revenue from North America (12% increase) and AWS (13% increase).\n",
      "\n",
      "2. Operating income improved significantly, with a margin of 6.4% in 2023 compared to 2.4% in 2022. \n",
      "\n",
      "3. The company's Free Cash Flow (FCF) improved dramatically, going from -$12.8B in 2022 to $35.5B in 2023. \n",
      "\n",
      "These financial metrics suggest that Amazon's business is growing stronger and more profitable, and that the company is well-positioned for continued success in the future.\n",
      "\n",
      "Would you like to know anything else from the letter? \n",
      "\n",
      "[Document(page_content='Dear shareholders:\\nOver the past 25 years at Amazon, I’ve had the opportunity to write many narratives, emails, letters, and\\nkeynotes for employees, customers, and partners. But, this is the first time I’ve had the honor of writing ourannual shareholder letter as CEO of Amazon. Jeff set the bar high on these letters, and I will try to keepthem worth reading.\\nWhen the pandemic started in early 2020, few people thought it would be as expansive or long-running as', metadata={'year': 2021, 'source': 'AMZN-2021-Shareholder-Letter.pdf'}), Document(page_content='Dear Shareholders:\\nLast year at this time, I shared my enthusiasm and optimism for Amazon’s future. Today, I have even more.\\nThe reasons are many, but start with the progress we’ve made in our financial results and customerexperiences, and extend to our continued innovation and the remarkable opportunities in front of us.\\nIn 2023, Amazon’s total revenue grew 12% year-over-year (“Y oY”) from $514B to $575B. By segment, North\\nAmerica revenue increased 12% Y oY from $316B to $353B, International revenue grew 11% Y oY from$118B to $131B, and AWS revenue increased 13% Y oY from $80B to $91B.\\nFurther, Amazon’s operating income and Free Cash Flow (“FCF”) dramatically improved. Operating\\nincome in 2023 improved 201% Y oY from $12.2B (an operating margin of 2.4%) to $36.9B (an operatingmargin of 6.4%). Trailing Twelve Month FCF adjusted for equipment finance leases improved from -$12.8Bin 2022 to $35.5B (up $48.3B).', metadata={'year': 2023, 'source': 'AMZN-2023-Shareholder-Letter.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Based on the latest shareholder letter, provide me some highlights of the business in the year 2023\"\n",
    "result = filter_qa.invoke({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe1bfe-915c-4c03-974c-8a36cae15948",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Overall, we notice that responses are lacking some level of detail. This is due to the use of LangChain's `BedrockLLM` class for chat completion tasks. Cohere Command R an R+ models are being added to LangChain's `ChatBedrock` class and will be updated in this notebook in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e4c55-0f80-47eb-83f7-6ae28fedf57f",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d260d-8771-41ae-b173-0f81228b28dc",
   "metadata": {},
   "source": [
    "In this notebook, we presented a solution that allows you to leverage Cohere's Command models and their state of the art Embeddings model to implement different langchain retrieval algorithms into your retrieval chains to enhance the ability of the models to process and generate information. We also explored using persistent storage for embeddings and document chunks and integration with enterprise data stores. Overall, we notice gthat while using retrievers, they are able to provide more detailed responses as opposed to vectore store backed retriever and wrapper, which give answers that do not provide explicit examples from the documents. The retrievers we used not only refine the way LLM models access and incorporate external knowledge, but also significantly improve the quality, relevance, and efficiency of their outputs. By combining retrieval from large text corpora with language generation capabilities, these advanced RAG techniques enable LLMs to produce more factual, coherent, and context-appropriate responses, enhancing their performance across various natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0319f2-5309-45ba-a15c-2ff7b3720133",
   "metadata": {},
   "source": [
    "### Take-aways\n",
    "---\n",
    "- Experiment with different retrieval techniques\n",
    "- Leverage `Cohere Command` and `Cohere-Embed-english` models available under Amazon SageMaker JumpStart\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
