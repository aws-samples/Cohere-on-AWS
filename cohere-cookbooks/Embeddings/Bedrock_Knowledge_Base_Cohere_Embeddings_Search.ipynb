{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0deaf99b-0107-48a7-8844-10c7e87e87f6",
   "metadata": {},
   "source": [
    "# Using Knowledge Bases for Amazon Bedrock, Amazon OpenSearch Serverless, and Cohere Embed\n",
    "---\n",
    "# Introduction\n",
    "This notebook builds on the content in the [Text Embeddings using Cohere LLM stored in Amazon OpenSearch Serverless](https://github.com/aws-samples/Cohere-on-AWS/blob/main/cohere-cookbooks/Embeddings/Cohere_Embeddings_Search.ipynb) notebook. Amazon OpenSearch Serverless allows developers to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. OpenSearch Serverless delivers millisecond response times with the simplicity of a serverless environment, making it ideal as a vector store for Retrieval-Augmented Generation (RAG). Using OpenSearch also allows developers to take advantage of visualization and monitoring through OpenSearch Dashboard features that developers may already be familiar with. In addition, OpenSearch Serverless is cost-effective because you only pay for the resources you consume. There is no need for upfront provisioning and overprovisioning for peak workloads. OpenSearch Serverless also automatically updates your collections to consume the latest bug fixes, features, and performance improvements.\n",
    "\n",
    "Within the AWS ecosystem, there are different ways to use Amazon OpenSearch Serverless. One way is to use Amazon OpenSearch Serverless as a vector store within a [Knowledge Base for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/). A Knowledge Base is fully managed Retrieval-Augmented Generation (RAG) capability that allows you to connect to foundation models (FMs) to deliver more relevant, context-specific, and accurate responses. \n",
    "\n",
    "This notebook focuses on using the Cohere Embed Multilingual V3 LLM (Large Language Model) to create embedings stored in a Knowledge Base for Amazon Bedrock powered by an Amazon OpenSearch Serverles vector store. The goal is to help developers generate accurate responses without performing undifferentiated steps to implement RAG. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e189327-6df5-4e6a-8549-b1f85b5be0e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prerequisites\n",
    "1.  Ensure you have requested access to the models provided by Cohere in the Bedrock console by clicking \"model access.\" Instructions can be found here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n",
    "1.  Make sure you have the permissions to access Bedrock and you have the correct IAM permissions from your administrator.\n",
    "2. Run the following cell to install boto3 and necessary packages.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e760aab-3672-42d5-b86e-be5d96ee3196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opensearch-py==2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.26.19)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.32.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.9.0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: boto3==1.33.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.33.2)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (1.33.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.2->boto3==1.33.2) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.2->boto3==1.33.2) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.2->boto3==1.33.2) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: retrying==1.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: six>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from retrying==1.3.4) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "import random\n",
    "from retrying import retry\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install dependencies\n",
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U boto3==1.33.2\n",
    "%pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f3a23-f0b1-4f20-9941-56f9ec00b6c3",
   "metadata": {},
   "source": [
    "## Step 0: Configure permissions by creating helper functions \n",
    "One benefit of using an Amazon Bedrock Knowledge Base is that users have centralized control over permissions through AWS Identity and Access Management (IAM). The helper functions below allow Amazon Bedrock to access resources such as Amazon Simple Storage Service (S3) and Amazon OpenSearch Serverless (AOSS). By using Knowledge Bases, users also do not need to perform undifferentiated steps such as configuring an orchestrator framework such as LangChain.\n",
    "\n",
    "Run the cell below to configure the permissions required to create a Knowledge Base from this Python notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f54ac0-5b87-404e-a14f-85b18003b6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The create_bedrock_execution_role function creates an IAM role that Bedrock can assume to access resources like S3 and OpenSearch. \n",
    "# This role has policies to allow access to specific S3 bucket and the Amazon Cohere Embed Multilingual V3 LLM.\n",
    "def create_bedrock_execution_role(bucket_name):\n",
    "    foundation_model_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    # f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\",\n",
    "                    # f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-multilingual-v3\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    s3_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket_name}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # create policies based on the policy documents\n",
    "    fm_policy = iam_client.create_policy(\n",
    "        PolicyName=fm_policy_name,\n",
    "        PolicyDocument=json.dumps(foundation_model_policy_document),\n",
    "        Description='Policy for accessing foundation model',\n",
    "    )\n",
    "\n",
    "    s3_policy = iam_client.create_policy(\n",
    "        PolicyName=s3_policy_name,\n",
    "        PolicyDocument=json.dumps(s3_policy_document),\n",
    "        Description='Policy for reading documents from s3')\n",
    "\n",
    "    # create bedrock execution role\n",
    "    bedrock_kb_execution_role = iam_client.create_role(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "        Description='Amazon Bedrock Knowledge Base Execution Role for accessing OSS and S3',\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "\n",
    "    # fetch arn of the policies and role created above\n",
    "    bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "    s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "    fm_policy_arn = fm_policy[\"Policy\"][\"Arn\"]\n",
    "    \n",
    "\n",
    "    # attach policies to Amazon Bedrock execution role\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=s3_policy_arn\n",
    "    )\n",
    "    return bedrock_kb_execution_role\n",
    "\n",
    "\n",
    "# The create_oss_policy_attach_bedrock_execution_role creates an IAM policy granting access to a specific OpenSearch collection\n",
    "# The IAM policy created is attached to the Bedrock execution role.\n",
    "def create_oss_policy_attach_bedrock_execution_role(collection_id, bedrock_kb_execution_role):\n",
    "    # define oss policy document\n",
    "    oss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:APIAccessAll\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:aoss:{region_name}:{account_number}:collection/{collection_id}\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    oss_policy = iam_client.create_policy(\n",
    "        PolicyName=oss_policy_name,\n",
    "        PolicyDocument=json.dumps(oss_policy_document),\n",
    "        Description='Policy for accessing opensearch serverless',\n",
    "    )\n",
    "    oss_policy_arn = oss_policy[\"Policy\"][\"Arn\"]\n",
    "    print(\"Opensearch serverless arn: \", oss_policy_arn)\n",
    "\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=oss_policy_arn\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "# The create_policies_in_oss function creates OpenSearch policies for encryption, network access, and data access for a vector store.\n",
    "def create_policies_in_oss(vector_store_name, aoss_client, bedrock_kb_execution_role_arn):\n",
    "    encryption_policy = aoss_client.create_security_policy(\n",
    "        name=encryption_policy_name,\n",
    "        policy=json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                           'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type='encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name=network_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                            'ResourceType': 'collection'}],\n",
    "                 'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type='network'\n",
    "    )\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name=access_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity, bedrock_kb_execution_role_arn],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type='data'\n",
    "    )\n",
    "    return encryption_policy, network_policy, access_policy\n",
    "\n",
    "\n",
    "def delete_iam_role_and_policies():\n",
    "    fm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{fm_policy_name}\"\n",
    "    s3_policy_arn = f\"arn:aws:iam::{account_number}:policy/{s3_policy_name}\"\n",
    "    oss_policy_arn = f\"arn:aws:iam::{account_number}:policy/{oss_policy_name}\"\n",
    "    sm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{sm_policy_name}\"\n",
    "\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=s3_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=oss_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=sm_policy_arn\n",
    "    )\n",
    "    iam_client.delete_role(RoleName=bedrock_execution_role_name)\n",
    "    iam_client.delete_policy(PolicyArn=s3_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=fm_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=oss_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=sm_policy_arn)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = ''\n",
    "    for i in range(seconds):\n",
    "        dots += '.'\n",
    "        print(dots, end='\\r')\n",
    "        time.sleep(1)\n",
    "\n",
    "def create_bedrock_execution_role_multi_ds(bucket_names = None, secrets_arns = None):\n",
    "    \n",
    "    # 0. Create bedrock execution role\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # create bedrock execution role\n",
    "    bedrock_kb_execution_role = iam_client.create_role(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "        Description='Amazon Bedrock Knowledge Base Execution Role for accessing OSS, secrets manager and S3',\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "\n",
    "    # fetch arn of the role created above\n",
    "    bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "    # 1. Cretae and attach policy for foundation models\n",
    "    foundation_model_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-multilingual-v3\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    fm_policy = iam_client.create_policy(\n",
    "        PolicyName=fm_policy_name,\n",
    "        PolicyDocument=json.dumps(foundation_model_policy_document),\n",
    "        Description='Policy for accessing foundation model',\n",
    "    )\n",
    "  \n",
    "    # fetch arn of this policy \n",
    "    fm_policy_arn = fm_policy[\"Policy\"][\"Arn\"]\n",
    "    \n",
    "    # attach this policy to Amazon Bedrock execution role\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "\n",
    "    # 2. Cretae and attach policy for s3 bucket\n",
    "    if bucket_names:\n",
    "        s3_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [item for sublist in [[f'arn:aws:s3:::{bucket}', f'arn:aws:s3:::{bucket}/*'] for bucket in bucket_names] for item in sublist], \n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # create policies based on the policy documents\n",
    "        s3_policy = iam_client.create_policy(\n",
    "            PolicyName=s3_policy_name,\n",
    "            PolicyDocument=json.dumps(s3_policy_document),\n",
    "            Description='Policy for reading documents from s3')\n",
    "\n",
    "        # fetch arn of this policy \n",
    "        s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "        \n",
    "        # attach this policy to Amazon Bedrock execution role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "            PolicyArn=s3_policy_arn\n",
    "        )\n",
    "\n",
    "    # 3. Cretae and attach policy for secrets manager\n",
    "    if secrets_arns:\n",
    "        secrets_manager_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"secretsmanager:GetSecretValue\",\n",
    "                        \"secretsmanager:PutSecretValue\"\n",
    "                    ],\n",
    "                    \"Resource\": secrets_arns\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # create policies based on the policy documents\n",
    "        \n",
    "        secrets_manager_policy = iam_client.create_policy(\n",
    "            PolicyName=sm_policy_name,\n",
    "            PolicyDocument=json.dumps(secrets_manager_policy_document),\n",
    "            Description='Policy for accessing secret manager',\n",
    "        )\n",
    "\n",
    "        # fetch arn of this policy\n",
    "        sm_policy_arn = secrets_manager_policy[\"Policy\"][\"Arn\"]\n",
    "\n",
    "        # attach policy to Amazon Bedrock execution role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "            PolicyArn=sm_policy_arn\n",
    "        )\n",
    "    \n",
    "    return bedrock_kb_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47dff7d-82f2-4312-aea7-4972be784b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a suffix to use as a unique name for the S3 bucket\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "# Initialize a boto3 session.\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Initialize the AWS Identity and Access Management (IAM) client\n",
    "iam_client = boto3_session.client('iam')\n",
    "account_number = boto3.client('sts').get_caller_identity().get('Account')\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "# Initialize f-strings to be used as variable names\n",
    "encryption_policy_name = f\"bedrock-sample-rag-sp-{suffix}\"\n",
    "network_policy_name = f\"bedrock-sample-rag-np-{suffix}\"\n",
    "access_policy_name = f'bedrock-sample-rag-ap-{suffix}'\n",
    "bedrock_execution_role_name = f'AmazonBedrockExecutionRoleForKnowledgeBase_{suffix}'\n",
    "fm_policy_name = f'AmazonBedrockFoundationModelPolicyForKnowledgeBase_{suffix}'\n",
    "s3_policy_name = f'AmazonBedrockS3PolicyForKnowledgeBase_{suffix}'\n",
    "sm_policy_name = f'AmazonBedrockSecretPolicyForKnowledgeBase_{suffix}'\n",
    "oss_policy_name = f'AmazonBedrockOSSPolicyForKnowledgeBase_{suffix}'\n",
    "\n",
    "# Initialize a AWS Security Token Service client for temporary credentials\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}' # replace it with your bucket name.\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05eb0ab-1eaa-4b90-8d63-c3e5c49c8bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-east-1-809719347864 Exists\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except ClientError as e:\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    if region_name == \"us-east-1\":\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name)\n",
    "    else:\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2015f034-9af6-4678-bacc-eb5655331727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_name' (str)\n"
     ]
    }
   ],
   "source": [
    "# Store the bucket name\n",
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0ba97-b4a1-4196-99b8-a60e622ebc2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Create the Amazon OpenSearch Vector Store\n",
    "OpenSearch Serverless continually adjusts to get millisecond response times during changing usage patterns and demand. This makes it an ideal solution for a RAG-based workflow. Run the cell below to use the f-strings initialized in Step 0 to create the vector store. An Amazon OpenSearch Serverless collection is a logical grouping of one or more indexes that represent an analytics workload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d4c5e7-6779-42b3-b862-5be74fe9304f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Fri, 30 Aug 2024 05:23:04 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': 'cf2ac17e-bc6b-4a44-8813-f0c5a59be626'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': 'cf2ac17e-bc6b-4a44-8813-f0c5a59be626',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:809719347864:collection/8gtddkrsd44cx7fqnl4k',\n",
      "                              'createdDate': 1724995384485,\n",
      "                              'id': '8gtddkrsd44cx7fqnl4k',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1724995384485,\n",
      "                              'name': 'bedrock-sample-rag-749',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "# Create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "# Print the OpenSearch vector search collection\n",
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3e0239-a47e-442f-84e0-9cb0878e8a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encryption_policy' (dict)\n",
      "Stored 'network_policy' (dict)\n",
      "Stored 'access_policy' (dict)\n",
      "Stored 'collection' (dict)\n"
     ]
    }
   ],
   "source": [
    "# Store the encryption policy, network policy, access policy, and collection variables\n",
    "%store encryption_policy network_policy access_policy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7100c9-c42e-4cd3-8a28-644533f6266d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8gtddkrsd44cx7fqnl4k.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0293e36a-54ed-498d-b8b8-40f96ec6aeec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "..............................\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:809719347864:collection/8gtddkrsd44cx7fqnl4k',\n",
      "    'collectionEndpoint': 'https://8gtddkrsd44cx7fqnl4k.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1724995384485,\n",
      "    'dashboardEndpoint': 'https://8gtddkrsd44cx7fqnl4k.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': '8gtddkrsd44cx7fqnl4k',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1724995408124,\n",
      "    'name': 'bedrock-sample-rag-749',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# Wait for collection creation. This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261ac7ba-839c-4331-8768-49a63f395793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::809719347864:policy/AmazonBedrockOSSPolicyForKnowledgeBase_749\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa4b98-cf3f-45f5-b1bc-55dc7dcc710b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Create Vector Index\n",
    "In Step 1, we created a vector store. This is where we will be storing the index which powers the Knowledge Base for Amazon Bedrock. An index is a way to structure your data. In this case, the data we need to store and structure are the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aabdd8e-ff83-4bb5-b4ac-2970788d57f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-749',\n",
      "  'shards_acknowledged': True}\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "# Use the credentials from the boto3 session.\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            # \"dimension\": 1536,\n",
    "            \"dimension\": 1024,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "# An OpenSearch client interfaces with OpenSearch to perform actions such as indexing, searching, or updating data. \n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548a601-7753-47aa-8593-8b8714c8f4f2",
   "metadata": {},
   "source": [
    "## Step 3: Download data to ingest into our Knowledge Base\n",
    "Now that we have created the Knowledge Base, it is time to configure data for testing. Note that it is also possible to test your own documents by directly uploading these documents to the S3 bucket created in step 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f1d736-e68b-4a1b-971b-29a9dfcee840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7159ca2-cece-4457-93f2-c2cb8a9ab591",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run the cell below to upload the data to the S3 bucket created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2387229e-4013-49f6-8b7f-8b6597675783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload data to s3 to the bucket that was configured as a data source to the knowledge base\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucket_name,file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a0a21-4b7b-4ff8-b0a0-e7bcc6d540d7",
   "metadata": {},
   "source": [
    "## Step 4: Create a Knowledge Base\n",
    "Amazon Bedrock splits your documents or content into manageable chunks for efficient data retrieval. Chunks are converted to embeddings and written to a vector index while maintaining a mapping to the original document. If a single document or piece of content contains less than the specified number of tokens in a chunk, the document is not further split. The overlap percentage controls the overlap tokens that each parent chunk has with its children. You can experiment with these parameters to find the parameters that provide the best quality of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "325775a9-6886-4147-9a3b-25ad14e0af99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ingest strategy - How to ingest data from the data source\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "# embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-multilingual-v3\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n",
    "\n",
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b8870-aa9d-41da-864d-f75b16bb9115",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run the cell below to create the Knowledge Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68ebea06-5daf-454f-b0f1-9240ce0c8b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err=ConflictException('An error occurred (ConflictException) when calling the CreateKnowledgeBase operation: KnowledgeBase with name bedrock-sample-knowledge-base-201 already exists.'), type(err)=<class 'botocore.errorfactory.ConflictException'>\n",
      "{ 'createdAt': datetime.datetime(2024, 8, 30, 4, 12, 31, 775542, tzinfo=tzlocal()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:809719347864:knowledge-base/ODK0AD66TQ',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3'}},\n",
      "  'knowledgeBaseId': 'ODK0AD66TQ',\n",
      "  'name': 'bedrock-sample-knowledge-base-201',\n",
      "  'roleArn': 'arn:aws:iam::809719347864:role/AmazonBedrockExecutionRoleForKnowledgeBase_385',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:809719347864:collection/o2gjsxvnvzk88aolis03',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-201'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 4, 12, 31, 775542, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")\n",
    "    \n",
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec97ea3-5276-4fd8-8808-cacedf2f36a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get KnowledgeBase\n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9924add3-7aba-4891-82d8-3176e60a6b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 8, 30, 4, 28, 38, 922964, tzinfo=tzlocal()),\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-809719347864'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'MHMQOKD4CN',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'ODK0AD66TQ',\n",
      "  'name': 'bedrock-sample-knowledge-base-201',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 4, 28, 38, 922964, tzinfo=tzlocal()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb3997ac-a770-4a1d-b637-a1cea28bb86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e5b4219e-739a-418b-bf74-b60f091ac083',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 30 Aug 2024 04:28:44 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '603',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'e5b4219e-739a-418b-bf74-b60f091ac083',\n",
       "   'x-amz-apigw-id': 'dTiTbHKFoAMEY1g=',\n",
       "   'x-amzn-trace-id': 'Root=1-66d14a7c-4a64679f1abc86f46a88cf7c'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'knowledgeBaseId': 'ODK0AD66TQ',\n",
       "  'dataSourceId': 'MHMQOKD4CN',\n",
       "  'name': 'bedrock-sample-knowledge-base-201',\n",
       "  'status': 'AVAILABLE',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'dataSourceConfiguration': {'type': 'S3',\n",
       "   's3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-809719347864'}},\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}},\n",
       "  'createdAt': datetime.datetime(2024, 8, 30, 4, 28, 38, 922964, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 30, 4, 28, 38, 922964, tzinfo=tzlocal())}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource\n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82855a8e-4abf-48a0-bb8e-77f9586c5204",
   "metadata": {},
   "source": [
    "## Step 5: Start data ingestion \n",
    "After you create your Knowledge Base, you ingest your data source into your Knowledge Base to be queried. Data ingestion converts the raw data in your data source into vector embeddings. You must sync the data sourch each time you add, modify, or remove files so that it is re-indexed to the Knowledge Base. Syncing is incremental, so only added, modified, or deleted documents since the last sync are processed.\n",
    "\n",
    "OpenSearch Serverless uses a cloud-native architecture that separates the indexing (ingest) components from the search (query) components with Amazon S3 as the primary data storage for indexes. Refer to this document to understand more about Amazon OpenSearch Serverless indexing and search compute units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e886f819-9840-4334-8587-1d70708c2bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d390fde-d4d5-43fe-a65e-f54a73add718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'MHMQOKD4CN',\n",
      "  'ingestionJobId': '4OL04V2CD1',\n",
      "  'knowledgeBaseId': 'ODK0AD66TQ',\n",
      "  'startedAt': datetime.datetime(2024, 8, 30, 4, 28, 52, 474536, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 0},\n",
      "  'status': 'STARTING',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 30, 4, 28, 52, 474536, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "452ec9fb-5e89-41e7-9af2-3bea65b11922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'EA1ZIPGJEY',\n",
      "  'ingestionJobId': 'FYFWKD7YZJ',\n",
      "  'knowledgeBaseId': 'GKEHKIWESJ',\n",
      "  'startedAt': datetime.datetime(2024, 8, 28, 12, 58, 1, 652880, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 28, 12, 58, 17, 148888, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d820276b-de54-4ded-b374-c6ffb73d7c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'EA1ZIPGJEY',\n",
      "  'ingestionJobId': 'FYFWKD7YZJ',\n",
      "  'knowledgeBaseId': 'GKEHKIWESJ',\n",
      "  'startedAt': datetime.datetime(2024, 8, 28, 12, 58, 1, 652880, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 28, 12, 58, 17, 148888, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "033f12cf-38db-4e70-b654-3b6fade1769e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "988ab6ec-be85-4887-84ec-13bf7ad38013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ODK0AD66TQ'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97c8bb1f-40cf-4837-9232-6fbb2cb8b279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# Store the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850fd0a-2f04-4227-beaa-a081f9378aa2",
   "metadata": {},
   "source": [
    "## Step 6: Test the Knowledge Base\n",
    "Now that we have indexed data into the Knowledge Base, it is time to test the Knowledge Base with the Retrieve and Generate API. [Here are the models supported by the Retrieve and Generate API](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f99cce9-3a41-422b-9236-5b01c2a69b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Lets see how different Anthropic Claude 3 models responds to the input text we provide\n",
    "claude_model_ids = [ [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"], [\"Claude 3 Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d20147f-def8-4766-a879-60e6e8c7cd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to send a query to a FM using a Knowledge Base.\n",
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48401c09-ec9a-4260-8a14-a90d800fa4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generated using Claude 3 Sonnet:\n",
      "('Amazon has been working on developing its own large language models (LLMs) '\n",
      " 'for generative AI applications. The company believes generative AI will '\n",
      " 'transform and improve virtually every customer experience across its '\n",
      " 'consumer, seller, brand, and creator offerings. Amazon is investing '\n",
      " 'substantially in LLMs and plans to continue doing so. Through its cloud '\n",
      " 'computing arm AWS, Amazon is democratizing generative AI technology by '\n",
      " 'offering price-performant machine learning chips like Trainium and '\n",
      " 'Inferentia. This allows companies of all sizes to afford training and '\n",
      " 'running LLMs in production. AWS also enables companies to choose from '\n",
      " \"various pre-trained LLMs and build applications using AWS's security, \"\n",
      " \"privacy and other features. One example is AWS's CodeWhisperer, which uses \"\n",
      " 'generative AI to provide real-time code suggestions to improve developer '\n",
      " 'productivity.')\n",
      "---------- The citations for the response generated by Claude 3 Sonnet:\n",
      "[ 'The customer reaction to what weve shared thus far about Kuiper has been '\n",
      "  'very positive, and we believe Kuiper represents a very large potential '\n",
      "  'opportunity for Amazon. It also shares several similarities to AWS in that '\n",
      "  'its capital intensive at the start, but has a large prospective consumer, '\n",
      "  'enterprise, and government customer base, significant revenue and operating '\n",
      "  'profit potential, and relatively few companies with the technical and '\n",
      "  'inventive aptitude, as well as the investment hypothesis to go after it.   '\n",
      "  'One final investment area that Ill mention, thats core to setting Amazon '\n",
      "  'up to invent in every area of our business for many decades to come, and '\n",
      "  'where were investing heavily is Large Language Models (LLMs) and '\n",
      "  'Generative AI. Machine learning has been a technology with high promise for '\n",
      "  'several decades, but its only been the last five to ten years that its '\n",
      "  'started to be used more pervasively by companies. This shift was driven by '\n",
      "  'several factors, including access to higher volumes of compute capacity at '\n",
      "  'lower prices than was ever available. Amazon has been using machine '\n",
      "  'learning extensively for 25 years, employing it in everything from '\n",
      "  'personalized ecommerce recommendations, to fulfillment center pick paths, '\n",
      "  'to drones for Prime Air, to Alexa, to the many machine learning services '\n",
      "  'AWS offers (where AWS has the broadest machine learning functionality and '\n",
      "  'customer base of any cloud provider). More recently, a newer form of '\n",
      "  'machine learning, called Generative AI, has burst onto the scene and '\n",
      "  'promises to significantly accelerate machine learning adoption. Generative '\n",
      "  'AI is based on very Large Language Models (trained on up to hundreds of '\n",
      "  'billions of parameters, and growing), across expansive datasets, and has '\n",
      "  'radically general and broad recall and learning capabilities.',\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weve '\n",
      "  'done for years in AWS, were democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, were delivering applications like AWSs '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'Ill leave that for a future letter. Lets just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, Im optimistic that well emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and Ive mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thats $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, its a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weve '\n",
      "  'done for years in AWS, were democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, were delivering applications like AWSs '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'Ill leave that for a future letter. Lets just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, Im optimistic that well emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and Ive mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thats $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, its a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n",
      "---------- Generated using Claude 3 Haiku:\n",
      "('Amazon is heavily investing in large language models (LLMs) and generative '\n",
      " 'AI, which they believe will transform and improve virtually every customer '\n",
      " 'experience. They have been working on their own LLMs and are democratizing '\n",
      " 'this technology so companies of all sizes can leverage generative AI. AWS is '\n",
      " 'offering the most price-performant machine learning chips in Trainium and '\n",
      " 'Inferentia to enable companies to train and run their LLMs in production. '\n",
      " 'Additionally, AWS is delivering applications like CodeWhisperer that use '\n",
      " 'generative AI to revolutionize developer productivity by generating code '\n",
      " 'suggestions in real time.')\n",
      "---------- The citations for the response generated by Claude 3 Haiku:\n",
      "[ 'The customer reaction to what weve shared thus far about Kuiper has been '\n",
      "  'very positive, and we believe Kuiper represents a very large potential '\n",
      "  'opportunity for Amazon. It also shares several similarities to AWS in that '\n",
      "  'its capital intensive at the start, but has a large prospective consumer, '\n",
      "  'enterprise, and government customer base, significant revenue and operating '\n",
      "  'profit potential, and relatively few companies with the technical and '\n",
      "  'inventive aptitude, as well as the investment hypothesis to go after it.   '\n",
      "  'One final investment area that Ill mention, thats core to setting Amazon '\n",
      "  'up to invent in every area of our business for many decades to come, and '\n",
      "  'where were investing heavily is Large Language Models (LLMs) and '\n",
      "  'Generative AI. Machine learning has been a technology with high promise for '\n",
      "  'several decades, but its only been the last five to ten years that its '\n",
      "  'started to be used more pervasively by companies. This shift was driven by '\n",
      "  'several factors, including access to higher volumes of compute capacity at '\n",
      "  'lower prices than was ever available. Amazon has been using machine '\n",
      "  'learning extensively for 25 years, employing it in everything from '\n",
      "  'personalized ecommerce recommendations, to fulfillment center pick paths, '\n",
      "  'to drones for Prime Air, to Alexa, to the many machine learning services '\n",
      "  'AWS offers (where AWS has the broadest machine learning functionality and '\n",
      "  'customer base of any cloud provider). More recently, a newer form of '\n",
      "  'machine learning, called Generative AI, has burst onto the scene and '\n",
      "  'promises to significantly accelerate machine learning adoption. Generative '\n",
      "  'AI is based on very Large Language Models (trained on up to hundreds of '\n",
      "  'billions of parameters, and growing), across expansive datasets, and has '\n",
      "  'radically general and broad recall and learning capabilities.',\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as weve '\n",
      "  'done for years in AWS, were democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, were delivering applications like AWSs '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'Ill leave that for a future letter. Lets just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, Im optimistic that well emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and Ive mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business thats $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, its a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modify this query if you customized the documents uploaded to Amazon S3.\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113040e-a16c-4529-be74-b8baea9f6e28",
   "metadata": {},
   "source": [
    "## Step 7: Clean up\n",
    "\n",
    "When you finish this exercise, remove your resources with the following steps to prevent incurring costs:\n",
    "\n",
    "Delete the vector index.\n",
    "Delete data, network, and encryption access ploicies.\n",
    "Delete the OpenSearch collection.\n",
    "Delete the SageMaker Studio user profile and domain.\n",
    "Optionally, empty and delete the S3 bucket, or keep whatever you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0337b04e-d37c-400c-9899-d56b352f6c08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NotFoundError(404, 'resource_not_found_exception', 'Collection with ID [o2gjsxvnvzk88aolis03] cannot be found.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# delete vector index\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43moss_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# delete data, network, and encryption access ploicies\u001b[39;00m\n\u001b[1;32m      5\u001b[0m aoss_client\u001b[38;5;241m.\u001b[39mdelete_access_policy(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39maccess_policy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccessPolicyDetail\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/client/utils.py:179\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             params[p] \u001b[38;5;241m=\u001b[39m _escape(v)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/client/indices.py:304\u001b[0m, in \u001b[0;36mIndicesClient.delete\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m SKIP_IN_PATH:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty value passed for a required argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDELETE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_make_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/transport.py:409\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# connection didn't fail, confirm it's live status\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\u001b[38;5;241m.\u001b[39mmark_live(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/transport.py:370\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    367\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_connection()\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     status, headers_response, data \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     headers_response \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    382\u001b[0m         header\u001b[38;5;241m.\u001b[39mlower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers_response\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    383\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/connection/http_requests.py:230\u001b[0m, in \u001b[0;36mRequestsHttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, allow_redirects, ignore, headers)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore\n\u001b[1;32m    220\u001b[0m ):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_request_fail(\n\u001b[1;32m    222\u001b[0m         method,\n\u001b[1;32m    223\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         raw_data,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_request_success(\n\u001b[1;32m    237\u001b[0m     method,\n\u001b[1;32m    238\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     duration,\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mheaders, raw_data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/connection/base.py:301\u001b[0m, in \u001b[0;36mConnection._raise_error\u001b[0;34m(self, status_code, raw_data, content_type)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    299\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndecodable raw error response from server: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, err)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(status_code, TransportError)(\n\u001b[1;32m    302\u001b[0m     status_code, error_message, additional_info\n\u001b[1;32m    303\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: NotFoundError(404, 'resource_not_found_exception', 'Collection with ID [o2gjsxvnvzk88aolis03] cannot be found.')"
     ]
    }
   ],
   "source": [
    "# delete vector index\n",
    "oss_client.indices.delete(index=index_name)\n",
    "\n",
    "# delete data, network, and encryption access ploicies\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])\n",
    "\n",
    "# delete collection\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "aoss_client.delete_collection(id=collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb30c40-d356-41b9-bbd8-76579cd043ab",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we discussed the benefit of using a Knowledge Base for Amazon Bedrock. "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
