{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28462a3-c250-4b91-b024-f6595bfd033a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::809719347864:role/AmazonBedrockExecutionRoleForKnowledgeBase_SageMaker-ht-custom'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the execution role\n",
    "import sagemaker  \n",
    "sagemaker.get_execution_role()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c1a22d0-7e8d-47b8-bb54-2ef22e767958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon reported its fourth-quarter earnings on Thursday, and the company beat expectations on revenue and profit. Here are some key financial highlights from the earnings report:\n",
      "\n",
      "- Revenue: Amazon reported quarterly revenue of $157.3 billion, up 9% from the same period last year and beating expectations of $155.3 billion.\n",
      "- Profit: The company reported a quarterly profit of $3.5 billion, down 10% from the previous year but above expectations of $2.8 billion.\n",
      "- AWS: Amazon Web Services, the company's cloud computing division, continued to be a strong performer. AWS revenue grew 20% year-over-year to $17.8 billion, and operating income increased 29% to $5.2 billion.\n",
      "- North America: Revenue from Amazon's North America business grew 13% year-over-year to $105.4 billion, while operating income decreased 1% to $3.3 billion.\n",
      "- International: Revenue from Amazon's international business grew 3% year-over-year to $44.6 billion, while the division's operating loss widened to $2.3 billion from $1.6 billion in the previous year.\n",
      "- Advertising: Amazon's \"Other\" revenue, which primarily consists of advertising sales, increased 23% year-over-year to $9.7 billion.\n",
      "- Prime Day: Amazon held its annual Prime Day shopping event in October during the fourth quarter, after postponing it from July due to the pandemic. The company said Prime Day 2020 was the largest in its history, with sales surpassing those of the previous Black Friday and Cyber Monday combined.\n",
      "- Guidance: Amazon provided guidance for the first quarter of 2021, with expected net sales of between $100 billion and $106 billion, representing growth of between 33% and 40% compared to the same period last year. Operating income is expected to be between $3 billion and $6.5 billion, including approximately $1.5 billion of costs related to COVID-19.None\n"
     ]
    }
   ],
   "source": [
    "# # Working example of generating description for Amaozn financial results\n",
    "\n",
    "DEFAULT_MODEL= \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R_PLUS = \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R = \"cohere.command-r-v1:0\"\n",
    "model_id = DEFAULT_MODEL\n",
    "\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "#a function to generate the text\n",
    "#temp set to 0.3 by default\n",
    "def generate_text(prompt, model_id, temp=0.3):\n",
    "    body = {\n",
    "    'message': prompt,\n",
    "    'temperature': temp,\n",
    "    'preamble':\"\"\n",
    "    }\n",
    "    # Invoke the Bedrock model\n",
    "    response = bedrock_rt.invoke_model_with_response_stream(\n",
    "        modelId= model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    # Print the response\n",
    "    stream = response.get('body')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                byte = chunk.get('bytes').decode()\n",
    "                output=json.loads(byte)\n",
    "            if output['event_type'] == 'text-generation':\n",
    "                print(output['text'], end='')\n",
    "                \n",
    "user_input =\"Give financial highlights from Amazon's earnings call\"\n",
    "prompt = user_input\n",
    "response = generate_text(prompt, model_id)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4f54ac0-5b87-404e-a14f-85b18003b6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility.py - this is where the notebook starts\n",
    "\n",
    "import boto3\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "suffix = random.randrange(200, 900)\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "iam_client = boto3_session.client('iam')\n",
    "account_number = boto3.client('sts').get_caller_identity().get('Account')\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "encryption_policy_name = f\"bedrock-sample-rag-sp-{suffix}\"\n",
    "network_policy_name = f\"bedrock-sample-rag-np-{suffix}\"\n",
    "access_policy_name = f'bedrock-sample-rag-ap-{suffix}'\n",
    "bedrock_execution_role_name = f'AmazonBedrockExecutionRoleForKnowledgeBase_{suffix}'\n",
    "fm_policy_name = f'AmazonBedrockFoundationModelPolicyForKnowledgeBase_{suffix}'\n",
    "s3_policy_name = f'AmazonBedrockS3PolicyForKnowledgeBase_{suffix}'\n",
    "sm_policy_name = f'AmazonBedrockSecretPolicyForKnowledgeBase_{suffix}'\n",
    "oss_policy_name = f'AmazonBedrockOSSPolicyForKnowledgeBase_{suffix}'\n",
    "\n",
    "\n",
    "def create_bedrock_execution_role(bucket_name):\n",
    "    foundation_model_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    s3_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket_name}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    # create policies based on the policy documents\n",
    "    fm_policy = iam_client.create_policy(\n",
    "        PolicyName=fm_policy_name,\n",
    "        PolicyDocument=json.dumps(foundation_model_policy_document),\n",
    "        Description='Policy for accessing foundation model',\n",
    "    )\n",
    "\n",
    "    s3_policy = iam_client.create_policy(\n",
    "        PolicyName=s3_policy_name,\n",
    "        PolicyDocument=json.dumps(s3_policy_document),\n",
    "        Description='Policy for reading documents from s3')\n",
    "\n",
    "    # create bedrock execution role\n",
    "    bedrock_kb_execution_role = iam_client.create_role(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "        Description='Amazon Bedrock Knowledge Base Execution Role for accessing OSS and S3',\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "\n",
    "    # fetch arn of the policies and role created above\n",
    "    bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "    s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "    fm_policy_arn = fm_policy[\"Policy\"][\"Arn\"]\n",
    "    \n",
    "\n",
    "    # attach policies to Amazon Bedrock execution role\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=s3_policy_arn\n",
    "    )\n",
    "    return bedrock_kb_execution_role\n",
    "\n",
    "\n",
    "def create_oss_policy_attach_bedrock_execution_role(collection_id, bedrock_kb_execution_role):\n",
    "    # define oss policy document\n",
    "    oss_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"aoss:APIAccessAll\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:aoss:{region_name}:{account_number}:collection/{collection_id}\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    oss_policy = iam_client.create_policy(\n",
    "        PolicyName=oss_policy_name,\n",
    "        PolicyDocument=json.dumps(oss_policy_document),\n",
    "        Description='Policy for accessing opensearch serverless',\n",
    "    )\n",
    "    oss_policy_arn = oss_policy[\"Policy\"][\"Arn\"]\n",
    "    print(\"Opensearch serverless arn: \", oss_policy_arn)\n",
    "\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=oss_policy_arn\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_policies_in_oss(vector_store_name, aoss_client, bedrock_kb_execution_role_arn):\n",
    "    encryption_policy = aoss_client.create_security_policy(\n",
    "        name=encryption_policy_name,\n",
    "        policy=json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                           'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type='encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name=network_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                            'ResourceType': 'collection'}],\n",
    "                 'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type='network'\n",
    "    )\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name=access_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity, bedrock_kb_execution_role_arn],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type='data'\n",
    "    )\n",
    "    return encryption_policy, network_policy, access_policy\n",
    "\n",
    "\n",
    "def delete_iam_role_and_policies():\n",
    "    fm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{fm_policy_name}\"\n",
    "    s3_policy_arn = f\"arn:aws:iam::{account_number}:policy/{s3_policy_name}\"\n",
    "    oss_policy_arn = f\"arn:aws:iam::{account_number}:policy/{oss_policy_name}\"\n",
    "    sm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{sm_policy_name}\"\n",
    "\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=s3_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=oss_policy_arn\n",
    "    )\n",
    "    iam_client.detach_role_policy(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        PolicyArn=sm_policy_arn\n",
    "    )\n",
    "    iam_client.delete_role(RoleName=bedrock_execution_role_name)\n",
    "    iam_client.delete_policy(PolicyArn=s3_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=fm_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=oss_policy_arn)\n",
    "    iam_client.delete_policy(PolicyArn=sm_policy_arn)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = ''\n",
    "    for i in range(seconds):\n",
    "        dots += '.'\n",
    "        print(dots, end='\\r')\n",
    "        time.sleep(1)\n",
    "\n",
    "def create_bedrock_execution_role_multi_ds(bucket_names = None, secrets_arns = None):\n",
    "    \n",
    "    # 0. Create bedrock execution role\n",
    "\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # create bedrock execution role\n",
    "    bedrock_kb_execution_role = iam_client.create_role(\n",
    "        RoleName=bedrock_execution_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "        Description='Amazon Bedrock Knowledge Base Execution Role for accessing OSS, secrets manager and S3',\n",
    "        MaxSessionDuration=3600\n",
    "    )\n",
    "\n",
    "    # fetch arn of the role created above\n",
    "    bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "    # 1. Cretae and attach policy for foundation models\n",
    "    foundation_model_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModel\",\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\",\n",
    "                    f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    fm_policy = iam_client.create_policy(\n",
    "        PolicyName=fm_policy_name,\n",
    "        PolicyDocument=json.dumps(foundation_model_policy_document),\n",
    "        Description='Policy for accessing foundation model',\n",
    "    )\n",
    "  \n",
    "    # fetch arn of this policy \n",
    "    fm_policy_arn = fm_policy[\"Policy\"][\"Arn\"]\n",
    "    \n",
    "    # attach this policy to Amazon Bedrock execution role\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )\n",
    "\n",
    "    # 2. Cretae and attach policy for s3 bucket\n",
    "    if bucket_names:\n",
    "        s3_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [item for sublist in [[f'arn:aws:s3:::{bucket}', f'arn:aws:s3:::{bucket}/*'] for bucket in bucket_names] for item in sublist], \n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # create policies based on the policy documents\n",
    "        s3_policy = iam_client.create_policy(\n",
    "            PolicyName=s3_policy_name,\n",
    "            PolicyDocument=json.dumps(s3_policy_document),\n",
    "            Description='Policy for reading documents from s3')\n",
    "\n",
    "        # fetch arn of this policy \n",
    "        s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "        \n",
    "        # attach this policy to Amazon Bedrock execution role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "            PolicyArn=s3_policy_arn\n",
    "        )\n",
    "\n",
    "    # 3. Cretae and attach policy for secrets manager\n",
    "    if secrets_arns:\n",
    "        secrets_manager_policy_document = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"secretsmanager:GetSecretValue\",\n",
    "                        \"secretsmanager:PutSecretValue\"\n",
    "                    ],\n",
    "                    \"Resource\": secrets_arns\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # create policies based on the policy documents\n",
    "        \n",
    "        secrets_manager_policy = iam_client.create_policy(\n",
    "            PolicyName=sm_policy_name,\n",
    "            PolicyDocument=json.dumps(secrets_manager_policy_document),\n",
    "            Description='Policy for accessing secret manager',\n",
    "        )\n",
    "\n",
    "        # fetch arn of this policy\n",
    "        sm_policy_arn = secrets_manager_policy[\"Policy\"][\"Arn\"]\n",
    "\n",
    "        # attach policy to Amazon Bedrock execution role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_kb_execution_role[\"Role\"][\"RoleName\"],\n",
    "            PolicyArn=sm_policy_arn\n",
    "        )\n",
    "    \n",
    "    return bedrock_kb_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae256cef-ecf9-4d09-9304-c2787d2ab4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py==2.3.1\n",
      "  Downloading opensearch_py-2.3.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting urllib3<2,>=1.21.1 (from opensearch-py==2.3.1)\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.32.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.9.0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.7)\n",
      "Downloading opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, opensearch-py\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "Successfully installed opensearch-py-2.3.1 urllib3-1.26.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting boto3==1.33.2\n",
      "  Downloading boto3-1.33.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.34.0,>=1.33.2 (from boto3==1.33.2)\n",
      "  Downloading botocore-1.33.13-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3==1.33.2) (1.0.1)\n",
      "Collecting s3transfer<0.9.0,>=0.8.0 (from boto3==1.33.2)\n",
      "  Downloading s3transfer-0.8.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.2->boto3==1.33.2) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.2->boto3==1.33.2) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.2->boto3==1.33.2) (1.16.0)\n",
      "Downloading boto3-1.33.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.33.13-py3-none-any.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.2\n",
      "    Uninstalling botocore-1.35.2:\n",
      "      Successfully uninstalled botocore-1.35.2\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.10.2\n",
      "    Uninstalling s3transfer-0.10.2:\n",
      "      Successfully uninstalled s3transfer-0.10.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.2\n",
      "    Uninstalling boto3-1.35.2:\n",
      "      Successfully uninstalled boto3-1.35.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.34.2 requires botocore==1.35.2, but you have botocore 1.33.13 which is incompatible.\n",
      "awscli 1.34.2 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "sagemaker 2.229.0 requires boto3<2.0,>=1.34.142, but you have boto3 1.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.33.2 botocore-1.33.13 s3transfer-0.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting retrying==1.3.4\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: six>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from retrying==1.3.4) (1.16.0)\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: retrying\n",
      "Successfully installed retrying-1.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U boto3==1.33.2\n",
    "%pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb4ee137-7ded-4aed-ba79-d5ab79a9befe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3708759c-8bd1-4107-8673-e835c20944fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "# from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss, interactive_sleep\n",
    "import random\n",
    "from retrying import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f47dff7d-82f2-4312-aea7-4972be784b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}' # replace it with your bucket name.\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d05eb0ab-1eaa-4b90-8d63-c3e5c49c8bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket bedrock-kb-us-east-1-809719347864\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except ClientError as e:\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    if region_name == \"us-east-1\":\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name)\n",
    "    else:\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2015f034-9af6-4678-bacc-eb5655331727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5d4c5e7-6779-42b3-b862-5be74fe9304f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8776c00a-5d88-4166-b1c0-a134de989ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d069ae48-963b-4138-81f4-87b8731eb5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '310',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Wed, 28 Aug 2024 07:00:09 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '397a5772-74cc-4715-93b0-d83811eee396'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '397a5772-74cc-4715-93b0-d83811eee396',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:809719347864:collection/9iupha0l00c6fr1ygh',\n",
      "                              'createdDate': 1724828409309,\n",
      "                              'id': '9iupha0l00c6fr1ygh',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1724828409309,\n",
      "                              'name': 'bedrock-sample-rag-457',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc3e0239-a47e-442f-84e0-9cb0878e8a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encryption_policy' (dict)\n",
      "Stored 'network_policy' (dict)\n",
      "Stored 'access_policy' (dict)\n",
      "Stored 'collection' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store encryption_policy network_policy access_policy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e7100c9-c42e-4cd3-8a28-644533f6266d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9iupha0l00c6fr1ygh.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0293e36a-54ed-498d-b8b8-40f96ec6aeec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:809719347864:collection/9iupha0l00c6fr1ygh',\n",
      "    'collectionEndpoint': 'https://9iupha0l00c6fr1ygh.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1724828409309,\n",
      "    'dashboardEndpoint': 'https://9iupha0l00c6fr1ygh.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': '9iupha0l00c6fr1ygh',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1724828433221,\n",
      "    'name': 'bedrock-sample-rag-457',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# wait for collection creation\n",
    "# This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "261ac7ba-839c-4331-8768-49a63f395793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::809719347864:policy/AmazonBedrockOSSPolicyForKnowledgeBase_565\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa4b98-cf3f-45f5-b1bc-55dc7dcc710b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2...Create Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2aabdd8e-ff83-4bb5-b4ac-2970788d57f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecbf7182-423b-46a6-96c7-c031a994f8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-565',\n",
      "  'shards_acknowledged': True}\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548a601-7753-47aa-8593-8b8714c8f4f2",
   "metadata": {},
   "source": [
    "# Download data to ingest into our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1f1d736-e68b-4a1b-971b-29a9dfcee840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2387229e-4013-49f6-8b7f-8b6597675783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload data to s3 to the bucket that was configured as a data source to the knowledge base\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucket_name,file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a0a21-4b7b-4ff8-b0a0-e7bcc6d540d7",
   "metadata": {},
   "source": [
    "# Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "325775a9-6886-4147-9a3b-25ad14e0af99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ingest strategy - How to ingest data from the data source\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4936320-93d8-47ce-8ede-2f5489ee097d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68ebea06-5daf-454f-b0f1-9240ce0c8b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7c7a10b-cc7e-41ca-a0b6-e3b259d22745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 8, 28, 7, 12, 36, 45552, tzinfo=tzlocal()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:809719347864:knowledge-base/NRUZOOJE1L',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'}},\n",
      "  'knowledgeBaseId': 'NRUZOOJE1L',\n",
      "  'name': 'bedrock-sample-knowledge-base-565',\n",
      "  'roleArn': 'arn:aws:iam::809719347864:role/AmazonBedrockExecutionRoleForKnowledgeBase_259',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:809719347864:collection/9iupha0l00c6fr1ygh',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-565'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 8, 28, 7, 12, 36, 45552, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dec97ea3-5276-4fd8-8808-cacedf2f36a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9924add3-7aba-4891-82d8-3176e60a6b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 8, 28, 7, 13, 14, 807078, tzinfo=tzlocal()),\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-809719347864'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'YTMMESX7YU',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'NRUZOOJE1L',\n",
      "  'name': 'bedrock-sample-knowledge-base-565',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 28, 7, 13, 14, 807078, tzinfo=tzlocal()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb3997ac-a770-4a1d-b637-a1cea28bb86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '82472358-1ba6-418b-916b-06219d16b196',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 28 Aug 2024 07:13:25 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '603',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '82472358-1ba6-418b-916b-06219d16b196',\n",
       "   'x-amz-apigw-id': 'dNUjdHrhoAMEPFg=',\n",
       "   'x-amzn-trace-id': 'Root=1-66cece15-4061a1d64a69a30b6725c70b'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'knowledgeBaseId': 'NRUZOOJE1L',\n",
       "  'dataSourceId': 'YTMMESX7YU',\n",
       "  'name': 'bedrock-sample-knowledge-base-565',\n",
       "  'status': 'AVAILABLE',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'dataSourceConfiguration': {'type': 'S3',\n",
       "   's3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-809719347864'}},\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}},\n",
       "  'createdAt': datetime.datetime(2024, 8, 28, 7, 13, 14, 807078, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 28, 7, 13, 14, 807078, tzinfo=tzlocal())}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82855a8e-4abf-48a0-bb8e-77f9586c5204",
   "metadata": {},
   "source": [
    "# Start ingestion job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e886f819-9840-4334-8587-1d70708c2bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d390fde-d4d5-43fe-a65e-f54a73add718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'YTMMESX7YU',\n",
      "  'ingestionJobId': 'ALHPIDBCAK',\n",
      "  'knowledgeBaseId': 'NRUZOOJE1L',\n",
      "  'startedAt': datetime.datetime(2024, 8, 28, 7, 13, 55, 875877, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 0},\n",
      "  'status': 'STARTING',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 28, 7, 13, 55, 875877, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "452ec9fb-5e89-41e7-9af2-3bea65b11922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'YTMMESX7YU',\n",
      "  'ingestionJobId': 'ALHPIDBCAK',\n",
      "  'knowledgeBaseId': 'NRUZOOJE1L',\n",
      "  'startedAt': datetime.datetime(2024, 8, 28, 7, 13, 55, 875877, tzinfo=tzlocal()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 8, 28, 7, 14, 13, 297835, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "988ab6ec-be85-4887-84ec-13bf7ad38013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NRUZOOJE1L'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97c8bb1f-40cf-4837-9232-6fbb2cb8b279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850fd0a-2f04-4227-beaa-a081f9378aa2",
   "metadata": {},
   "source": [
    "# Test the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f99cce9-3a41-422b-9236-5b01c2a69b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Lets see how different Anthropic Claude 3 models responds to the input text we provide\n",
    "claude_model_ids = [ [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"], [\"Claude 3 Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d20147f-def8-4766-a879-60e6e8c7cd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48401c09-ec9a-4260-8a14-a90d800fa4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generated using Claude 3 Sonnet:\n",
      "('Amazon has been working on developing its own large language models (LLMs) '\n",
      " 'for generative AI applications. The company believes generative AI will '\n",
      " 'transform and improve virtually every customer experience across its '\n",
      " 'consumer, seller, brand, and creator offerings. Amazon is investing '\n",
      " 'substantially in LLMs and plans to continue doing so. On the AWS cloud '\n",
      " 'platform, Amazon is democratizing generative AI technology by offering '\n",
      " 'machine learning chips like Trainium and Inferentia that provide '\n",
      " 'cost-effective training and running of LLMs. AWS also enables companies to '\n",
      " \"choose from various LLMs and build applications with AWS's security, privacy \"\n",
      " \"and other features. One example is AWS's CodeWhisperer, which uses \"\n",
      " 'generative AI to provide real-time code suggestions to improve developer '\n",
      " 'productivity.')\n",
      "---------- The citations for the response generated by Claude 3 Sonnet:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n",
      "---------- Generated using Claude 3 Haiku:\n",
      "('Amazon has been investing heavily in large language models (LLMs) and '\n",
      " 'generative AI, which they believe will significantly accelerate the adoption '\n",
      " 'of machine learning across their various businesses. Specifically: Amazon '\n",
      " 'has been working on developing its own LLMs, which they believe will '\n",
      " 'transform and improve virtually every customer experience across their '\n",
      " 'consumer, seller, brand, and creator offerings. Additionally, Amazon is '\n",
      " 'democratizing this technology through its AWS cloud platform, offering the '\n",
      " 'most price-performant machine learning chips in Trainium and Inferentia to '\n",
      " 'enable companies of all sizes to train and run their own LLMs in production. '\n",
      " \"As an example, Amazon has already launched AWS's CodeWhisperer, which uses \"\n",
      " 'generative AI to revolutionize developer productivity by generating code '\n",
      " 'suggestions in real-time. Amazon sees LLMs and generative AI as a big '\n",
      " 'opportunity that will be transformative for their customers, shareholders, '\n",
      " 'and the company overall.')\n",
      "---------- The citations for the response generated by Claude 3 Haiku:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada3897-eb7c-489e-9659-28f9783ca008",
   "metadata": {},
   "source": [
    "# Buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b0ece-bd1f-4eb6-b2ff-b9edc8c1a51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72132a-d817-4087-9f7c-7480938e41c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "724a8cce-b821-46e6-b802-32f56ed0234c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Embedding using Cohere LLM Model and stored in Amazon OpenSearch serverless\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Embeddings are integral to various natural language processing applications, with their quality crucial for optimal performance. They are commonly used in knowledge bases to represent textual data as dense vectors enabling efficient similarity search and retrieval. Embeddings play a key role in personalization and recommendation systems by representing user preferences, item characteristics, and historical interactions as vectors, allowing calculation of similarities for personalized recommendations based on user behavior and item embeddings.\n",
    "\n",
    "In this notebook, we demonstrate how to use the Cohere Embed Multilingual V3 LLM (Large Language Model) for creating text embedding that will be stored in Amazon OpenSearch with vector engine support for assisting with the prompt engineering task for more accurate response from LLMs.\n",
    "\n",
    "\n",
    "## Prepare Documents\n",
    "\n",
    "\n",
    "Before being able to search text based on meaning, not just keywords the questions, the documents must be processed and a stored in a document store index\n",
    "\n",
    "* Load the documents\n",
    "* Create a numerical vector representation using Amazon Bedrock Cohere model\n",
    "* Create an index and the corresponding embeddings in the Amazon Open Search Serverless\n",
    "\n",
    "## Search Text\n",
    "\n",
    "When the documents index is prepared, you are ready to search text and relevant documents will be fetched based on the query being asked. Following steps will be executed.\n",
    "\n",
    "* Create an embedding of the input query\n",
    "* Compare the query embedding with the embeddings in the index\n",
    "* Fetch the (top N) relevant document chunks\n",
    "* Add those chunks as part of the context in the prompt\n",
    "* Send the prompt to the Cohere Command R+ model under Amazon Bedrock\n",
    "* Get the contextual answer based on the documents retrieved\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eadd13-b0d0-4522-9a82-316e4717e3fb",
   "metadata": {},
   "source": [
    "It's recommended to execute the notebook in SageMaker Studio Notebooks `Python 3.0(Data Science)` Kernel with `ml.t3.medium` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f19203-f712-4669-82a6-ef583c5a9a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df481b1e-5052-4bd4-a906-9a6fe87c16ca",
   "metadata": {},
   "source": [
    "## Step 0: Install Dependencies\n",
    "\n",
    "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87b5af-e909-4941-b5e4-33341b96a4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opensearch-py\n",
    "!pip install requests-aws4auth\n",
    "!pip install -U boto3\n",
    "!pip install -U botocore\n",
    "!pip install -U awscli\n",
    "!pip install -U datasets\n",
    "!pip install -U pypdf\n",
    "!pip install langchain\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedfd22-e34e-4d0d-9d90-ddf122161c5b",
   "metadata": {},
   "source": [
    "Install some python packages we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ae200-eac9-40ba-b9fb-dad4ad1ae964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# External Dependencies:\n",
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import sagemaker\n",
    "import json\n",
    "import time\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1a111-d79e-4b16-8bdf-35b68b198c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting boto3 clients for required AWS services\n",
    "\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    \"us-east-1\", \n",
    "    endpoint_url=\"https://bedrock-runtime.us-east-1.amazonaws.com\"\n",
    ")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "region_name = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecd2f7-45d6-4701-8462-dfe2f8bf3141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SageMaker session\n",
    "sagemaker_role_arn = sagemaker.get_execution_role()\n",
    "sagemaker_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbfd79-97b7-4067-9688-26be5148693c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf6b82-7e3e-4c56-bb2f-288c0e0c8fb8",
   "metadata": {},
   "source": [
    "For this notebook, Let's first download some of the files to build our document store. For this example we will be using public IRS documents from [here](https://www.irs.gov/publications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2238bd-b82f-4f0c-a1b0-c0f6befd68cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1544.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p15.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1212.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78385806-4d92-4631-8e95-8b131163c93d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51461e70-aafa-4a95-af73-bf57dae3efa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44aa72b-6b23-4801-b16c-a59c61111a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(\n",
    "    documents\n",
    ")\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f\"Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.\")\n",
    "print(f\"After the split we have {len(docs)} documents more than the original {len(documents)}.\")\n",
    "print(\n",
    "    f\"Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb6f7e-5f07-4e41-9b38-24cbb8434930",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Generate embedding using Cohere Embed Multilingual V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167e377-7457-4853-b984-d246bf27eec9",
   "metadata": {},
   "source": [
    "Cohere Embed Multilingual V3: An embedding model designed to encode text from various languages into dense vector representations, enabling efficient similarity comparisons and semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7307-a892-4ed9-9856-2de4772d8ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vector_embedding_with_bedrock(text, bedrock_client):\n",
    "    modelId = \"cohere.embed-multilingual-v3\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    input_type = \"search_document\"\n",
    "    body = json.dumps({\n",
    "    \"texts\": [text],\n",
    "    \"input_type\": input_type }\n",
    "    )\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    embedding = response_body.get(\"embeddings\")\n",
    "    return {\"text\": text, \"vector_field\": embedding[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b1e1e-f620-40fe-835c-82cd6a929f31",
   "metadata": {},
   "source": [
    "Lets try embedding first document and check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c79d80-88a9-4a80-8947-ec8899fc6c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_embeddings = create_vector_embedding_with_bedrock(docs[0].page_content, bedrock_client)\n",
    "      \n",
    "print(bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac091541-8f6a-4e19-a940-ef6a8321e050",
   "metadata": {},
   "source": [
    "## Step 4: Storing embedding in Amazon OpenSearch serverless\n",
    "\n",
    "Following the similar pattern embeddings could be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "First of all we have to create a vector store. In this notebook we will use Amazon OpenSearch serverless.\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion.\n",
    "\n",
    "### Step 4.1: Create a vector store with Opensearch Serverless using Cohere Multilingual Embed V3\n",
    "\n",
    "Before creating the new vector search collection and index, we must first create three associated OpenSearch policies: encryption security policy, network security policy, and data access policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e840a4-93cc-4a84-b338-ce79f825a0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "def create_policies_in_oss(vector_store_name, aoss_client, role_arn):\n",
    "    \n",
    "    encryption_policy_name = f\"cohere-sample-sp-{suffix}\"\n",
    "    network_policy_name = f\"cohere-sample-np-{suffix}\"\n",
    "    access_policy_name = f'cohere-sample-ap-{suffix}'\n",
    "\n",
    "    try:\n",
    "        encryption_policy = aoss_client.create_security_policy(\n",
    "            name=encryption_policy_name,\n",
    "            policy=json.dumps(\n",
    "                {\n",
    "                    'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                               'ResourceType': 'collection'}],\n",
    "                    'AWSOwnedKey': True\n",
    "                }),\n",
    "            type='encryption'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "    try:\n",
    "        network_policy = aoss_client.create_security_policy(\n",
    "            name=network_policy_name,\n",
    "            policy=json.dumps(\n",
    "                [\n",
    "                    {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                                'ResourceType': 'collection'}],\n",
    "                     'AllowFromPublic': True}\n",
    "                ]),\n",
    "            type='network'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        access_policy = aoss_client.create_access_policy(\n",
    "            name=access_policy_name,\n",
    "            policy=json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        'Rules': [\n",
    "                            {\n",
    "                                'Resource': ['collection/' + vector_store_name],\n",
    "                                'Permission': [\n",
    "                                    'aoss:CreateCollectionItems',\n",
    "                                    'aoss:DeleteCollectionItems',\n",
    "                                    'aoss:UpdateCollectionItems',\n",
    "                                    'aoss:DescribeCollectionItems'],\n",
    "                                'ResourceType': 'collection'\n",
    "                            },\n",
    "                            {\n",
    "                                'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                                'Permission': [\n",
    "                                    'aoss:CreateIndex',\n",
    "                                    'aoss:DeleteIndex',\n",
    "                                    'aoss:UpdateIndex',\n",
    "                                    'aoss:DescribeIndex',\n",
    "                                    'aoss:ReadDocument',\n",
    "                                    'aoss:WriteDocument'],\n",
    "                                'ResourceType': 'index'\n",
    "                            }],\n",
    "                        'Principal': [identity, role_arn],\n",
    "                        'Description': 'Easy data policy'}\n",
    "                ]),\n",
    "            type='data'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "    return encryption_policy, network_policy, access_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88b614-b072-4edc-b272-1f1f7879fc8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.2: Create a new collection of type VECTORSEARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b812c4d-84ac-442e-b16a-70ec613d9423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Collection\n",
    "vector_store_name = f'cohere-embedding-collection-{suffix}'\n",
    "\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       role_arn=sagemaker_role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3475239-09ef-4b89-939d-691eb889ca15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b125c5-f33f-4b9c-843d-8fe6e4050de1",
   "metadata": {},
   "source": [
    "### Step 4.3: Setting up the Amazon OpenSearch Serverless index using KNN settings\n",
    "\n",
    "k-NN for Amazon OpenSearch Service lets you search for points in a vector space and find the \"nearest neighbors\" for those points by Euclidean distance or cosine similarity. Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "Once the OpenSearch collection is created, create an index to store the embeddings. The index settings must be configured beforehand to enable the KNN functionality using the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9480e8c-3847-4506-a138-05c7c590d8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34c21c-b62c-42e9-911f-daabeba3c4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"cohere-embedding-index\"\n",
    "index_body = {\n",
    "   \"settings\": {\n",
    "       \"index\":{\n",
    "          \"knn\": \"true\",\n",
    "       }\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector_field\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1024 \n",
    "         },\n",
    "          \"text\": {\n",
    "                    \"type\": \"keyword\"\n",
    "        }\n",
    "      }\n",
    "   }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcce70-295d-429f-b182-939a9bbb2cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a0b6b-668a-4ea8-9b01-c9bef003b3e6",
   "metadata": {},
   "source": [
    "To confirm its creation, we can retrieve the description of the new vector index you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c750939-a886-480c-8d6e-aa841162ec5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We would get an index already exists exception if the index already exists, and that is fine.\n",
    "try:\n",
    "    response = oss_client.indices.create(index_name, body=index_body)\n",
    "    print(f\"response received for the create index -> {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"error in creating index={index_name}, exception={e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfb94f-4a8f-4904-8d5d-531438132c4c",
   "metadata": {},
   "source": [
    "Now we are ready to inject our documents into vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b90fd4-76d6-4475-addd-2d1bef3c75d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deleting indices\n",
    "# aoss_client.indices.delete(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534e699-88d8-42a7-a6d6-5b228a04c9bf",
   "metadata": {},
   "source": [
    "### Step 4.4: Ingest the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d969f-4a2a-4f6a-b53b-fc4c7f7a059c",
   "metadata": {},
   "source": [
    "Next you need to loop through your dataset and ingest items data into the cluster. A more robust and scalable solution for the embedding ingestion can be found in [Ingesting enriched data into Amazon ES](https://aws.amazon.com/blogs/industries/novartis-ag-uses-amazon-elasticsearch-k-nearest-neighbor-knn-and-amazon-sagemaker-to-power-search-and-recommendation/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268791c4-600f-4c28-ac86-9f93cd567cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(docs)): \n",
    "    embedding = create_vector_embedding_with_bedrock(docs[idx].page_content, bedrock_client)\n",
    "    document = {\n",
    "                'vector_field': embedding['vector_field'],\n",
    "                'text': embedding['text']\n",
    "                }\n",
    "    response = oss_client.index(\n",
    "    index = index_name,\n",
    "    body = document\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c52b22-2d08-43fb-88e2-fd39f4ff0b1b",
   "metadata": {},
   "source": [
    "## Step 5: Perform Search based on Text Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55530289-7577-4384-a671-812aac736e81",
   "metadata": {},
   "source": [
    "Let’s take a look at the results of a simple query. In below example, we'll receive an text input from user, and then will send it to search engine to get the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674e889-e207-4290-9991-52fb977a57a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_prompt = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "# query embedding\n",
    "query_emb = create_vector_embedding_with_bedrock(query_prompt, bedrock_client)['vector_field']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4debd-798d-49d9-b5bf-ee39734eab3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"vector_field\": {\n",
    "                    \"vector\": query_emb,\n",
    "                    \"k\": 5,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f6bb7-cfe4-41f1-8d77-53ae5ac110ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform search based on query input\n",
    "res = oss_client.search(index=index_name, body=body)\n",
    "results = \"\"\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    id_ = hit[\"_id\"]\n",
    "    text = hit[\"_source\"][\"text\"]\n",
    "    results += text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c2825-4f7a-43ae-9cfb-f518007305ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Generative Question Answering\n",
    "\n",
    "For Generative Question Answering we will use the RAG(Retrieval Augmented Generation) approach retrieves information most relevant to the user’s request from the enterprise knowledge base and bundles it as context along with the user’s request as a prompt, and then sends it to the LLM to get a GenAI response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85eef4e-b0fd-4921-9ac4-b53d559799a4",
   "metadata": {},
   "source": [
    "Define utility function for conversation with Bedrock converse API\n",
    "\n",
    "Model used - Cohere Command R+: A powerful Large Language Model (LLM) capable of understanding and generating text in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17726acc-ddb7-4353-a9f3-969f8bb1007a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history=[],\n",
    "    temperature=0.3,\n",
    "    max_tokens=400,\n",
    "    top_p=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompt (str) : The system prompt for the model to use.\n",
    "        prompt (str) : The message/question to send to the model.\n",
    "        chat_history (list): The chat history from user and assistant.\n",
    "\n",
    "    Returns:\n",
    "        response (str): The text generated output from the model.\n",
    "        chat_history (str): The full conversation between user and assistant that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompts = [\n",
    "        {\n",
    "            \"text\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    chat_history.extend(messages)\n",
    "\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"topP\": top_p,\n",
    "    }\n",
    "\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    chat_history.append(response[\"output\"][\"message\"])\n",
    "\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"], chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf004f02-28d6-4372-b837-1a63c1f3b569",
   "metadata": {},
   "source": [
    "Define the system prompt and guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d172292-cb89-42d7-9346-84a92c311df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "## Instructions\n",
    "You are an AI assistant. Your knowledge is based solely on the information provided between the <documents> and </documents> tags.\n",
    "\n",
    "Before answering any questions, first check if the user has provided information between the <documents> and </documents> tags. If no information is provided, respond with the following JSON:\n",
    "\n",
    "{\n",
    "    \"answer\": \"I do not have enough information to answer that question.\"\n",
    "}\n",
    "\n",
    "If documents are provided, your task is to answer questions accurately and concisely, using only the details from the given documents. Do not use your own knowledge or any external sources to answer the questions, even if you know the answer.\n",
    "\n",
    "If a question cannot be fully answered using the provided documents, respond with the following JSON:\n",
    "\n",
    "{\n",
    "    \"answer\": \"I do not have enough information to answer that question.\"\n",
    "}\n",
    "\n",
    "All responses must be in valid JSON format, with the 'answer' key containing the actual response text.\n",
    "\n",
    "To provide transparency, include your reasoning process with the 'thinking' key as the following format:\n",
    "\n",
    "{\n",
    "    \"answer\": \"Your response here\",\n",
    "    \"thinking\": \"Your reasoning process here\"\n",
    "}\n",
    "\n",
    "Be concise and objective in your responses, without any personal opinions or subjective statements.\n",
    "\"\"\"\n",
    "prompt_template = \"## Documents<documents>\\n{documents}\\n</documents>\\n\\n ## Questions Question: {question}\\nThink step-by-step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab34e6f-e31d-4f45-992e-75aded6a2849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model ID parameter\n",
    "model_id = \"cohere.command-r-plus-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcb404-82f9-4daf-bc68-25f589bf3f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "prompt = prompt_template.format(documents=results, question=query_prompt)\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113040e-a16c-4529-be74-b8baea9f6e28",
   "metadata": {},
   "source": [
    "## Step 7: Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87766ad7-6326-4000-a9de-3543c94fff47",
   "metadata": {},
   "source": [
    "When you finish this exercise, remove your resources with the following steps:\n",
    "\n",
    "Delete vector index.\n",
    "Delete data, network, and encryption access ploicies.\n",
    "Delete collection.\n",
    "Delete SageMaker Studio user profile and domain.\n",
    "Optionally, empty and delete the S3 bucket, or keep whatever you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337b04e-d37c-400c-9899-d56b352f6c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete vector index\n",
    "oss_client.indices.delete(index=index_name)\n",
    "\n",
    "# delete data, network, and encryption access ploicies\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])\n",
    "\n",
    "# delete collection\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "aoss_client.delete_collection(id=collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a053d40-3185-407e-818f-2a8d44800d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
