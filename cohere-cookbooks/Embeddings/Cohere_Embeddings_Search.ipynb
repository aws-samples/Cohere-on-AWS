{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724a8cce-b821-46e6-b802-32f56ed0234c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Embedding using Cohere LLM Model and stored in Amazon OpenSearch serverless\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Embeddings are integral to various natural language processing applications, with their quality crucial for optimal performance. They are commonly used in knowledge bases to represent textual data as dense vectors enabling efficient similarity search and retrieval. Embeddings play a key role in personalization and recommendation systems by representing user preferences, item characteristics, and historical interactions as vectors, allowing calculation of similarities for personalized recommendations based on user behavior and item embeddings.\n",
    "\n",
    "In this notebook, we demonstrate how to use the Cohere Embed Multilingual V3 LLM (Large Language Model) for creating text embedding that will be stored in Amazon OpenSearch with vector engine support for assisting with the prompt engineering task for more accurate response from LLMs.\n",
    "\n",
    "## Prepare documents\n",
    "\n",
    "\n",
    "Before being able to search text based on meaning, not just keywords the questions, the documents must be processed and a stored in a document store index\n",
    "\n",
    "* Load the documents\n",
    "* Create a numerical vector representation using Amazon Bedrock Cohere model\n",
    "* Create an index and the corresponding embeddings in the Amazon Open Search Serverless\n",
    "\n",
    "## Search Text\n",
    "\n",
    "When the documents index is prepared, you are ready to search text and relevant documents will be fetched based on the query being asked. Following steps will be executed.\n",
    "\n",
    "* Create an embedding of the input query\n",
    "* Compare the query embedding with the embeddings in the index\n",
    "* Fetch the (top N) relevant document chunks\n",
    "* Add those chunks as part of the context in the prompt\n",
    "* Send the prompt to the Cohere Command R+ model under Amazon Bedrock\n",
    "* Get the contextual answer based on the documents retrieved\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eadd13-b0d0-4522-9a82-316e4717e3fb",
   "metadata": {},
   "source": [
    "It's recommended to execute the notebook in SageMaker Studio Notebooks `Python 3.0(Data Science)` Kernel with `ml.t3.medium` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f19203-f712-4669-82a6-ef583c5a9a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df481b1e-5052-4bd4-a906-9a6fe87c16ca",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87b5af-e909-4941-b5e4-33341b96a4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opensearch-py\n",
    "!pip install requests-aws4auth\n",
    "!pip install -U boto3\n",
    "!pip install -U botocore\n",
    "!pip install -U awscli\n",
    "!pip install -U datasets\n",
    "!pip install -U pypdf\n",
    "!pip install langchain\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedfd22-e34e-4d0d-9d90-ddf122161c5b",
   "metadata": {},
   "source": [
    "Install some python packages we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ae200-eac9-40ba-b9fb-dad4ad1ae964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# External Dependencies:\n",
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import sagemaker\n",
    "import json\n",
    "import time\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1a111-d79e-4b16-8bdf-35b68b198c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting boto3 clients for required AWS services\n",
    "\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    \"us-east-1\", \n",
    "    endpoint_url=\"https://bedrock-runtime.us-east-1.amazonaws.com\"\n",
    ")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "region_name = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecd2f7-45d6-4701-8462-dfe2f8bf3141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SageMaker session\n",
    "sagemaker_role_arn = sagemaker.get_execution_role()\n",
    "sagemaker_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbfd79-97b7-4067-9688-26be5148693c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf6b82-7e3e-4c56-bb2f-288c0e0c8fb8",
   "metadata": {},
   "source": [
    "For this notebook, Let's first download some of the files to build our document store. For this example we will be using public IRS documents from [here](https://www.irs.gov/publications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2238bd-b82f-4f0c-a1b0-c0f6befd68cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1544.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p15.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1212.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78385806-4d92-4631-8e95-8b131163c93d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51461e70-aafa-4a95-af73-bf57dae3efa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44aa72b-6b23-4801-b16c-a59c61111a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(\n",
    "    documents\n",
    ")\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f\"Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.\")\n",
    "print(f\"After the split we have {len(docs)} documents more than the original {len(documents)}.\")\n",
    "print(\n",
    "    f\"Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb6f7e-5f07-4e41-9b38-24cbb8434930",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate embedding using Cohere Embed Multilingual V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167e377-7457-4853-b984-d246bf27eec9",
   "metadata": {},
   "source": [
    "Cohere Embed Multilingual V3: An embedding model designed to encode text from various languages into dense vector representations, enabling efficient similarity comparisons and semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7307-a892-4ed9-9856-2de4772d8ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vector_embedding_with_bedrock(text, bedrock_client):\n",
    "    payload = {\"inputText\": f\"{text}\"}\n",
    "    body = json.dumps(payload)\n",
    "    modelId = \"amazon.titan-embed-text-v1\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return {\"text\": text, \"vector_field\": embedding}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b1e1e-f620-40fe-835c-82cd6a929f31",
   "metadata": {},
   "source": [
    "Lets try embedding first document and check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c79d80-88a9-4a80-8947-ec8899fc6c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_embeddings = create_vector_embedding_with_bedrock(docs[0].page_content, bedrock_client)\n",
    "      \n",
    "print(bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac091541-8f6a-4e19-a940-ef6a8321e050",
   "metadata": {},
   "source": [
    "## Storing embedding in Amazon OpenSearch serverless\n",
    "\n",
    "Following the similar pattern embeddings could be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "First of all we have to create a vector store. In this notebook we will use Amazon OpenSearch serverless.\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion.\n",
    "\n",
    "### Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "Before creating the new vector search collection and index, we must first create three associated OpenSearch policies: encryption security policy, network security policy, and data access policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e840a4-93cc-4a84-b338-ce79f825a0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "def create_policies_in_oss(vector_store_name, aoss_client, role_arn):\n",
    "    \n",
    "    encryption_policy_name = f\"cohere-sample-sp-{suffix}\"\n",
    "    network_policy_name = f\"cohere-sample-np-{suffix}\"\n",
    "    access_policy_name = f'cohere-sample-ap-{suffix}'\n",
    "\n",
    "    try:\n",
    "        encryption_policy = aoss_client.create_security_policy(\n",
    "            name=encryption_policy_name,\n",
    "            policy=json.dumps(\n",
    "                {\n",
    "                    'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                               'ResourceType': 'collection'}],\n",
    "                    'AWSOwnedKey': True\n",
    "                }),\n",
    "            type='encryption'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "    try:\n",
    "        network_policy = aoss_client.create_security_policy(\n",
    "            name=network_policy_name,\n",
    "            policy=json.dumps(\n",
    "                [\n",
    "                    {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                                'ResourceType': 'collection'}],\n",
    "                     'AllowFromPublic': True}\n",
    "                ]),\n",
    "            type='network'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        access_policy = aoss_client.create_access_policy(\n",
    "            name=access_policy_name,\n",
    "            policy=json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        'Rules': [\n",
    "                            {\n",
    "                                'Resource': ['collection/' + vector_store_name],\n",
    "                                'Permission': [\n",
    "                                    'aoss:CreateCollectionItems',\n",
    "                                    'aoss:DeleteCollectionItems',\n",
    "                                    'aoss:UpdateCollectionItems',\n",
    "                                    'aoss:DescribeCollectionItems'],\n",
    "                                'ResourceType': 'collection'\n",
    "                            },\n",
    "                            {\n",
    "                                'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                                'Permission': [\n",
    "                                    'aoss:CreateIndex',\n",
    "                                    'aoss:DeleteIndex',\n",
    "                                    'aoss:UpdateIndex',\n",
    "                                    'aoss:DescribeIndex',\n",
    "                                    'aoss:ReadDocument',\n",
    "                                    'aoss:WriteDocument'],\n",
    "                                'ResourceType': 'index'\n",
    "                            }],\n",
    "                        'Principal': [identity, role_arn],\n",
    "                        'Description': 'Easy data policy'}\n",
    "                ]),\n",
    "            type='data'\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "    return encryption_policy, network_policy, access_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88b614-b072-4edc-b272-1f1f7879fc8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a new collection of type VECTORSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b812c4d-84ac-442e-b16a-70ec613d9423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Collection\n",
    "vector_store_name = f'cohere-embedding-collection-{suffix}'\n",
    "\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       role_arn=sagemaker_role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3475239-09ef-4b89-939d-691eb889ca15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b125c5-f33f-4b9c-843d-8fe6e4050de1",
   "metadata": {},
   "source": [
    "### Setting up the Amazon OpenSearch Serverless index using KNN settings\n",
    "Once the OpenSearch collection is created, create an index to store the embeddings. The index settings must be configured beforehand to enable the KNN functionality using the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9480e8c-3847-4506-a138-05c7c590d8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34c21c-b62c-42e9-911f-daabeba3c4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"cohere-embedding-index\"\n",
    "index_body = {\n",
    "   \"settings\": {\n",
    "       \"index\":{\n",
    "          \"knn\": \"true\",\n",
    "       }\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector_field\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536 \n",
    "         },\n",
    "          \"text\": {\n",
    "                    \"type\": \"keyword\"\n",
    "        }\n",
    "      }\n",
    "   }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcce70-295d-429f-b182-939a9bbb2cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a0b6b-668a-4ea8-9b01-c9bef003b3e6",
   "metadata": {},
   "source": [
    "To confirm its creation, we can retrieve the description of the new vector index you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c750939-a886-480c-8d6e-aa841162ec5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We would get an index already exists exception if the index already exists, and that is fine.\n",
    "try:\n",
    "    response = oss_client.indices.create(index_name, body=index_body)\n",
    "    print(f\"response received for the create index -> {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"error in creating index={index_name}, exception={e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfb94f-4a8f-4904-8d5d-531438132c4c",
   "metadata": {},
   "source": [
    "Now we are ready to inject our documents into vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b90fd4-76d6-4475-addd-2d1bef3c75d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deleting indices\n",
    "# aoss_client.indices.delete(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534e699-88d8-42a7-a6d6-5b228a04c9bf",
   "metadata": {},
   "source": [
    "### Ingest the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d969f-4a2a-4f6a-b53b-fc4c7f7a059c",
   "metadata": {},
   "source": [
    "Next you need to loop through your dataset and ingest items data into the cluster. A more robust and scalable solution for the embedding ingestion can be found in [Ingesting enriched data into Amazon ES](https://aws.amazon.com/blogs/industries/novartis-ag-uses-amazon-elasticsearch-k-nearest-neighbor-knn-and-amazon-sagemaker-to-power-search-and-recommendation/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268791c4-600f-4c28-ac86-9f93cd567cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(docs)): \n",
    "    embedding = create_vector_embedding_with_bedrock(docs[idx].page_content, bedrock_client)\n",
    "    document = {\n",
    "                'vector_field': embedding['vector_field'],\n",
    "                'text': embedding['text']\n",
    "                }\n",
    "    response = oss_client.index(\n",
    "    index = index_name,\n",
    "    body = document\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c52b22-2d08-43fb-88e2-fd39f4ff0b1b",
   "metadata": {},
   "source": [
    "## Perform Search based on Text Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55530289-7577-4384-a671-812aac736e81",
   "metadata": {},
   "source": [
    "Let’s take a look at the results of a simple query. In below example, we'll receive an text input from user, and then will send it to search engine to get the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674e889-e207-4290-9991-52fb977a57a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_prompt = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "#query_prompt = \"Who Must File Form 8300?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8e609-a900-469b-9844-70695dcdcdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_emb = create_vector_embedding_with_bedrock(query_prompt, bedrock_client)['vector_field']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4debd-798d-49d9-b5bf-ee39734eab3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"vector_field\": {\n",
    "                    \"vector\": query_emb,\n",
    "                    \"k\": 5,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f6bb7-cfe4-41f1-8d77-53ae5ac110ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = oss_client.search(index=index_name, body=body)\n",
    "results = \"\"\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    id_ = hit[\"_id\"]\n",
    "    text = hit[\"_source\"][\"text\"]\n",
    "    # results.append(text)\n",
    "    results += text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c2825-4f7a-43ae-9cfb-f518007305ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generative Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85eef4e-b0fd-4921-9ac4-b53d559799a4",
   "metadata": {},
   "source": [
    "Define utility function for conversation with Bedrock converse API\n",
    "\n",
    "Model used - Cohere Command R+: A powerful Large Language Model (LLM) capable of understanding and generating text in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17726acc-ddb7-4353-a9f3-969f8bb1007a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history=[],\n",
    "    temperature=0.3,\n",
    "    max_tokens=400,\n",
    "    top_p=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompt (str) : The system prompt for the model to use.\n",
    "        prompt (str) : The message/question to send to the model.\n",
    "        chat_history (list): The chat history from user and assistant.\n",
    "\n",
    "    Returns:\n",
    "        response (str): The text generated output from the model.\n",
    "        chat_history (str): The full conversation between user and assistant that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompts = [\n",
    "        {\n",
    "            \"text\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    chat_history.extend(messages)\n",
    "\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature,\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"topP\": top_p,\n",
    "    }\n",
    "\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    chat_history.append(response[\"output\"][\"message\"])\n",
    "\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"], chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf004f02-28d6-4372-b837-1a63c1f3b569",
   "metadata": {},
   "source": [
    "Define the system prompt and guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d172292-cb89-42d7-9346-84a92c311df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an AI assistant. Your knowledge is based solely on the information provided between the <documents> and </documents> tags.\n",
    "\n",
    "Before answering any questions, first check if the user has provided information between the <documents> and </documents> tags. If no information is provided, respond with the following JSON:\n",
    "\n",
    "{\n",
    "    \"answer\": \"I do not have enough information to answer that question.\"\n",
    "}\n",
    "\n",
    "If documents are provided, your task is to answer questions accurately and concisely, using only the details from the given documents. Do not use your own knowledge or any external sources to answer the questions, even if you know the answer.\n",
    "\n",
    "If a question cannot be fully answered using the provided documents, respond with the following JSON:\n",
    "\n",
    "{\n",
    "    \"answer\": \"I do not have enough information to answer that question.\"\n",
    "}\n",
    "\n",
    "All responses must be in valid JSON format, with the 'answer' key containing the actual response text.\n",
    "\n",
    "To provide transparency, include your reasoning process with the 'thinking' key as the following format:\n",
    "\n",
    "{\n",
    "    \"answer\": \"Your response here\",\n",
    "    \"thinking\": \"Your reasoning process here\"\n",
    "}\n",
    "\n",
    "Be concise and objective in your responses, without any personal opinions or subjective statements.\n",
    "\"\"\"\n",
    "prompt_template = \"<documents>\\n{documents}\\n</documents>\\n\\nQuestion: {question}\\nThink step-by-step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab34e6f-e31d-4f45-992e-75aded6a2849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model ID parameter\n",
    "model_id = \"cohere.command-r-plus-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcb404-82f9-4daf-bc68-25f589bf3f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "prompt = prompt_template.format(documents=results, question=query_prompt)\n",
    "response, chat_history = generate_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    system_prompt,\n",
    "    prompt,\n",
    "    chat_history\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113040e-a16c-4529-be74-b8baea9f6e28",
   "metadata": {},
   "source": [
    "## 6. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87766ad7-6326-4000-a9de-3543c94fff47",
   "metadata": {},
   "source": [
    "When you finish this exercise, remove your resources with the following steps:\n",
    "\n",
    "Delete vector index.\n",
    "Delete data, network, and encryption access ploicies.\n",
    "Delete collection.\n",
    "Delete SageMaker Studio user profile and domain.\n",
    "Optionally, empty and delete the S3 bucket, or keep whatever you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337b04e-d37c-400c-9899-d56b352f6c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete vector index\n",
    "oss_client.indices.delete(index=index_name)\n",
    "\n",
    "# delete data, network, and encryption access ploicies\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])\n",
    "\n",
    "# delete collection\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "aoss_client.delete_collection(id=collection_id)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
