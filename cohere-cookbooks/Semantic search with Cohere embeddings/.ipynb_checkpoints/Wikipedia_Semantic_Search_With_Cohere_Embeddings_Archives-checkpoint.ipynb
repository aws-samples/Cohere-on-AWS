{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embeddings Archives\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In this notebook, we demonstrate how to use the [Amazon Bedrock InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://cohere.com/blog/embedding-archives-wikipedia) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use the 2023 version of [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary) and binary embeddings. We also use the [Amazon Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html) to demonstrate how we can use the results of semantic search.\n",
    "\n",
    "### Semantic Search and Text Embeddings\n",
    "Semantic search leverages text embeddings and similarity to find responses based on meaning, not just keywords. Text embeddings represent pieces of text as numeric vectors that encode semantic meaning. These embeddings allow for mathematical comparisons of word and sentence meaning. Multilingual embeddings map text in different languages to the same vector space, enabling semantic search across languages. See [What is Semantic Search](https://cohere.com/blog/what-is-semantic-search) to reaad about improvement algorithms such as hierarchical navigable small world (HNSW) and multiple negative ranking loss.\n",
    "\n",
    "### Int8/byte and Binary Encoded Embeddings\n",
    "Semantic search over large datasets can require a lot of memory because most vector databases store embeddings and vector indices in memory. Dimensionality reduction to conserve memory and reduce costs can perform poorly ([Cohere research](https://arxiv.org/abs/2205.11498?ref=cohere-ai.ghost.io)). \n",
    "\n",
    "A better approach is to use a model that uses fewer bits per dimension. Cohere's Embed is a text embedding model that offers leading performance in 100+ languages. It translates text into vector representations which encode semantic meaning. Cohere's Embed is the first embedding model that natively supports int8/byte and binary embeddings.\n",
    "\n",
    "Binary embeddings give you a 32x reduction in memory and can be searched 40x faster. Given that embeddings are typically stored as float32, an embedding with 1024 dimensions requires 1024 x 4 bytes = 4096 bytes. Using 1 bit per dimension results in a 32x reduction in required memory (or, 4096 * 8 / 1024). See [Cohere int8 & binary embeddings](https://cohere.com/blog/int8-binary-embeddings).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Step 0: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "IUnwp2cYNnP0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's install HF datasets and boto3, the AWS SDK for Python\n",
    "%pip install datasets --quiet\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "### Step 1: Install the Wikipedia embeddings archives published by Cohere\n",
    "\n",
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "v8Pogz7gPQwg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb_int8', 'emb_ubinary'],\n",
      "    n_shards: 7\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Import torch, the open-source machine learning library\n",
    "import torch\n",
    "\n",
    "# Load at max 1000 documents and embeddings\n",
    "max_docs = 1000\n",
    "# Use the Simple English Wikipedia subset\n",
    "lang = \"simple\"\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\", lang, split=\"train\", streaming=True)\n",
    "\n",
    "# To verify we have loaded the data, print docs_stream\n",
    "print(docs_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `IterableDataset` object contains a collection of 1000 examples, each with `features` which are the names of the columns for each example.\n",
    "\n",
    "The `emb_int8` is an integer encoded embedding while `emb_ubinary` is a binary encoded embedding for each Wikipedia article article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Create tensor of binary embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's create lists of documents and binary embeddings\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc[\"emb_ubinary\"])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "# Convert doc_embeddings into a PyTorch tensor\n",
    "doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://cohere.com/blog/sentence-word-embeddings) of 128 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the tensor shape\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "### Step 3: Embed query and compute dot product with document embeddings\n",
    "We can now search these vectors for any query we want. For this example, we'll ask a question about Alan Turing since we know the Wikipedia page for Alan Turing is included in this subset of the archive.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product).\n",
    "\n",
    "This shows the top `k` passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Alan Turing because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embeddings: [[18, 63, 75, 232, 59, 67, 51, 160, 255, 68, 251, 186, 114, 165, 136, 58, 82, 15, 211, 232, 128, 37, 107, 204, 75, 163, 74, 251, 32, 233, 200, 154, 106, 241, 127, 125, 74, 31, 123, 209, 82, 220, 228, 15, 254, 151, 220, 43, 199, 230, 143, 73, 67, 229, 149, 61, 34, 86, 69, 56, 215, 178, 131, 49, 108, 251, 76, 187, 134, 2, 155, 169, 129, 130, 229, 103, 12, 113, 145, 9, 32, 139, 212, 3, 224, 64, 27, 151, 175, 217, 139, 30, 132, 192, 111, 60, 221, 162, 108, 120, 153, 219, 214, 165, 164, 133, 78, 232, 203, 63, 149, 53, 135, 117, 100, 213, 75, 46, 114, 159, 22, 216, 255, 233, 98, 26, 252, 22]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To use Cohere models on Bedrock we need to install dependencies\n",
    "import boto3, json, logging\n",
    "# Set up the Bedrock client\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create request paramaters for Bedrock\n",
    "model_id = \"cohere.embed-multilingual-v3\"\n",
    "accept = \"*/*\"\n",
    "content_type = \"application/json\"\n",
    "embedding_types = [\"ubinary\"]\n",
    "input_type = \"search_query\"\n",
    "\n",
    "# Create the text used for semantic search\n",
    "query = \"Tell me about Alan Turing\"\n",
    "\n",
    "# Set the number of nearest neighbors\n",
    "k = 7\n",
    "\n",
    "body = json.dumps({\n",
    "    \"texts\": [query],\n",
    "    \"input_type\": input_type,\n",
    "    \"embedding_types\": embedding_types}\n",
    ")\n",
    "\n",
    "# Call the Bedrock InvokeModel API\n",
    "response = bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept=accept,\n",
    "    contentType=content_type\n",
    ")\n",
    "\n",
    "# Load the response into response_body\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "# Extract the binary embeddings\n",
    "query_emb_int8 = response_body[\"embeddings\"][\"ubinary\"]\n",
    "print(\"Query embeddings:\", query_emb_int8, \"\\n\")\n",
    "\n",
    "# Convert query into a PyTorch tensor\n",
    "query_emb_int8 = torch.tensor(query_emb_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's imagine that we didn't know that our documents contain text with information about Alan Turing. The way to search for relevant documents is to search the `doc_embeddings` with the binary embeddings that we just created above. The semantic meaning of our query is captured by the embeddings, and a simliar query will return an embedding with similar elements. A high score for the dot product indicates similarity. See [What is similarity between sentences](https://cohere.com/blog/what-is-similarity-between-sentences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest element from the dot_scores tensor is 2703919\n",
      "As expected, this value is still lower than the dot product of the query embeddings and itself 2873374 \n",
      "\n",
      "The query is: Tell me about Alan Turing \n",
      "\n",
      "The below is a list of top k relevant documents:\n",
      "Title: Alan Turing\n",
      "Text: Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Alan Turing\n",
      "Text: In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Botany\n",
      "Text: Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.\n",
      "https://simple.wikipedia.org/wiki/Botany \n",
      "\n",
      "Title: Creativity\n",
      "Text: Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.\n",
      "https://simple.wikipedia.org/wiki/Creativity \n",
      "\n",
      "Title: Computer\n",
      "Text: In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.\n",
      "https://simple.wikipedia.org/wiki/Computer \n",
      "\n",
      "Title: Alan Turing\n",
      "Text: Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Angel\n",
      "Text: The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "https://simple.wikipedia.org/wiki/Angel \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dot score between query embeddings and document embeddings\n",
    "dot_scores = torch.mm(query_emb_int8, doc_embeddings.transpose(0, 1))\n",
    "\n",
    "print(\"The largest element from the dot_scores tensor is\", dot_scores.max().item())\n",
    "print(\"As expected, this value is still lower than the dot product of the query embeddings and itself\", torch.mm(query_emb_int8, query_emb_int8.transpose(0, 1)).item(), '\\n')\n",
    "\n",
    "# Use topk to return the largest elements of the dot_scores tensor\n",
    "top_k = torch.topk(dot_scores, k)\n",
    "\n",
    "# Print results\n",
    "print(\"The query is:\", query, \"\\n\\nThe below is a list of top k relevant documents:\")\n",
    "# This loop iterates over the indices of the top k relevant documents\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print(\"Title:\", docs[doc_id][\"title\"])\n",
    "    print(\"Text:\", docs[doc_id][\"text\"])\n",
    "    print(docs[doc_id][\"url\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add the results of semantic search as context to a prompt\n",
    "\n",
    "Let's start by sending the same query to the Command R+ model using Amazon Bedrock to compare responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of API call 1 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to many fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and is often called the \"Father of Computer Science.\"\n",
      "\n",
      "Turing was born in London, England, in 1912 and showed a talent for science and mathematics from an early age. He attended the University of Cambridge, where he studied mathematics and gained a reputation for his work in computability and the emerging field of computer science.\n",
      "\n",
      "One of Turing's most significant achievements was his work during World War II at Britain's code-breaking center, Bletchley Park. There, he played a crucial role in breaking the German Enigma machine, a complex encryption device used by the German military to send secure messages. Turing designed a series of techniques and machines, including the \"Turing bombe,\" which helped decipher Enigma messages and \n",
      "\n",
      "\n",
      "Result of API call 2 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to several fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and is often called the \"Father of Computer Science\" and the \"Father of Artificial Intelligence.\"\n",
      "\n",
      "Turing was born in London, England, in 1912 and showed a talent for science and mathematics from an early age. He attended the University of Cambridge, where he studied mathematics and gained a reputation for his work in computability and the emerging field of computer science.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, particularly those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, significantly contributed to the Allied victory. Turing designed a series of techniques and machines to expedite the code-breaking process, including the bombe, a machine that \n",
      "\n",
      "\n",
      "Result of API call 3 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made significant contributions to many fields, including computer science, artificial intelligence, cryptography, and mathematics. He is widely considered to be one of the most influential figures in the development of modern computing and one of the key people who helped the Allies win World War II.\n",
      "\n",
      "Here is a brief overview of his life and contributions:\n",
      "\n",
      "Early Life and Education:\n",
      "- Alan Mathison Turing was born on June 23, 1912, in Maida Vale, London, to Julius and Ethel Turing. He showed an early aptitude for science and mathematics.\n",
      "- Turing attended Sherborne School, where he excelled in mathematics and science, and also showed interest in cryptography. He went on to study at King's College, University of Cambridge, where he graduated with a degree in mathematics in 1934.\n",
      "\n",
      "Contributions to Computing and Cryptanalysis:\n",
      "- In 1936, Turing \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the variables to make a call to the converse API\n",
    "user_message = \"Tell me about Alan Turing.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Make the API call three times to visualize the different responses\n",
    "    for i in range(3):\n",
    "        print(\"Result of API call\", str(i+1), ':')\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = bedrock_rt.converse(\n",
    "            modelId=\"cohere.command-r-plus-v1:0\",\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text, \"\\n\\n\")\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the large language model (LLM) is clearly non-deterministic. [Read more about LLM parameters here](https://cohere.com/blog/llm-parameters-best-outputs-language-ai) to learn about the parameters used to control model output. Note that we can also add the results of the semantic search as context for our prompt to augment the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is now: Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "Given the information above, answer this question: Tell me about Alan Turing.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string\n",
    "context = \"\"\n",
    "\n",
    "# Append the text from the relevant documents to the context\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    context += docs[doc_id][\"text\"]\n",
    "\n",
    "# Create a new prompt\n",
    "prompt = f\"\"\"{context}\n",
    "Given the information above, answer this question: {user_message}\"\"\"\n",
    "\n",
    "print(\"The prompt is now:\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 0.5 for the `temperature` to illustrate variance in responses from the LLM. The default value for `temperature` is 0.3 and lower values decrease randomness in the response. [Read more about inference requests to Amazon Bedrock here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command-r-plus.html). Also, change the `modelId` to `cohere.command-r-v1:0` and use the Command R model if you want to test chat history. The Command R model supports a conversation history with multiple turns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of API call 1 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science and is often referred to as the \"father of computer science and artificial intelligence.\" \n",
      "\n",
      "As you mentioned, Turing is known for his concept of the 'Turing machine,' which he introduced in 1936. This hypothetical device served as a simple model for a general-purpose computer, and it played a significant role in the development of modern computers. \n",
      "\n",
      "During World War II, Turing's expertise in cryptography was instrumental in breaking German military codes, including those generated by the Enigma machine. His work at Bletchley Park, the central site for British code-breaking operations, is credited with shortening the war and saving countless lives. \n",
      "\n",
      "However, despite his remarkable contributions, Turing's life was tragically cut short. In 1952, he was convicted of 'gross indecency' due to \n",
      "\n",
      "\n",
      "Result of API call 2 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science and is often referred to as the \"father of computer science and artificial intelligence.\"\n",
      "\n",
      "As you mentioned, Turing is known for his concept of the Turing machine, which he described in 1936. The Turing machine was a theoretical device that could perform any computation if given the appropriate instructions. This concept introduced the idea of a programmable computer and is considered a cornerstone of computer theory.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, including those generated by the Enigma machine. His work at Bletchley Park, the UK's code-breaking center, significantly contributed to the Allied victory.\n",
      "\n",
      "Unfortunately, Turing was prosecuted in 1952 for homosexual acts, which were criminalized in the UK at the time. He underwent chemical castration as an alternative to prison. \n",
      "\n",
      "\n",
      "Result of API call 3 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made significant contributions to the field of computing and played a crucial role during World War II. He is widely considered one of the most influential figures in the history of computer science and artificial intelligence. \n",
      "\n",
      "Here's what I can gather about Alan Turing from the provided information: \n",
      "\n",
      "- In 1936, Turing created the concept of a theoretical \"Turing machine,\" which was a hypothetical device that helped explain the functions of a computer program. This machine was imaginary but laid the groundwork for the development of modern computers. \n",
      "- During World War II, Turing was a key figure in breaking German military codes, particularly those generated by the Enigma machine. His work in cryptanalysis was instrumental in helping the Allies gain valuable intelligence, which many believe shortened the war and saved countless lives. \n",
      "- Turing was posthumously pardoned in 2013 by Queen Elizabeth II for charges related to his homosexuality, which was criminal \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send the same query to the Command R+ model using the Bedrock converse API but add the results of the semantic search as context\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}],\n",
    "    }\n",
    "]\n",
    "\n",
    "# For use as the chat_history parameter when modelId is cohere.command-r-v1:0\n",
    "# history = [\n",
    "#     {\"role\": \"USER\", \"message\": \"Example question from user\"},\n",
    "#     {\"role\": \"CHATBOT\", \"message\": \"Example response from chatbot\"}\n",
    "# ]\n",
    "\n",
    "try:\n",
    "    for i in range(3):\n",
    "        print('Result of API call', str(i+1), ':')\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = bedrock_rt.converse(\n",
    "            modelId='cohere.command-r-plus-v1:0',\n",
    "            messages=conversation,\n",
    "            # chat_history = history,\n",
    "            inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text, '\\n\\n')\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "In this notebook, we discussed the benefits of using different types of embeddings. Dimensionality reduction negatively impacts performance, but Cohere's Embed helps solve this problem by natively supporting int8/byte and binary embeddings. Binary embeddings provide a 32x memory reduction and enable 40x faster search compared to float32 embeddings, offering a highly efficient solution for large-scale semantic search. In the example code of this notebook, we used Amazon Bedrock to call Cohere's Embed and Command R+ LLMs. In addition, we can use the results of semantic search as context when sending prompts to a LLM. As seen above, using the augmented prompt containing the top `k` results from the semantic search gives much more deterministic responses. This is just one of the techniques we can use to return responses with up-to-date and domain-specific information. Semantic search also forms the basis for more advanced improvement algorithms such as HNSW and multiple negative ranking loss."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
